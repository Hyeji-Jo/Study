# Abstract(초록)
- **Whisper의 문제점**
  - Whisper 모델은 인코더-디코더 구조로 되어 있어 실시간 스트리밍 음성 인식에 적합하지 않음
- **Simul-Whisper**
  - Whisper의 크로스-어텐션 메커니즘에서 얻은 시간 정렬 정보를 사용하여 자기 회귀적 디코딩 활용
  - Whisper 모델의 fine-tuning 없이 청크 단위로 실시간 음성 인식을 수행
- **절단 문제**
  - 청크 경계에서 단어가 잘리는 문제를 발견
    - 이는 디코딩 결과에 부정적인 영향을 미침
  - Truncation Detection 제안
    - 통합 및 발화 기반 절단 감지 모델
- **성능**
  - 오류율: Simul-Whisper는 1초 청크 크기에서 평균 1.46%의 절대 단어 오류율 감소
  - 현재 상태의 최고 성능을 자랑하는 스트리밍 ASR 모델보다 월등한 성능을 보임

# Introduction(소개)
- Whisper 모델 스트리밍 ASR로 사용하기 어려움
  - Whisper는 스트리밍 ASR을 위해 사전 학습되지 않음
  - 회의 기록, 동시 번역 및 라이브 스트리밍 등 실시간 응용에서 적용하기 어려움
- 스트리밍 ASR의 어려움
  - 전체 문맥이 부족한 상태에서 추론을 수행해야 하기 때문에 오프라인 ASR보다 더 어려운 작업
  - 일반적으로 Transformer 기반 스트리밍 ASR의 경우 time-restricted, chunk-wise, memory based attention을 사용
    - 하지만 이러한 방법으로 사전 학습된 매개변수를 수정하는 것은 많은 계산 자원을 요구
- Whisper 모델의 변화 도전
  - 인코더는 입력 오디오를 한 번에 잠재 표현으로 변환한 후
  - 디코더는 특수한 “⟨eos⟩” 토큰을 만날 때까지 반복적으로 자기 회귀적 디코딩을 수행
  - 스트리밍 ASR에서 모델에 대한 입력은 고정된 기간의 짧은 세그먼트로 잘리는 경우가 많음
  - 이러한 무작위 잘림으로 인해 전사가 끝날 때 신뢰할 수 없는 출력이 발생할 수 있음
  - 심지어 Atention에 실패하여 의미 없는 반복 출력이 발생할 수도 있음
  - 위와 같은 오류는 디코딩을 경계에서 멈추어도 방지할 수 없음
    - 인코더-디코더 cross-attention의 비선형성으로 인해 출력 토큰의 시작 및 끝 타임스탬프를 찾기 어렵기 때문
