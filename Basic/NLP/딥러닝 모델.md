# Recurrent Neural Network (RNN)
<img width="595" alt="image" src="https://github.com/user-attachments/assets/de3a538f-4107-41b8-92a5-893e092fe179">

- 언어의 Seqeuntial한 특징을 잘 학습하기 위해 등장
- 현재 타임스텝에 대해 이전 스텝까지의 정보를 기반으로 예측값을 산출하는 구조
- 계산 방법
  <img width="657" alt="image" src="https://github.com/user-attachments/assets/08e25a8b-bc94-4fd5-9421-155eac08dffb">


# Character-level Language Model
- 언어 모델 : 이전에 등장한 문자열을 기반으로 다음 단어를 예측하는 태스크
- Character-level Language Model : 문자 단위로 다음에 올 문자를 예측하는 언어 모델

# RNN
- 들어오는 입력값에 대해서, 많은 유연성을 가지고 학습되는 딥러닝 모델
- 그레디언트 소실/증폭 문제가 있어 실제로 많이 사용되지는 않음
- 모델이 학습하는 방법 : Truncation , BPTT
  - Truncation
    - 제한된 리소스(메모리) 내에서 모든 시퀀스를 학습할 수 없기때문에 아래 사진과 같이 잘라서 학습에 사용하는 것
  - BPTT
    - Backpropagation through time의 줄임말
    - RNN에서 타임스텝마다 계산된 weight를 backward propagation을 통해 학습하는 방식을 의미

# LSTM
- 단기 기억으로 저장하여 이걸 때에 따라 꺼내 사용함으로 더 오래 기억할 수 있도록 개선하는 것

## LSTM vs RNN
- LSTM : 각 time step마다 필요한 정보를 단기 기억으로 hidden state에 저장하여 관리되도록 학습하는 것
- RNN : 오차역전파(backpropagation) 진행시 가중치(W)를 계속해서 곱해주는 연산
  - LSTM은 forget gate를 거친 값에 대해 필요로하는 정보를 덧셈을 통해 연산하여 그레디언트 소실/증폭 문제를 방지
  - LSTM과 GRU 모델은 RNN과 달리 가중치를 곱셈이 아닌 덧셈을 통한 그레디언트 복사로 그레디언트 소실/증폭 문제를 해결
 
 
