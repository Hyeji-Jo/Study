- 단어를 벡터로 만드는 방법 중 하나
- 페이스북에서 개발한 FastText
- 메커니즘 자체는 Word2Vec의 확장
- Word2Vec와 FastText와의 가장 큰 차이점
  - Word2Vec는 단어를 쪼개질 수 없는 단위로 생각
  - FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주

## 1) 내부 단어(subword)의 학습
- **FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급**
- n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정
  - n을 3으로 잡은 트라이그램(tri-gram)의 경우, apple은 app, ppl, ple로 분리하고 이들을 벡터로 만듬
  - 시작과 끝을 의미하는 <, >를 도입하여 아래의 5개 내부 단어(subword) 토큰을 벡터로 만듬
- 실제 사용할 때는 n의 최소값과 최대값으로 범위 설정 가능
- 내부 단어들을 벡터화한다는 의미는 저 단어들에 대해서 Word2Vec을 수행한다는 의미
  - 내부 단어들의 벡터값을 얻었다면, apple의 벡터값은 총 합으로 구성
    
## 2) 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응
- 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩
- 장점
  - 내부 단어(Subword)를 통해 모르는 단어(OOV)에 대해서도 다른 단어와의 유사도 계산 가능
  - 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점

## 3) 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응
- Word2Vec의 경우에는 등장 빈도 수가 적은 단어에 대해서는 임베딩의 정확도가 높지 않다는 단점 존재
- FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교하여 비교적 높은 임베딩 벡터값
  - 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 됨
  - FastText는 이에 대해서도 일정 수준의 성능을 보임

## 4) 한국어에서의 FastText
1. 음절 단위
  - <자연, 자연어, 연어처, 어처리, 처리>
2. 자모 단위 (초성, 중성, 종성 단위)
  - < ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>
