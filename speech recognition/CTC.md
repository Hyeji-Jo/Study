- 마르코프 랜덤 필드 공부
- HMM 모델 공부

# Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks 논문 리뷰

## 0. Abstract
> 많은 sequence learning tasks는 **노이즈가 있고 segmentation 되어있지 않은 인풋 데이터로부터 라벨의 sequence를 예측** 해야한다.
> 예를 들어, 음성 인식에서는 음향 신호를 단어 또는 단어의 하위 단위로 변환한다.
> **순환 신경망(RNN)** 은 이러한 작업에 매우 적합한 **강력한 시퀀스 학습기**입니다. 
> 하지만, **RNN은 사전 분할된 학습 데이터가 필요**하고, **출력을 레이블 시퀀스로 변환하기 위한 후처리가 필요**하기 때문에 적용이 제한적이었다.
> 이 논문은 **RNN이 분할되지 않은 시퀀스에 직접 레이블을 지정하도록 학습**하는 새로운 방법을 제시하여 두 가지 문제를 모두 해결합니다.
> **TIMIT 음성 말뭉치**에서 수행한 실험은 이 방법이 기존 HMM과 하이브리드 HMM-RNN보다 우수한 성능을 발휘함을 보여준다.

- 분할되지 않은 시퀀스 데이터 레이블링 문제를 다룸
- CTC 방법을 통해 RNN을 사전 데이터 분할 없이 사용할 수 있음
- 실험 결과 CTC가 기존 모델들(HMM, 하이브리드 모델)보다 우수한 성능을 발휘


### 🔎 Unsegmented data
- **개념**
  - 데이터를 명확하게 나누지 않은 상태를 의미
  - 즉, 데이터가 연속적인 흐름으로 존재하며, 특정 구간으로 미리 나누어지지 않은 상태
- **음성인식에서의 개념**
  - 음성 인식에서 입력 신호는 연속적인 음향 신호
  - 신호의 어느 부분이 특정 단어 또는 음소에 해당하는지 명확하지 않음을 의미
  - 음성인식을 예로 들면, 프레임 당 무슨 음운인지는 모르지만 전체 라벨인 단어는 아는 상태를 의미
  - Waveform(파형) 파일에 대한 라벨이 "the sound of"라고 달려 있다고 할 때, 해당 waveform 파일에서 어디까지가 각각 "the", "sound", "of"인지의 경계가 명확하게 분할되어 있지 않은 경우를 생각
- **특징**
  - 이 개념은 특히 음성 인식, 필기 인식, 제스처 인식 등과 같은 순차 데이터 처리에 자주 등장
  - 음성, 영상, 필기, 제스처 등 시간에 따라 변하는 연속적인 데이터는 특정 단위를 나누는 기준을 정하기가 어려움
  - 이미지 데이터: 하나의 이미지에서 구분되지 않은 객체들이 여러 개 있을 때
  - 텍스트 데이터: 자연어 처리에서도 문장이 나뉘어 있지 않고, 구두점 없이 이어져 있는 텍스트
 
### 🔎 Sequence
- 일정한 순서나 연속성을 가진 데이터나 사건의 나열을 의미
- 연속적인 요소들의 나열
  - 해당 요소들은 특정한 순서를 가지고 있음
  - 순서가 시퀀스의 핵심
- **음성인식에서의 개념**
  - 음성 데이터는 시간에 따라 변화하는 연속적인 신호의 시퀀스
  - 이 신호는 음소(소리의 최소 단위)의 연속으로 구성되며, 음성을 인식하려면 시퀀스의 순서와 패턴을 분석해야 함
  - 시간 순서대로 이어진 소리의 연속적인 데이터
  - **음성 신호 자체가 시퀀스**이고, 그 안에는 **주파수, 볼륨, 진폭 같은 소리의 다양한 특성들이 포함**되어 있어
    - 예를 들어, "안녕하세요"라고 말하면 이 말이 끝나는 그 순간까지 **시간에 따라 변하는 신호의 흐름(시퀀스)** 이 기록되는 거 
- **시퀀스처리에 적합한 모델**
  - 순서를 반영하여 처리할 수 있는 알고리즘
  - RNN(순환 신경망), LSTM

### 🔎 Acoustic signal(음향 신호)
- 소리가 물리적인 매체(공기, 물, 고체 등)를 통해 전달될 때 발생하는 신호
- 소리는 기압의 변화로 인해 발생하는 파동인데, 이 파동이 매체를 통해 전달될 때 진동이 발생하고, 이 진동이 시간에 따라 변화하면서 신호로 기록
- 소리가 전달될 때 발생하는 연속적인 신호로, 주파수, 진폭, 시간 등의 요소를 포함한 소리의 물리적 특성을 나타내는 데이터

### 🔎 Post processing(후처리)
- 모델이나 시스템에서 처음으로 생성된 결과물을 추가적으로 가공하는 과정
- **음성인식에서의 개념**
  - 모델이 처음 예측한 결과를 가공하여, 사람이 이해할 수 있는 문장이나 단어로 바꾸는 과정
  - RNN 기반의 음성 인식 시스템이 프레임 단위로 음성 신호를 처리해 음소(소리의 작은 단위)를 예측했다면, 이 음소들을 연결하고, 불필요한 중복을 제거하며, 최종적으로 정확한 단어나 문장으로 바꾸는 작업
  - 음소 연결: 음성 인식 모델이 "안녕하세요"라는 문장을 소리 단위로 인식했을 때, "ㅇ-ㅇ-ㄴ-ㄴ-ㅇ-ㅕ-ㅅ-ㅛ" 같은 음소들로 나올 수 있습니다. 이 음소들을 올바른 순서로 연결하여 실제 문장으로 변환하는 과정
  - 불필요한 중복 제거: 모델이 같은 음소를 여러 번 예측할 수 있습니다. 예를 들어 "ㄴ-ㄴ"이 두 번 나온다면, 이걸 한 번의 "ㄴ"으로 축약하는 과정이 필요
  - 정확한 문장으로 변환: 모든 음소를 적절하게 조합해 문장으로 변환하는 과정도 후처리의 일부입니다. 예를 들어 "ㅇ-ㅕ-ㅅ-ㅛ"를 "여쑤"로 변환

### 🔎 TIMIT
- 음성 인식 연구와 실험에서 자주 사용되는 표준 음성 데이터베이스
- 음성 인식 시스템을 학습시키고 평가하는 데 사용
- TIMIT은 다양한 발음과 억양을 가진 영어 화자들의 녹음된 음성 데이터를 포함
  - 630명의 다른 화자가 각기 다른 문장을 읽는 데이터를 포함
- 각 녹음된 음성 파일에 대해 **정확한 발음 레이블(음소 단위)** 이 포함
  - 각 음성 신호가 어떤 소리(음소)로 구성되어 있는지를 명확하게 레이블링한 것이 특징
- 8개 문장: 각 화자는 10개의 문장을 읽습니다. 이 중 2개는 모든 화자가 동일한 문장이고, 5개는 서로 다른 문장, 3개는 서로 교차된 문장
- 음소 레벨의 주석: TIMIT 데이터는 음소 단위로 주석이 달려 있어, 음성 인식 모델이 음소 수준에서 정확한 학습가능 

  
## 1. Introduction
> 분할되지 않은 시퀀스 데이터에 레이블을 지정하는 것은 실제 시퀀스 학습에서 흔히 있는 문제입니다. 특히, 잡음이 있는 실수 값의 입력 스트림에 문자나 단어와 같은 개별 레이블 문자열로 주석이 달린 지각 작업(예: 필기 인식, 음성 인식, 제스처 인식)에서 일반적입니다.
> 
> 현재 HMM(은닉 마르코프 모델), CRF(조건부 랜덤 필드) 등과 같은 **그래프 기반 모델들이 시퀀스 레이블링의 주요 프레임워크**로 사용되고 있습니다. 이러한 접근 방식은 많은 문제에서 성공적이었지만, 몇 가지 **한계점**이 있습니다: 1. HMM의 상태 모델을 설계하거나 CRF의 입력 특징을 선택하는 데 상당한 양의 과제 특화 지식이 필요합니다. 2. 의존성 가정을 명시적으로 설정해야 하는데, 이는 HMM의 경우 관측치가 독립적이라는 가정이 필요하며, 이를 통해 추론이 가능하게 됩니다. 3. 표준 HMM의 경우, 시퀀스 레이블링이 **판별적(discriminative)** 인데도 불구하고 생성적(generative) 학습 방식을 사용해야 합니다.
>
> 반면에 **RNN(순환 신경망)** 은 데이터의 입력과 출력 표현을 선택하는 것 외에는 사전 지식이 필요하지 않습니다. RNN은 판별적으로 학습할 수 있고, 내부 상태를 통해 시간 시리즈 데이터를 강력하게 모델링할 수 있는 일반적인 메커니즘을 제공합니다. 또한 시간적 및 공간적 노이즈에 강한 경향이 있습니다. 그러나, 현재까지 RNN을 시퀀스 레이블링에 직접 적용하는 것은 불가능했습니다. 그 이유는 표준 신경망 목적 함수가 시퀀스의 각 지점에 독립적으로 정의되기 때문입니다. 즉, RNN은 독립적인 레이블 분류를 위한 학습만 가능했습니다. 따라서 학습 데이터가 사전 분할되어야 하고, 최종 레이블 시퀀스를 생성하기 위해 네트워크 출력을 후처리해야 했습니다.
>
> 현재로서 RNN을 시퀀스 레이블링에 가장 효과적으로 사용하는 방법은 HMM과 결합하는 하이브리드 방식입니다. 하이브리드 시스템은 HMM을 통해 데이터의 장기적인 시퀀스 구조를 모델링하고, 신경망을 통해 국소적인 분류를 제공합니다. HMM은 학습 중에 자동으로 시퀀스를 분할하고, 네트워크 분류 결과를 레이블 시퀀스로 변환합니다. 그러나 하이브리드 시스템은 앞서 언급한 HMM의 단점을 그대로 가지고 있으며, RNN이 시퀀스 모델링에서 제공할 수 있는 잠재력을 완전히 활용하지는 못합니다.
>
> 

### 🔎 Perceptual task
- 사람이 감각을 통해 외부의 자극을 인식하고 처리하는 작업(시각, 청각, 촉각)
- 주로 사람이나 기계가 감각 정보를 바탕으로 주변 환경을 인식하거나, 그 정보를 처리하여 특정한 결론을 도출하는 과정에서 발생하는 작업들을 포함
- **Perceptual task가 중요한 이유**
  - 컴퓨터가 시각이나 청각 정보를 처리해 사람처럼 인식하고 결정을 내리는 능력을 개발하는 것이 목표인 경우가 많기 때문
  - 음성 인식: 음향 신호를 분석해 어떤 말을 하고 있는지 인식하는 것

 
### 🔎 Graphical model
![image](https://github.com/user-attachments/assets/7954c4dc-55d4-4b7a-80f3-1d464339e60f)

- 확률 변수들 간의 관계를 그래프 구조로 표현하는 모델
- 변수들간의 상호 의존 관계를 표현한 확률 모델
- 확률론적인 문제를 해결하기 위해 변수들 사이의 **의존성(dependencies)** 과 **독립성(independence)** 을 시각적으로 표현하고, 그 구조를 기반으로 추론이나 학습을 수행
- Conditional Independence라는 가정
- **구성요소**
  - Node (Vertex): 랜덤 변수(random variable)를 하나의 노드로 표현
  - Edge (Link) : 랜덤 변수 사이의 확률적인 관계
- 그래프 방향성 여부에 따라 2가지 형태 존재
  - **Undirected Graph** : 아래 Nodes와 Edges 그림 처럼 방향성이 없는 그래프
    ![image](https://github.com/user-attachments/assets/16121b9c-3725-456e-be1d-b6bad3e024b9)

    - 예시 : Markov Random Fields, Boltzmann Machine
      
  - **Directed Graph** : 아래 오른쪽 그림처럼, 화살표가 존재하는 그래프
    ![image](https://github.com/user-attachments/assets/1c3370ab-a932-4f1e-913e-1be53c7451f2)

    - 자식 노드 이전에 부모 노드가 존재
    - 예시 : Bayesian Networks, HMM (Hidden Markov Models), Latent Variable Models 
  ![image](https://github.com/user-attachments/assets/24947be2-2e74-41ab-ae93-ebe1728a50e5)

- **주요 유형**
  - **베이즈 네트워크(Bayesian Network)**
    - **방향성 비순환 그래프(DAG, directed acyclic graphs)** 로 구성된 모델
    - 노드들은 확률 변수를 나타내고, 엣지는 변수들 간의 조건부 의존성을 나타냄
  - **마르코프 랜덤 필드(Markov Random Field, MRF)**
    - 비방향 그래프로 구성된 모델
    - 각 변수 간의 상호작용이 엣지로 연결
    - 모델은 주로 공간적인 의존성이나 이웃 관계가 중요한 문제에서 사용(ex. 이미지 처리)

### 🔎 DAG(Directed Acyclic Graph)
- DAG는 cycle이 없는 방향성 그래프
![image](https://github.com/user-attachments/assets/2f827e08-eb26-4d02-8dd4-ed23e9651cca)

### 🔎 Conditional Vs. Marginal independence
- **Marginal independence(주변 독립성)**
  - P(A,B)=P(A)⋅P(B)
  - 주변 독립성은 다른 변수가 없을 때도 **두 변수가 완전히 독립적**임을 의미하며, **조건 없이 독립적**
  - 주사위 던지기: 두 개의 주사위를 던지는 경우, 첫 번째 주사위(A)와 두 번째 주사위(B)의 결과는 주변적으로 독립적
  - 두 개의 주사위가 서로 영향을 미치지 않기 때문에, A와 B는 서로 독립적
- **Conditional independence(조건부 독립성)**
 - P(A,B∣C)=P(A∣C)⋅P(B∣C)
 - 조건부 독립성은 제3의 변수(조건)가 주어졌을 때 두 변수 간의 독립성을 의미하며, **다른 변수가 존재할 때만 두 변수가 독립적**
 - 음성 인식에서: "날씨"라는 변수(C)가 주어졌을 때, "우산을 쓰는지(A)"와 "비가 오는지(B)"는 조건부 독립적
 - 날씨가 비가 오는 날이라면, 이미 날씨 정보를 알기 때문에 우산과 비의 관계는 독립적
 - conditional independent 하다고 independent 한 것은 아니다
- **관계**
  - 조건부 독립일 수 있지만 주변 독립이 아닐 수 있음
    - "환자의 나이(A)"와 "환자가 특정 질병을 가지고 있는지(B)"는 주변적으로 독립적
    - 하지만, "흡연 여부(C)"가 주어졌을 때, 나이와 질병 여부는 조건부 독립적
    - 흡연 여부를 알고 나면, 나이와 질병 간의 관계는 없어질 수 있기 때문 

### 🔎 Bayesian networks
- Bayes 네트워크, 신뢰 네트워크 또는 의사결정 네트워크라고도 함
- Conditional independence(조건부 독립성)
- 변수 집합과 변수의 조건부 종속성을 나타내는 확률적 그래픽 모델
- 결합확률 : p(a,b,c)=p(c|a,b)p(a,b)
- 만약 $\(x_i\), \(i = 1, \ldots, n\)$ 가 이항(binary) 변수라면, $\(p(x_i | x_{i-1}, \ldots, x_1)\)은 \(2^{i-1}\)$개의 모수가 필요
  - 결론적으로 조건부 확률에서 조건으로 주어지는 변수가 많아지면 모수가 **지수적으로 증가**
  - 영향을 주는 변수만을 골라서 조건으로 준다면, 고려해야하는 경우 적어지므로 필요한 모수가 적어질 것
- 예를 들어 베이지안 네트워크는 질병과 증상 간의 확률적 관계를 나타낼 수 있습니다
  - 증상이 주어지면 네트워크를 사용하여 다양한 질병의 존재 확률을 계산 
![image](https://github.com/user-attachments/assets/7b8c42fb-190d-4ea6-9c0e-fdfd087ea5b7)

- 활성 스프링클러 또는 비 두 가지 이벤트로 인해 잔디가 젖을 수 있음
  - 비는 스프링클러 사용에 직접적인 영향을 미침(즉, 비가 올 때 일반적으로 스프링클러는 작동하지 않음)
  - Pr(G,S,R) = Pr(G|S,R)Pr(S|R)Pr(R)
    - G = Grass wet(true/false)
    - S = Sprinkler turned on(true/false)
    - R = Raining(true/false)
    - Rain이 True일 때, Grass Wet이 True일 확률은 높음
    - Sprinkler와 Rain은 서로 독립적이지 않으며, 둘 다 Grass Wet에 영향을 미침
  - 잔디가 젖어 있을 때 비가 올 확률은?
    - $\[\Pr(R = T \mid G = T) = \frac{\Pr(G = T, R = T)}{\Pr(G = T)} = \frac{\sum_{x \in \{T,F\}} \Pr(G = T, S = x, R = T)}{\sum_{x,y \in \{T,F\}} \Pr(G = T, S = x, R = y)}\]$
    - $\[Pr(R = T \mid G = T) = \frac{0.00198_{TTT} + 0.1584_{TFT}}{0.00198_{TTT} + 0.288_{TFF} + 0.1584_{TFT} + 0.0_{TFF}} = \frac{891}{2491} \approx 35.77\%.\]$
  - 결합확률 계산 예시 $\[Pr(G = T, S = T, R = T) = Pr(G = T \mid S = T, R = T) \times Pr(S = T \mid R = T) \times Pr(R = T)\]$
    - $\[= 0.99 \times 0.01 \times 0.2 = 0.00198\]$ 

### 🔎 CRFs(conditional random fields)
- 순차 데이터와 의존 관계를 모델링하는 확률 그래프 모델
- CRF는 주로 자연어 처리(NLP) 분야에서 텍스트 데이터의 특정 부분에 라벨을 할당하는 태깅 작업에 활용(예: POS 태깅, 개체명 인식)

### 🔎 HMM의 State model
- **은닉 상태(hidden states)** 와 관측값(observed values) 간의 관계를 모델링하는 것
- 여기서 상태는 직접 관측할 수 없는 시스템의 내적 상태

- 이미지 처리에서 사용되며, 데이터 간의 상관관계를 잘 반영할 수 있다는 장점
- 입력 시퀀스가 주어졌을 때 각 항목에 가장 적합한 라벨을 예측하는 데 사용
- **마르코프 랜덤 필드(MRF)** 의 확장으로, 조건부 확률을 사용해 전역적인 최적화를 수행
- Undirected Graph, 주어진 관측값에 대해 조건부 확률을 모델링

### 🔎 생성적 접근법 Vs. 판별적 접근법
- **생성적 접근법 (Generative Approaches)**
  - 주어진 입력 데이터(예: 이미지)로부터 데이터셋 내의 다른 데이터를 생성하거나 재구성할 수 있는 모델을 학습시키는 데 초점
  - 데이터의 분포를 모델링하여 데이터를 생성하는 능력을 학습하는 것
  - 데이터와 레이블의 결합 분포를 학습하여, 주어진 입력에 대한 출력뿐만 아니라 데이터 자체를 생성할 수 있는 모델을 학습
  - 데이터 분포를 완벽히 모델링하므로, 데이터를 생성하거나 숨겨진 구조를 분석하는 데 적합
  - 예시: 나이브 베이즈, HMM(은닉 마르코프 모델)
  - Generative Adversarial Networks (GANs): 실제 데이터와 구별할 수 없는 데이터를 생성하도록 설계된 모델
  - Variational Autoencoders (VAEs): 입력 데이터를 잠재 공간의 낮은 차원의 표현으로 압축후, 이 표현에서 원본 데이터를 재생성하는 모델
  - Autoencoders: 입력을 잠재 표현으로 인코딩하고 다시 입력 데이터로 디코딩
- **판별적 접근법 (Discriminative Approaches)**
  - 라벨이 주어지지 않은 데이터로부터 표현을 학습
  - 주어진 입력에서 직접적으로 레이블을 예측하는 것
  - 정확하게 구분하는 데 초점을 맞추므로 예측 성능이 우수하며, 복잡한 의존 관계를 더 잘 학습할 수 있음
  - 로지스틱 회귀, SVM, 조건부 랜덤 필드(CRF)
  - Self-supervised Learning: 데이터 자체에서 라벨을 생성하고, 이를 예측하는 작업을 통해 유용한 표현을 학습
  - Contrastive Learning: 비슷한(긍정적인) 예시들이 서로 가까워지고, 다른(부정적인) 예시들이 멀어지도록 표현을 학습
  - Pretext Tasks: 이미지의 일부분을 회전시키거나, 퍼즐 조각을 재배열하는 등의 작업을 수행하도록 학습

### 🔎 Long-range Sequential structure
- 긴 시퀀스 데이터에서 시간적, 순차적 관계가 멀리 떨어져 있음에도 불구하고 의미 있는 상호작용이나 의존성이 존재하는 구조
- 언어 모델링: 문장의 앞부분과 뒷부분이 멀리 떨어져 있어도, 문맥을 이해하기 위해서는 앞부분의 정보를 기억해야 하는 경우
- 음성 신호 처리: 음성 데이터에서 첫 부분의 발음이 나중의 발음과 연관되어 있거나 의미를 가지는 상황
- 이러한 구조를 학습하기 위해 LSTM이나 Transformer 같은 모델이 효과적

### 🔎 Localized classification
- 데이터의 일부 영역이나 구간에 집중하여 클래스(레이블)를 예측하는 방법을 의미
- 전체 데이터가 아닌 특정 구간 또는 지역적 정보에 기반해 분류 작업을 수행하는 것
- 이미지 처리에서 특정 객체의 주변 픽셀 정보만을 활용하여 그 객체가 어떤 카테고리에 속하는지 판단하는 경우
- 자연어 처리에서 문장의 특정 구간에 기반해 그 구간의 의미를 분류하는 작업
