- 마르코프 랜덤 필드 공부
- HMM 모델 공부

# Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks 논문 리뷰

## 0. Abstract
> 많은 sequence learning tasks는 **노이즈가 있고 segmentation 되어있지 않은 인풋 데이터로부터 라벨의 sequence를 예측** 해야한다.
> 예를 들어, 음성 인식에서는 음향 신호를 단어 또는 단어의 하위 단위로 변환한다.
> **순환 신경망(RNN)** 은 이러한 작업에 매우 적합한 **강력한 시퀀스 학습기**입니다. 
> 하지만, **RNN은 사전 분할된 학습 데이터가 필요**하고, **출력을 레이블 시퀀스로 변환하기 위한 후처리가 필요**하기 때문에 적용이 제한적이었다.
> 이 논문은 **RNN이 분할되지 않은 시퀀스에 직접 레이블을 지정하도록 학습**하는 새로운 방법을 제시하여 두 가지 문제를 모두 해결합니다.
> **TIMIT 음성 말뭉치**에서 수행한 실험은 이 방법이 기존 HMM과 하이브리드 HMM-RNN보다 우수한 성능을 발휘함을 보여준다.

- 분할되지 않은 시퀀스 데이터 레이블링 문제를 다룸
- CTC 방법을 통해 RNN을 사전 데이터 분할 없이 사용할 수 있음
- 실험 결과 CTC가 기존 모델들(HMM, 하이브리드 모델)보다 우수한 성능을 발휘

  
## 1. Introduction
> 분할되지 않은 시퀀스 데이터에 레이블을 지정하는 것은 실제 시퀀스 학습에서 흔히 있는 문제입니다. 특히, 잡음이 있는 실수 값의 입력 스트림에 문자나 단어와 같은 개별 레이블 문자열로 주석이 달린 지각 작업(예: 필기 인식, 음성 인식, 제스처 인식)에서 일반적입니다.
> 
> 현재 HMM(은닉 마르코프 모델), CRF(조건부 랜덤 필드) 등과 같은 **그래프 기반 모델들이 시퀀스 레이블링의 주요 프레임워크**로 사용되고 있습니다. 이러한 접근 방식은 많은 문제에서 성공적이었지만, 몇 가지 **한계점**이 있습니다: 1. HMM의 상태 모델을 설계하거나 CRF의 입력 특징을 선택하는 데 상당한 양의 과제 특화 지식이 필요합니다. 2. 의존성 가정을 명시적으로 설정해야 하는데, 이는 HMM의 경우 관측치가 독립적이라는 가정이 필요하며, 이를 통해 추론이 가능하게 됩니다. 3. 표준 HMM의 경우, 시퀀스 레이블링이 **판별적(discriminative)** 인데도 불구하고 생성적(generative) 학습 방식을 사용해야 합니다.
>
> 반면에 **RNN(순환 신경망)** 은 데이터의 입력과 출력 표현을 선택하는 것 외에는 사전 지식이 필요하지 않습니다. RNN은 판별적으로 학습할 수 있고, 내부 상태를 통해 시간 시리즈 데이터를 강력하게 모델링할 수 있는 일반적인 메커니즘을 제공합니다. 또한 시간적 및 공간적 노이즈에 강한 경향이 있습니다. 그러나, 현재까지 RNN을 시퀀스 레이블링에 직접 적용하는 것은 불가능했습니다. 그 이유는 표준 신경망 목적 함수가 시퀀스의 각 지점에 독립적으로 정의되기 때문입니다. 즉, RNN은 독립적인 레이블 분류를 위한 학습만 가능했습니다. 따라서 학습 데이터가 사전 분할되어야 하고, 최종 레이블 시퀀스를 생성하기 위해 네트워크 출력을 후처리해야 했습니다.
>
> 현재로서 RNN을 시퀀스 레이블링에 가장 효과적으로 사용하는 방법은 HMM과 결합하는 하이브리드 방식입니다. 하이브리드 시스템은 HMM을 통해 데이터의 장기적인 시퀀스 구조를 모델링하고, 신경망을 통해 국소적인 분류를 제공합니다. HMM은 학습 중에 자동으로 시퀀스를 분할하고, 네트워크 분류 결과를 레이블 시퀀스로 변환합니다. 그러나 하이브리드 시스템은 앞서 언급한 HMM의 단점을 그대로 가지고 있으며, RNN이 시퀀스 모델링에서 제공할 수 있는 잠재력을 완전히 활용하지는 못합니다.
>
> 이 논문은 **RNN을 이용한 시퀀스 레이블링의 새로운 방법을 제시**합니다. 이 방법은 **사전 분할된 학습 데이터와 후처리된 출력값의 필요성을 제거**하고, **시퀀스의 모든 측면을 하나의 네트워크 아키텍처 내에서 모델링**합니다. 이 방법의 기본 아이디어는 네트워크 출력을 주어진 **입력 시퀀스에 대해 모든 가능한 레이블 시퀀스에 대한 확률 분포로 해석**하는 것입니다. 이 확률 분포를 바탕으로, 올바른 라벨링의 확률을 극대화하는 목적 함수를 도출할 수 있습니다. 이 목적 함수는 미분 가능하기 때문에, **시간 역전파(Backpropagation Through Time, BPTT)** 를 통해 네트워크를 학습할 수 있습니다.
>
> 이 논문에서는 분할되지 않은 데이터 시퀀스에 레이블을 지정하는 작업을 **시간 분류(Temporal Classification)** 라고 하고, RNN을 사용한 이 방법을 **연결 시간 분류(CTC: Connectionist Temporal Classification)** 라고 부르겠습니다. 반면, 입력 시퀀스의 각 **시간 단계(프레임)** 별 독립적으로 레이블링하는 작업은 **프레임 단위 분류(Framewise Classification)** 라고 하겠습니다.
>
> 다음 섹션에서는 시간 분류(Temporal Classification)를 위한 수학적 형식화를 제공하고, 이 논문에서 사용한 오류 측정 기준을 정의합니다. 3장에서는 RNN을 시간 분류기로 사용할 수 있게 해주는 출력 표현을 설명합니다. 4장에서는 CTC 네트워크 학습 방법을 설명하고, 5장에서는 TIMIT 음성 말뭉치에서 CTC와 하이브리드 시스템 및 HMM 시스템을 비교한 결과를 제시합니다. 6장에서는 CTC와 다른 시간적 분류기의 주요 차이점을 논의하며, 7장에서 논문을 결론짓습니다.

- **배경 및 문제 정의**
  - 음성 인식이나 필기 인식과 같은 작업에서 순차적으로 변화하는 데이터에서 레이블을 예측하는 것이 핵심
  - 분할되지 않은 시퀀스 데이터에 레이블을 붙이는 작업은 필기 인식, 음성 인식 등에서 자주 발생
  - Unsegment sequence를 라벨링 하는 것은 sequence learning에서 흔한 일 - HMM, CRF등의 프레임 워크 활용

- **그래프 기반 모델들의 한계점**
  - task를 특정하기 위한 **상당한 양의 정보가 필요**
  - 데이터의 **독립성 가정을 명시적으로 설정**해야함
  - **HMM은 generative model**이기에, sequence labeling에서도 **discriminative model적으로 접근하지 못함**

- **RNN의 가능성과 한계**
  - RNN은 **사전 지식 없이도 입력과 출력의 특성을 학습**할 수 있으며, 시간 시리즈 데이터를 다루는 데 적합
  - 내부 상태를 통해 데이터를 순차적으로 처리하며, **시간적/공간적 노이즈에 강한 모델**
  - 그러나 RNN은 현재까지 시퀀스 레이블링에 직접 사용되지 못했음
  - 이는 **표준 신경망 목적 함수가 시퀀스의 각 지점에 대해 독립적으로 정의**되기 때문으로, RNN이 학습을 위해 **사전 분할된 데이터와 후처리가 필요했음**

- **HMM-RNN 하이브리드**
  - 최근까지, **sequence labeling에 쓰이는 RNN의 가장 효과적인 사용**은 HMM-RNN 하이브리드 모델
  - 하이브리드 방식으로 HMM의 시퀀스 구조 모델링과 RNN의 지역적 분류 기능을 결합해 사용하나, **HMM의 기존 한계로 인해 RNN의 잠재력을 완전히 활용하지 못함**

- **CTC의 제안 및 목표**
  - 본 논문에서는 **연결 시간 분류(CTC)** 방식을 제안하여, 사전 데이터 분할 없이 RNN이 시퀀스 데이터를 처리할 수 있도록 함
  - 입력 시퀀스에 대한 모든 가능한 레이블 시퀀스의 확률 분포를 사용하여, 올바른 레이블링의 확률을 최대화하는 목적 함수를 통해 학습을 수행
  - 이 목적 함수는 미분 가능하므로, **시간 역전파(BPTT)** 를 통해 효율적으로 학습 가능
  - **Temporal Classification(시간 분류)** : 분할되지 않은 데이터 시퀀스에 레이블을 지정하는 작업
  - **CTC(connectionist Temporal Classification : 연결 시간 분류)** : RNN을 사용한 방법
  - **Framewise Calssification(프레임 단위 분류)** : 입력 시퀀스의 각 시간 단계(프레임)별 독립적으로 라벨링하는 작업


## 2. Temporal Classification
> S는 고정된 분포 **$\[D_{X×Z}\]$** 로부터 추출된 훈련 예시들의 집합이라고 하자. Input space **$\[X = (ℝ^m)^\*\]$** 은 m 차원의 실수 벡터 시퀀스들의 집합이다. Target space **Z = L∗**은 유한한 알파벳 L 위에서 이루어진 시퀀스들의 집합이다. 일반적으로, 우리는 L∗의 요소를 레이블 시퀀스 또는 레이블링이라고 부른다. S의 각 예시는 (x, z)라는 쌍으로 이루어진 시퀀스들로 구성되어 있다. Target 시퀀스 **$\[z = (z_1, z_2, ..., z_U)\]$** 는 입력 시퀀스 **$\[x = (x_1, x_2, ..., x_T)\]$** 보다 길지 않으며, 즉 **U ≤ T** 이다. **입력 시퀀스와 Target 시퀀스의 길이가 일반적으로 동일하지 않기 때문에 이들을 미리 정렬할 방법은 없다.**
>
> 목표는 S를 사용하여 **시간 분류기(temporal classifier)** h : X → Z를 훈련하여, 이전에 보지 못한 입력 시퀀스에 대해 일부 작업에 특화된 오류 측정값을 최소화하는 방식으로 분류할 수 있게 하는 것이다.
>
> **2.1. Label Error Rate**
> 이 논문에서는 다음과 같은 오류 측정 방법에 관심이 있습니다. **$\[S' ⊂ D_{X×Z}\]$** 인 테스트 집합이 주어졌을 때, 시간 분류기 h의 라벨 오류율(LER)을 **$\[S'\]** 에서의 분류 결과와 목표 사이의 정규화된 편집 거리로 정의합니다.
> **$\[LER(h, S') = \frac{1}{Z} \sum_{(x, z) \in S'} ED(h(x), z)\]$**
> 여기서 **Z**는 **S'**에 있는 목표 레이블들의 총 길이. **ED(p, q)** 는 두 시퀀스 **p**와 **q** 사이의 편집 거리(Edit Distance), 즉, **p를 q로 바꾸는 데** 필요한 삽입, 대체, 삭제의 **최소 횟수**입니다. 이것은 음성 인식 또는 필기 인식과 같이 작성 오류율을 최소화하려는 작업에서 자연스러운 측정 방법입니다.

- S : 고정된 분포 **$\[D_{X×Z}\]$** 로부터 추출된 훈련 예시들의 집합
  -  (x, z)라는 쌍으로 이루어진 시퀀스들로 구성
- S' : 테스트용 데이터셋 (모델 성능을 측정)
  - 고정된 분포 **$\[D_{X×Z}\]$** 의 부분집합, 즉 X와 Z의 곱집합에 속하는 데이터들로 이루어진 테스트 데이터셋을 의미 
- Input space :  **$\[X = (ℝ^m)^\*\]$** - m 차원의 실수 벡터 시퀀스들의 집합으로 구성
  - Input 시퀀스 :  **$\[x = (x_1, x_2, ..., x_T)\]$**
- Target space : **Z = L∗** - 유한한 알파벳 L 위에서 이루어진 레이블 시퀀스들의 집합
  - Target 시퀀스 :  **$\[z = (z_1, z_2, ..., z_U)\]$**
- **U ≤ T**
  - 입력 시퀀스와 Target 시퀀스의 **길이가 일반적으로 동일하지 않기에 미리 정렬할 방법 없음**
- 목표 : S를 사용해 시간 분류기 h : X → Z를 훈련시킴으로써, 오류 측정값을 최소화하는 방식으로 시퀀스를 분류하는 것
  - 오류 측정 방법 -> **(1)** **$\[LER(h, S') = \frac{1}{Z} \sum_{(x, z) \in S'} ED(h(x), z) 
 \tag{1}\]$**
  - Z는 S'에 있는 목표 레이블들의 총 길이
  - **ED(p, q)** 는 두 시퀀스 p와 q 사이의 **편집 거리(Edit Distance)** 를 의미하며, 이는 p를 q로 바꾸기 위해 필요한 삽입, 삭제, 대체 작업의 최소 횟수
  - LER는 음성 인식이나 필기 인식 작업에서 작성 오류율을 최소화하는 자연스러운 측정 방법


## 3. Connectionist Temporal Classification
> 이 섹션에서는 **순환 신경망(Recurrent Neural Network, RNN)** 을 CTC로 사용할 수 있게 해주는 출력 표현을 설명합니다. 핵심 단계는 **네트워크 출력을 라벨 시퀀스에 대한 조건부 확률 분포로 변환**하는 것입니다. 네트워크는 주어진 입력 시퀀스에 대해 **가장 확률이 높은 레이블 시퀀스를 선택**함으로써 분류기로 사용될 수 있습니다.
>
> **3.1. From Network Outputs to Labellings**
> CTC 네트워크는 **소프트맥스 출력층(Softmax Output Layer)** 을 사용하며, 이 층은 라벨의 수보다 하나 더 많은 유닛을 가집니다. 첫 번째 **|L|** 개의 유닛의 활성값은 특정 시간에서 해당 라벨을 관찰할 확률로 해석됩니다. 추가된 유닛의 활성값은 ‘빈(blank)’ 라벨 또는 레이블이 없는 것을 관찰할 확률입니다. 이 출력들은 주어진 입력 시퀀스에 대해 모든 가능한 라벨 시퀀스와의 정렬 방식들에 대한 확률을 정의합니다. 특정 라벨 시퀀스의 전체 확률은 다양한 정렬 방식에 대한 확률을 모두 합산하여 구할 수 있습니다.
>
> 좀 더 공식적으로 설명하자면, 길이가 T인 입력 시퀀스 x에 대해 m개의 입력과 n개의 출력을 가지는 가중치 벡터 w로 정의된 RNN을 다음과 같이 나타낼 수 있습니다 ->  **$\[N_w : (ℝ^m)^T \to (ℝ^n)^T\]$** 네트워크 출력 시퀀스를 **$\[y = N_w(x)\]$** 로 정의하고, **$\[y^t_k\]$** 는 시간 t에서 출력 유닛 k의 활성값을 의미합니다. 이때 **$\[y^t_k\]$** 는 t 시점에서 라벨 k가 관찰될 확률로 해석됩니다. 이는 **\( L' = L ∪ \{blank\} \)** 에서 길이 T인 시퀀스의 집합 **\( L'^T \)** 에 대한 확률 분포를 정의합니다.
> **(2)** **$\[p(\pi|x) = \prod_{t=1}^{T} y^t_{\pi_t}, \quad \forall \pi \in L_0^T. \tag{2}\]$**
>
> 이제부터 우리는 **$\( L'^T \)$** 의 요소들을 **paths** 라고 부르고, 이를 **𝜋** 로 나타냅니다.
>
> 수식 (2)에서 암시하는 것은 네트워크의 출력이 내부 상태를 기준으로 조건부 독립적이라는 가정입니다. 즉, 각 시간에서의 출력이 서로 독립적이라는 것입니다. 이는 출력 층에서 자기 자신 또는 네트워크로 피드백 연결이 존재하지 않아야 한다는 요구 사항을 통해 보장됩니다.
>
> 다음 단계는 여러 개의 경로를 하나의 레이블 시퀀스로 매핑하는 함수 B를 정의하는 것입니다. **$\( B : L'^T \to L^{\leq T} \)$** 여기서 **$\( L^{\leq T} \)$** 는 길이가 T 이하인 모든 가능한 레이블 시퀀스를 나타냅니다. 즉, 원래의 알파벳 L에 대한 시퀀스들로 이루어진 집합입니다. 이 매핑은 단순히 경로에서 모든 **blank(빈칸)** 과 반복된 레이블을 제거함으로써 이루어집니다. 예를 들어: B(a - ab-) = B(-aa - abb) = aab. 이 과정은 네트워크가 빈 라벨에서 라벨로, 또는 하나의 라벨에서 다른 라벨로 전환할 때 새로운 라벨을 출력하는 것과 같습니다.**(figure 1)** 마지막으로, 우리는 함수 B를 사용해 조건부 확률을 정의합니다. 주어진 라벨링 **$\( l \in L^{\leq T} \)$** 의 조건부 확률은 그 라벨링에 해당하는 모든 경로의 확률을 합한 값입니다:
> **(3)** **$\[p(l|x) = \sum_{\pi \in B^{-1}(l)} p(\pi|x). \tag{3}\]$**
>
> **[Figure 1]**  <img width="921" alt="image" src="https://github.com/user-attachments/assets/4bf00dca-a95d-4a3c-932e-f1e0faaca287"> <br/>
> 그림 1. Framewise 네트워크와 CTC 네트워크가 음성 신호를 분류하는 방법
> 그림에서 음성 신호는 시간에 따라 변화하는 **파형(waveform)** 으로 표현되어 있습니다. 이 음성 신호를 처리하여 네트워크는 특정 시간에 **음소(phonemes)** 가 나올 확률을 계산합니다. **Framewise 네트워크**: 음성 신호의 각 시간 프레임마다 해당 음소가 나올 확률을 예측합니다. 각 프레임에서 어떤 음소가 나올 확률을 개별적으로 계산하기 때문에, 연속된 프레임에서 발생할 확률이 급격하게 변하거나 혹은 잘못된 타이밍으로 예측될 수 있습니다. 예를 들어, 특정 음소가 정확히 맞았더라도, 세그먼트 경계(수동으로 설정된 분할 라인)와 맞지 않으면 오류가 발생합니다. 또한 음소가 붙어 있을 때(예: 'dcl'과 'd'가 같이 나올 때), framewise 네트워크는 이를 따로 예측하는 반면, CTC 네트워크는 이를 함께 예측하는 경향이 있습니다. **CTC 네트워크**: CTC는 전체 시퀀스를 처리한 후, 특정 시간 프레임에서 어떤 음소가 나왔는지에 대한 확률을 추론합니다. 여기서 **'blank'** 라는 출력이 존재하는데, 이는 해당 시간에 아무 음소도 출력되지 않았다는 것을 의미합니다. 이 네트워크는 음소 간 경계를 명확히 맞추는 것이 아니라 음소 자체의 순서에 초점을 맞추어 처리합니다. 또한, CTC는 framewise 방식에 비해 더 연속적이고 명확한 출력을 제공합니다.
>
> **3.2. Constructing the Classifier**
> 위의 수식에 따라, 분류기의 출력은 입력 시퀀스에 대해 가장 높은 확률을 가지는 레이블이 되어야 합니다:
> **$\[h(x) = \arg \max_{l \in L^{\leq T}} p(l|x)\]$**
> 즉, 주어진 입력 시퀀스에 대해, 가능한 레이블 시퀀스 중에서 가장 높은 확률을 가지는 레이블을 선택합니다.
> 이 과정은 **HMM(Hidden Markov Models)** 에서 **디코딩(decoding)** 이라고 부르는 작업입니다. 불행히도, 우리 시스템에 대한 일반적인, 실용적인 디코딩 알고리즘은 알려져 있지 않습니다. 하지만 실제로 좋은 결과를 얻을 수 있는 두 가지 근사적인 방법이 있습니다.
>
> 첫 번째 방법은 **Best Path Decoding(최적 경로 디코딩)** 으로, 가장 높은 확률을 가진 경로가 가장 높은 확률의 레이블에 대응한다는 가정에 기반하고 있습니다:
> **(4)** $\[h(x) \approx B(\pi^*)\]$
> where: $\[\pi^* = \arg \max_{\pi \in N_t} p(\pi|x)\]$
> 여기서, **𝜋***는 가장 높은 확률의 경로로 정의됩니다
> **Best Path Decoding(최적 경로 디코딩)**은 계산하기 매우 간단합니다. 왜냐하면 𝜋∗는 **각 시간 단계에서 가장 활성화된 출력을 연결**한 것이기 때문입니다. 즉, 가장 높은 확률을 가진 라벨들을 단순히 이어 붙이면 됩니다. 그러나, 이 방법은 항상 **가장 높은 확률의 레이블 시퀀스를 찾는 것을 보장하지는 않**습니다. 이는 최적의 레이블링을 보장하지 못할 수 있다는 단점이 있습니다.
>
> **두 번째 방법 (Prefix Search Decoding, 접두사 탐색 디코딩)** 은 section 4.1에서 다룬 전방-후방 알고리즘을 수정하여 라벨링 접두사의 확장에 따른 확률을 효율적으로 계산하는 것에 기반하고 있습니다. 이 방법은 각 라벨링 접두사(시작 부분)를 확장해 나가면서 각 접두사의 확률을 계산해나갑니다.(figure 2)
> 충분한 시간이 주어지면, Prefix Search Decoding은 항상 가장 높은 확률의 레이블링을 찾을 수 있습니다. 그러나 이 방법은 입력 시퀀스 길이가 길어질수록 확장해야 하는 접두사의 수가 기하급수적으로 증가하는 문제가 있습니다. 즉, 시퀀스가 길어질수록 이 방법이 확장해야 할 접두사(가능한 라벨의 시작 부분)가 많아지므로, 계산량이 매우 커질 수 있습니다. 다행히, 출력 확률 분포가 최대값 주변에 충분히 집중되어 있으면, 이 방법은 합리적인 시간 내에 완료될 수 있습니다. 그러나 이 논문에서 실험한 경우에는 추가적인 휴리스틱(경험적 규칙)이 필요하여, 이 방법을 실용적으로 사용할 수 있도록 했습니다.
>
> **[Figure 2]**   <img width="353" alt="image" src="https://github.com/user-attachments/assets/c1b564d1-b6ba-4b19-98b7-6d83dd3e981c"> <br/>
> 그림 2. Prefix Search Decoding(접두사 탐색 디코딩) 각 노드는 **'e'** 로 끝나거나, 그 부모 노드에서 접두사를 확장합니다. **확장하는 노드 위의 숫자는 해당 접두사로 시작하는 모든 레이블링의 총 확률**을 나타냅니다. 끝나는 노드(즉, 'e' 노드) 위의 숫자는 그 노드에 있는 단일 레이블링의 확률을 나타냅니다. 예를 들어, 노드 'X' 위의 0.7은 **'X'** 로 시작하는 모든 레이블링의 확률의 총합을 의미합니다. 매 단계마다, 가장 높은 확률을 가진 접두사를 확장하면서 탐색합니다. 예를 들어, 노드 'X'에서 시작해 'X'로 시작하는 레이블링을 확장한 다음, 다음 노드(확장된 라벨)에 대한 확률을 계산해나가는 방식입니다. 탐색은 **단일 레이블링(여기서는 'XY')** 의 확률이 남은 다른 모든 접두사보다 더 높은 확률을 가질 때 종료됩니다.
>
> 훈련된 CTC 네트워크의 출력은 '빈(blank)' 레이블로 강하게 예측되는 구간들로 나뉘고, 그 사이에 스파이크처럼 특정 라벨들이 나타나는 형태를 띠는 경향이 있습니다(그림 1 참고). 이를 바탕으로, 우리는 출력 시퀀스를 구간으로 나누는데, 이 구간은 매우 높은 확률로 빈 레이블로 시작하고 끝나는 구간입니다. 이 구간을 나누는 방법은 빈 레이블의 확률이 일정 임계값을 넘는 지점을 경계점으로 설정하여 나누는 것입니다. 그런 다음, 각 구간에 대해 개별적으로 가장 높은 확률의 라벨을 계산하고, 이를 연결하여 최종 분류 결과를 얻습니다.
>
> 실제로, Prefix Search는 이 방법(휴리스틱)과 잘 작동하며, 보통 Best Path Decoding보다 더 좋은 성능을 발휘합니다. 하지만 일부 경우에는 실패할 수 있습니다. 예를 들어, 같은 라벨이 구간의 양쪽에서 약하게 예측될 때는 실패할 수 있습니다.


- CTC 네트워크의 소프트맥스 출력층은 각 시간 단계에서 라벨이 나올 확률을 계산
  - 라벨의 수보다 하나 더 많은 유닛을 가지는데, 이 "하나 더 많은 유닛"은 바로 빈(blank) 유닛
  - 즉, 어떤 라벨도 출력되지 않을 확률
- CTC는 전체 시퀀스에 대한 여러 가능한 라벨 시퀀스를 모두 고려해서 가장 적합한 라벨 시퀀스를 선택
  - 이 확률들을 모두 합산하여, 특정 라벨 시퀀스가 발생할 전체 확률을 계산 
- RNN의 정의
  -  **$\[N_w : (ℝ^m)^T \to (ℝ^n)^T\]$**
    -  길이가 T인 입력 시퀀스 x에 대해 m개의 입력과 n개의 출력을 가지는 **가중치 벡터 w**로 정의된 RNN
    -  **입력 차원 𝑚**에서 **출력 차원 𝑛**으로 매핑되는 RNN의 연산
  
- 네트워크 출력 시퀀스
  -  **$\[y = N_w(x)\]$**
    -  **$\[y^t_k\]$** 는 시간 t에서 출력 유닛 k의 활성값을 의미
    -  해당 시간에 **라벨 𝑘**가 관찰될 확률
  - RNN이 입력 시퀀스 x를 처리한 후 출력 시퀀스 y를 생성
  - 이 출력값은 각 시간 단계 t에서 특정 라벨이 선택될 확률을 의미
    
- 확률 분포
  -  **\( L' = L ∪ \{blank\} \)**
    - **알파벳 𝐿**과 **'blank'** 레이블을 포함하는 **길이 𝑇**인 시퀀스들의 집합 **\( L'^T \)** 에 대한 확률 분포를 정의
  - **$\[p(\pi|x) = \prod_{t=1}^{T} y^t_{\pi_t}, \quad \forall \pi \in L_0^T. \tag{2}\]$**
    - 각 시퀀스 π에 대한 확률은 모든 시간 단계에서 해당 라벨이 관찰될 확률의 곱으로 정의
    - 각 시간 단계에서의 확률이 서로 독립적이라는 가정
      - **서로 독립적인 사건들의 결합 확률은 각각의 사건의 확률을 곱한 값으로 표현**
      - 즉, 이전 시간 단계의 출력이 현재 시간 단계의 출력에 영향을 주지 않는다는 것
      - 이러한 독립성 때문에, 전체 시퀀스의 확률은 각 시간 단계에서 출력된 레이블의 개별 확률을 모두 곱한 값으로 표현 
  - 전체 시퀀스의 확률은 각 시간 단계에서 발생하는 라벨 확률들을 모두 곱하여 계산
- CTC는 모든 가능한 라벨 시퀀스(경로)를 처리해야 함
  - 경로(𝜋)는 각 시간 단계에서 가능한 라벨들의 순서를 의미
  - 하지만, 실제로 중요한 건 중복된 라벨이나 빈(blank) 라벨을 제거하고 실제 의미 있는 라벨 시퀀스를 얻는 것
  - B(a−ab−)=B(−aa−abb)=aab: 빈 라벨과 중복된 라벨을 제거하고 중요한 라벨만 남김
- 함수 B의 역할
  - 여러 경로를 하나의 라벨 시퀀스로 매핑하는 함수
- 최종 확률 계산
  - **$\[p(l|x) = \sum_{\pi \in B^{-1}(l)} p(\pi|x). \tag{3}\]$**
  - 특정 라벨 시퀀스 **𝑙**에 해당하는 모든 경로들에 대해 각 경로의 확률을 모두 더한 값이 그 라벨 시퀀스의 전체 확률
- 그림1
  - Framewise 네트워크는 각 시간 프레임에 대해 독립적으로 라벨을 예측하는 반면, CTC 네트워크는 시퀀스 전체를 고려하여 라벨을 예측
  - Framewise 네트워크는 각 프레임에서 정확한 음소를 예측할 수 있지만, 경계가 맞지 않는 경우가 발생가능
  - CTC 네트워크는 음소의 경계보다는 전체적인 음소 순서와 흐름에 초점을 맞추어 더 자연스럽고 일관된 라벨을 예측 
- **CTC 네트워크가 어떻게 시퀀스 데이터에서 가장 적합한 라벨링을 찾는지에 대한 두 가지 방법**
  - **Best Path Decoding (최적 경로 디코딩)**
    - 가장 간단한 방식
    - 각 시간 단계에서 가장 높은 확률의 라벨을 선택하여 이를 연결
    - 단점: 항상 가장 높은 확률의 라벨링을 보장하지는 못하며, 중간에 중요한 라벨을 놓칠 수 있는 상황이 생길 수 있음
  - **Prefix Search Decoding (접두사 탐색 디코딩)**
    - 각 시간 단계에서 확률이 높은 라벨뿐만 아니라 **라벨의 접두사(시작 부분)** 까지 고려하여, 시퀀스를 확장해 나가는 방식
    - 모든 가능한 라벨링 접두사를 확장하면서, 그 접두사의 확률을 계산
    - 빈(blank) 레이블로 강하게 예측된 구간들을 기준으로 시퀀스 나누기
    - 각 구간에서 가장 높은 확률의 라벨을 계산하고, 이 라벨들을 연결하여 최종 결과 얻음
    - 장점: 이 방법은 최적의 라벨링을 찾을 가능성이 더 큼
    - 단점: 계산량이 기하급수적으로 늘어남, 시퀀스가 길어질수록 확장해야 할 접두사의 수가 매우 많아지기 때문에


## 4. Training the Network
> 지금까지 우리는 RNN이 CTC에 사용될 수 있도록 하는 출력 표현 방식을 설명했습니다. 이제 경사 하강법을 사용하여 CTC 네트워크를 훈련시키기 위한 목적 함수를 유도할 것입니다.
>
> 이 목적 함수는 최대 우도 원리로부터 유도됩니다. 즉, 이 함수는 최소화할수록 목표 라벨의 로그 우도(log likelihood)를 극대화하게 됩니다. 이는 표준 신경망의 목적 함수들이 따르는 원리와 동일합니다(Bishop, 1995). 주어진 목적 함수와 네트워크 출력에 대한 미분값을 사용하면, **시간을 역으로 거슬러 가며 전파(Backpropagation Through Time)** 하는 방식으로 **가중치의 경사도(gradient)** 를 계산할 수 있습니다. 그런 다음, 네트워크는 현재 신경망에서 사용되고 있는 경사 기반 최적화 알고리즘을 사용해 훈련될 수 있습니다(LeCun et al., 1998; Schraudolph, 2002). 우리는 먼저 최대 우도 함수에 필요한 알고리즘을 설명하는 것으로 시작합니다.
>
> **4.1. The CTC Forward-Backward Algorithm**
> 우리는 개별 라벨링에 대한 조건부 확률 p(l|x) 를 효율적으로 계산하는 방법이 필요합니다. 처음 보면 수식(3) 에서는 모든 경로의 합을 계산해야 하는데, 해당 경로가 매우 많을 수 있어 계산에 부담이 될 것 같습니다.
>
> 다행히도 이 문제는 동적 프로그래밍(dynamic programming) 알고리즘으로 해결할 수 있습니다. 이는 **HMM(Hidden Markov Models)** 의 전방-후방 알고리즘과 유사한 방식입니다. 핵심 아이디어는 특정 라벨링에 대응하는 경로에 대한 합을, 그 라벨링의 접두사(prefixes)에 해당하는 경로에 대한 반복적인 합으로 나누는 것입니다. 이러한 반복적인 계산은 재귀적인 전방(forward)과 후방(backward) 변수를 사용하여 효율적으로 수행될 수 있습니다.
>
> 길이가 r인 시퀀스 q에 대해, 첫 번째와 마지막 P 기호를 **q₁:p** 와 **qᵣ₋ₚ:ᵣ** 로 나타냅니다. 그런 다음, 특정 라벨링 l에 대해 전방 변수 αₜ(s) 를 시간 t에서 **l₁:s**의 전체 확률로 정의합니다. 즉, 이는 다음과 같습니다:
> **(5)**   **$\[\alpha_t(s) = \sum_{\pi \in N^T : B(\pi_{1:t}) = l_{1:s}} \prod_{t'=1}^{t} y_{\pi_{t'}}^{t'}\tag{5}\]$**
> 여기서 αₜ(s) 는 αₜ₋₁(s)와 αₜ₋₁(s-1)로부터 재귀적으로 계산할 수 있습니다.
>
> 출력 경로에서 빈칸(blank)을 허용하기 위해, 빈칸이 앞뒤에 추가되고 라벨 사이사이에 삽입된 수정된 라벨 시퀀스 l' 을 고려합니다. l' 의 길이는 2|l| + 1입니다. 우리는 l' 의 접두사에 대한 확률을 계산할 때, 빈칸과 비-빈칸 라벨 사이의 전환을 모두 허용하며, 또한 서로 다른 비-빈칸 라벨 사이의 전환도 허용합니다. 모든 접두사는 빈칸 b 또는 l 의 첫 번째 기호로 시작할 수 있습니다.
> 이렇게 하면 초기 규칙은 다음과 같이 주어집니다: <br/>
> $\[\alpha_1(1) = y_b^1\]$ <br/>
> $\[\alpha_1(2) = y_{l_1}^1\]$ <br/>
> $\[\\alpha_1(s) = 0, \quad \forall s > 2\]$ <br/>
> 그리고 재귀 관계식은 다음과 같습니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?\alpha_t(s)%20=%20\begin{cases}%20\bar{\alpha}_t(s)%20y_{l_s'}^t%20&%20\text{if%20}%20l_s'%20=%20b%20\text{or}%20l_{s-2}'%20=%20l_s'%20\\%20(\bar{\alpha}_t(s)%20+%20\alpha_{t-1}(s-2))%20y_{l_s'}^t%20&%20\text{otherwise}%20\end{cases}%20\tag{6}" alt="Equation (6)">
 <br/>
> 여기서 : <br/>
> <img src="https://latex.codecogs.com/png.latex?\bar{\alpha}_t(s)%20=%20\alpha_{t-1}(s)%20+%20\alpha_{t-1}(s-1)%20\tag{7}" alt="Equation (7)">
 <br/>
> 이러한 초기설정 및 재귀식을 사용하여 라벨링 접두사에 대한 확률을 계산할 수 있습니다.
>
> **[Figure 3]**  <img width="422" alt="image" src="https://github.com/user-attachments/assets/199e8275-c0e3-42df-89f0-150acdfaab77"> <br/>
> **'CAT'** 이라는 라벨링에 대해 전방-후방 알고리즘(forward-backward algorithm) 을 적용한 과정을 나타냅니다. 검은색 원은 라벨을 나타내고, 흰색 원은 빈(blank) 상태를 나타냅니다. 화살표는 허용되는 전이를 나타내며, 전방 변수는 화살표의 방향을 따라 업데이트되고, 후방 변수는 그 반대 방향으로 업데이트됩니다.
>
> 모든 **s < |l'| − 2(T − t) − 1** 에서 αₜ(s)은 0입니다. 이는 해당 변수들이 주어진 시퀀스를 완료하기에 충분한 시간이 남아있지 않은 상태에 대응하기 때문입니다. 이 경우에 대응되는 그래프의 연결되지 않은 원들이 **figure 3**의 오른쪽 상단에 있습니다. 또한, **모든 s < 1에서도 αₜ(s)은 0**입니다.
>
> l의 확률은 시간 T에서 l'의 마지막 빈칸이 포함된 경우와 포함되지 않은 경우의 전체 확률 합입니다. 즉, 다음과 같이 표현됩니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?p(l|x)%20=%20\alpha_T(l'|l')%20+%20\alpha_T(l'|l'%20-%201)" alt="Equation (8)"/><br/>

>
> 전방 변수(αₜ(s))와 유사하게 정의된 후방 변수(βₜ(s)) : 후방 변수는 시간 t에서 lₛ:|l|의 전체 확률을 나타냅니다. 후방 변수는 다음과 같이 정의됩니다:<br/>
> <img src="https://latex.codecogs.com/png.latex?\beta_t(s)\overset{\mathrm{def}}{=}\sum_{\pi\in%20N^T:%20B(\pi_{t:T})=l_{1:s:|l|}}\prod_{t'=t}^{T}y_{\pi_{t'}}^{t'}" alt="Equation (9)"/><br/>
<img src="https://latex.codecogs.com/png.latex?\beta_T(|l'|)%20=%20y_b^T" alt="Equation 9a"/><br/>
<img src="https://latex.codecogs.com/png.latex?\beta_T(|l'|-1)%20=%20y_{l_{|l|_1}}^T" alt="Equation 9b"/><br/>
<img src="https://latex.codecogs.com/png.latex?\beta_T(s)%20=%200,%20\forall%20s%20<%20|l'|%20-1" alt="Equation 9c"/><br/>
<img src="https://latex.codecogs.com/png.latex?\beta_t(s)%20=%20\begin{cases}%20\bar{\beta}_t(s)y_{l'_s}^t&\text{if}%20l'_s%20=%20b%20\text{or}%20l'_{s+2}%20=%20l'_s\\(\bar{\beta}_t(s)+\beta_{t+1}(s+2))y_{l'_s}^t&\text{otherwise}\end{cases}" alt="Equation (10)"/><br/>
<img src="https://latex.codecogs.com/png.latex?\bar{\beta}_t(s)\overset{\mathrm{def}}{=}\beta_{t+1}(s)+\beta_{t+1}(s+1)" alt="Equation (11)"/><br/>
> βₜ(s) = 0 for all s > 2t (figure 3의 왼쪽 아래에 있는 연결되지 않은 원들) 및 s > |l₀|입니다. 이는 βₜ(s)의 값이 특정 조건에서 0이 되는 경우를 설명하는데, s가 t의 두 배보다 크거나 라벨 시퀀스 l₀의 길이보다 클 경우에는 βₜ(s)가 0이 된다는 의미입니다. 실제로 위의 재귀식은 디지털 컴퓨터에서 곧 underflow를 초래하게 됩니다. 이를 피하는 한 가지 방법은 forward 및 backward 변수를 rescale하는 것입니다(Rabiner, 1989). <br/>
> <img src="https://latex.codecogs.com/png.latex?D_t%5E%7Bdef%7D%20%3D%20%5Csum_%7Bs%7D%20%5Cbeta_t%28s%29%2C%20%5Cquad%20%5Chat%7B%5Cbeta_t%28s%29%7D%5E%7Bdef%7D%20%3D%20%5Cfrac%7B%5Cbeta_t%28s%29%7D%7BD_t%7D" alt="Equation"/> <br/>
> (식 6)과 (식 7)의 우변에 α 대신 α̂를 대입한다면, 전방 변수(forward variables)는 계산 가능한 범위 내에 머물게 됩니다.
>
> <img src="https://latex.codecogs.com/png.latex?D_t%5E%7Bdef%7D%20%3D%20%5Csum_%7Bs%7D%20%5Cbeta_t%28s%29%2C%20%5Cquad%20%5Chat%7B%5Cbeta_t%28s%29%7D%5E%7Bdef%7D%20%3D%20%5Cfrac%7B%5Cbeta_t%28s%29%7D%7BD_t%7D" alt="Equation"/> <br/>
> 이와 마찬가지로, backward 변수에 대해 (10) 및 (11)에서 β 대신 β̂를 대입합니다.
>
> 최대 우도 오류를 평가하려면, 목표 레이블 확률의 자연 로그가 필요합니다. 재조정된 변수들을 사용하면, 이 값들은 특히 간단한 형태를 띱니다.
> <img src="https://latex.codecogs.com/png.latex?%5Cln%28p%28l%7C%5Cmathbf%7Bx%7D%29%29%20%3D%20%5Csum_%7Bt%3D1%7D%5ET%20%5Cln%28C_t%29" alt="Equation"/> <br/>
>
> **4.2. Maximum Likelihood Training**
> 최대 우도 학습의 목표는 학습 세트 내에서 올바른 분류에 해당하는 모든 로그 확률을 동시에 극대화하는 것입니다. 우리의 경우, 이것은 다음 목적 함수를 최소화하는 것을 의미합니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?O%5E%7BML%7D%28S%2C%5Cmathcal%7BN_w%7D%29%20%3D%20-%20%5Csum_%7B%28%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bz%7D%29%20%5Cin%20S%7D%20%5Cln%28p%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29%29" alt="Equation (12)"/> <br/>
>
> 경사 하강법을 사용하여 네트워크를 학습시키기 위해서는 네트워크 출력에 대해 (12)의 미분값을 구해야 합니다. 학습 예시들이 독립적이므로, 각각을 따로따로 고려할 수 있습니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20O%5E%7BML%7D%28%7B%5C%7B%28%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bz%7D%29%7D%2C%5Cmathcal%7BN_w%7D%29%7D%7B%5Cpartial%20y_k%5E%7Bt%7D%7D%20%3D%20-%5Cfrac%7B%5Cpartial%20%5Cln%28p%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29%29%7D%7B%5Cpartial%20y_k%5E%7Bt%7D%7D" alt="Equation (13)"/>
  <br/>
> 이제 4.1절의 알고리즘을 사용하여 (13)을 계산하는 방법을 보여주겠습니다.
>
> 핵심은 라벨링 l에 대해 주어진 s와 t에서 전방 및 후방 변수의 곱은 시간 t에서 기호 s를 통과하는 l에 해당하는 모든 경로의 확률이라는 것입니다. 더 정확하게 말하면 (5)와 (9)에서 우리는 다음을 얻습니다.  <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Calpha_t%28s%29%5Cbeta_t%28s%29%20%3D%20%5Csum_%7B%5Cpi%20%5Cin%20%5Cmathcal%7BB%7D%5E%7B-1%7D%28l%29%2C%20%5Cpi_t%3Dl_s%27%7D%20y_%7Bl_s%27%7D%5E%7Bt%7D%20%5Cprod_%7Bt%3D1%7D%5ET%20y_%7B%5Cpi_t%7D%5E%7Bt%7D" alt="Equation"/> <br/>
>
> (2)에서 재배열하여 대입하면 다음을 얻습니다. <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Calpha_t%28s%29%5Cbeta_t%28s%29%7D%7By_%7Bl_s%27%7D%5E%7Bt%7D%7D%20%3D%20%5Csum_%7B%5Cpi%20%5Cin%20%5Cmathcal%7BB%7D%5E%7B-1%7D%28l%29%2C%20%5Cpi_t%20%3D%20l_s%27%7D%20p%28%5Cpi%7C%5Cmathbf%7Bx%7D%29" alt="Equation"/> <br/>
>
> (3)에서 우리는 주어진 시간 t에 **$\[l'_s\]$**를 통과하는 경로들에 의한 p(l|x) 의 전체 확률의 일부를 확인할 수 있습니다. 따라서, 임의의 t에 대해 모든 s에 대해 합을 구할 수 있습니다:  <br/>
> <img src="https://latex.codecogs.com/png.latex?p%28l%7C%5Cmathbf%7Bx%7D%29%20%3D%20%5Csum_%7Bs%3D1%7D%5E%7B%7C%5Cl_s%27%7C%7D%20%5Cfrac%7B%5Calpha_t%28s%29%5Cbeta_t%28s%29%7D%7By_%7Bl_s%27%7D%5E%7Bt%7D%7D" alt="Equation (14)"/>
 <br/>
>
> 이 값을 $\[y_k^t\]$에 대해 미분하려면, 시간 t에서 라벨 k를 통과하는 경로들만 고려하면 됩니다. 같은 라벨(또는 빈칸)이 동일한 라벨링 l에서 여러 번 반복될 수 있으므로, 우리는 **$\[lab(l,k) = {s : l'_s = k}\]$**로 라벨 k가 발생하는 위치 집합을 정의합니다. 이 집합은 비어 있을 수도 있습니다. 그런 다음 (14)를 미분하여 다음을 얻습니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20p%28l%7C%5Cmathbf%7Bx%7D%29%7D%7B%5Cpartial%20y_k%5E%7Bt%7D%7D%20%3D%20%5Cfrac%7B1%7D%7B%28y_k%5E%7Bt%7D%29%5E2%7D%20%5Csum_%7Bs%20%5Cin%20lab%28l%2Ck%29%7D%20%5Calpha_t%28s%29%5Cbeta_t%28s%29.%20%5Cquad%2815%29" alt="Equation (15)"/>
 <br/>
> 다음과 같이 관찰할 수 있습니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cln%28p%28l%7C%5Cmathbf%7Bx%7D%29%29%7D%7B%5Cpartial%20y_k%5E%7Bt%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bp%28l%7C%5Cmathbf%7Bx%7D%29%7D%5Cfrac%7B%5Cpartial%20p%28l%7C%5Cmathbf%7Bx%7D%29%7D%7B%5Cpartial%20y_k%5E%7Bt%7D%7D" alt="Equation"/> <br/>
> 이제 l = z로 설정하고 (8)과 (15)를 (13)에 대입하여 목적 함수를 미분할 수 있습니다.
>
> 마지막으로, 소프트맥스 층을 통해 기울기를 역전파하려면 정규화되지 않은 출력 $\[u_k^t\]$에 대한 목적 함수의 미분값이 필요합니다.
>
> 만약 4.1절의 재스케일링이 사용된다면, 우리는 다음을 얻습니다: <br/>
> <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20O%5E%7BML%7D%28%7B%5C%7B%28%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bz%7D%29%7D%2C%5Cmathcal%7BN_w%7D%29%7D%7B%5Cpartial%20u_k%5E%7Bt%7D%7D%20%3D%20y_k%5E%7Bt%7D%20-%20%5Cfrac%7B1%7D%7By_k%5E%7Bt%7D%20Z_t%7D%20%5Csum_%7Bs%20%5Cin%20lab%28z%2Ck%29%7D%20%5Chat%7B%5Calpha_t%28s%29%7D%5Chat%7B%5Cbeta_t%28s%29%7D.%20%5Cquad%2816%29" alt="Equation (16)"/>
<br/>
> 여기서 <img src="https://latex.codecogs.com/png.latex?Z_t%5E%7Bdef%7D%20%3D%20%5Csum_%7Bs%3D1%7D%5E%7B%7Cl%27%7C%7D%20%5Cfrac%7B%5Chat%7B%5Calpha_t%28s%29%7D%20%5Chat%7B%5Cbeta_t%28s%29%7D%7D%7By_%7Bl_s%27%7D%5E%7Bt%7D%7D." alt="Equation"/>
 <br/>
> 식 (16)은 네트워크가 훈련 중에 받는 '오류 신호'를 나타냅니다 (그림 4 참조). <br/>
> **[Figure 4]**   <img width="400" alt="image" src="https://github.com/user attachments/assets/18c1cc16-9d47-4f40-968b-37c8d42f7a2f"> <br/>
>
> 해당 그림은 CTC 훈련 중 오류 신호의 변화를 보여줍니다. 왼쪽 열은 동일한 시퀀스에 대해 훈련의 각 단계에서 출력 활성화를 나타내며, 오른쪽 열은 해당되는 오류 신호를 보여줍니다. 점선은 'blank' 유닛을 나타냅니다. <br/>
>   **(a)** 처음에는 네트워크가 작은 랜덤 가중치를 가지고 있어 예측이 무작위적입니다. 오류는 주로 목표 시퀀스에 의해 결정되며, 출력이 거의 아무런 의미가 없는 상태입니다. 오류 신호는 출력 활성화의 증가나 감소를 유도합니다.<br/>
>   **(b)** 네트워크가 목표 시퀀스를 어느 정도 예측하기 시작하며, 오류가 점차 특정 구간에 집중됩니다. 이 단계에서 네트워크는 잘못된 예측을 줄여가고, 특정 시퀀스에서 높은 확률의 활성화를 보입니다.<br/>
>   **(c)** 네트워크가 목표 레이블을 정확히 예측하고, 오류는 거의 사라집니다. 이 시점에서 네트워크는 매우 정확한 예측을 수행하며, 오류 신호는 거의 없거나 매우 미미합니다.


## 5. Experiments
> 우리는 CTC의 성능을 실제 세계의 시간적 분류 문제인 **TIMIT 음성 말뭉치(코퍼스)** 에서 음소(phonetic) 라벨링 작업을 통해 HMM, 그리고 HMM-RNN 하이브리드의 성능과 비교했습니다. 보다 구체적으로, TIMIT 테스트 세트의 발화를 가장 낮은 라벨 오류율로 주석(annotate)하는 것이 목표였습니다(섹션 2.1에서 정의된 대로).
>
> 공정한 비교를 위해, CTC와 하이브리드 네트워크는 동일한 RNN 아키텍처를 사용했습니다: 양방향 장기 단기 기억(BLSTM) 네트워크였습니다(Graves & Schmidhuber, 2005). BLSTM은 LSTM(장기 단기 기억; Hochreiter & Schmidhuber, 1997)의 장기 시간 지연 문제를 해결하는 능력과 양방향 RNN(BRNN)(Schuster & Paliwal, 1997)의 과거 및 미래 문맥 접근 능력을 결합한 것입니다. 우리는 다른 아키텍처도 사용할 수 있음을 강조하며, 실험에서 BLSTM을 선택한 이유는 표준 BRNN과 단방향 네트워크를 사용했을 때 동일한 작업에서 더 나쁜 결과를 보였기 때문입니다.
>
> **5.1. Data**
> TIMIT는 영어 발화가 수록된 음성 녹음을 포함하며, **수동으로 분할된 음소(phonetic)** 전사가 함께 제공됩니다. TIMIT에는 61개의 고유한 음소가 포함된 **어휘집(lexicon)** 이 있으며, 으며, 각각 4620개와 1680개의 발화를 포함하는 훈련 및 테스트 세트로 나뉩니다. 훈련 발화 중 **5%** 에 해당하는 184개의 발화는 무작위로 선택되어 하이브리드 및 CTC 실험에서 **조기 종료를 위한 검증 세트**로 사용되었습니다. 니다. 오디오 데이터는 26개필터 뱅크 채널의 12개 Mel-Frequency Cepstrum Coefficient(MFCC)를 사용하여 **5ms로 중첩된 10ms 프레임**으로 사전 처리되었습니다. 모든 계수의 **1차 도함수와 함께 로그 에너지도 포함되어 프레임당 총 26개의 계수 벡터가 제공**되었습니다. 계수는 훈련 세트에서 **평균 0, 표준 편차 1**이 되도록 개별적으로 정규화되었습니다.
>
> **5.2. Experimental Setup**
> CTC 네트워크는 피프홀(peepholes)과 포겟 게이트(forget gates)를 사용한 **확장된 BLSTM(Bidirectional Long Short-Term Memory)** 아키텍처를 사용했습니다(Gers et al., 2002). 각 전방 및 후방 은닉층에는 100개의 블록이 있으며, 입력 및 출력 셀 활성화 함수로는 **하이퍼볼릭 탄젠트(tanh)** 가 사용되었고, 게이트에는 [0, 1] 범위의 로지스틱 시그모이드 함수가 사용되었습니다.
>
> 은닉층은 입력층 및 출력층과 완전히 연결되어 있었고, 입력층은 크기가 26이며, **소프트맥스 출력층은 62개의 유닛**으로 구성되었습니다(61개의 음소 카테고리 + 빈(blank) 라벨). **총 가중치의 수는 114,662개**였습니다.
>
> 훈련은 **시간을 역으로 거슬러 가며 전파하는 기법(backpropagation through time)** 과 **온라인 경사 하강법(online gradient descent)** 으로 진행되었습니다(각 훈련 샘플마다 가중치를 업데이트). 학습률은 $[\10^−4\]$ 로 설정되었고, **모멘텀(momentum)** 은 0.9였습니다. 각 샘플이 시작될 때 네트워크 활성화 값은 0으로 초기화되었습니다. Prefix search decoding에서 빈 라벨 확률 임계값은 **99.99%** 로 설정되었습니다. 가중치는 [−0.1, 0.1] 범위의 평평한 랜덤 분포로 초기화되었습니다. 훈련 중에는 일반화를 개선하기 위해 표준 편차가 0.6인 가우시안 노이즈가 입력에 추가되었습니다.
>
> 기본 **HMM(Hidden Markov Model)** 및 **하이브리드 시스템**은 Graves 외 연구에서 설명된 것과 동일하게 구현되었습니다(Graves et al., 2005). 기본 HMM은 문맥 독립 및 문맥 의존의 세 상태 좌우 모델로 훈련 및 테스트되었으며, **HTK Toolkit**을 사용했습니다. **관측 확률은 가우시안 혼합 모델로 모델링**되었습니다. 가우시안의 수와 삽입 패널티는 작업에서 최상의 성능을 얻기 위해 선택되었습니다. 이 시스템에는 언어적 정보나 **부분 음소 시퀀스의 확률**이 포함되지 않았습니다. 총 매개변수의 수는 **90만 개 이상**이었습니다.
>
> 하이브리드 시스템은 HMM과 BLSTM 네트워크로 구성되었으며, **비터비 기반 강제 정렬(Viterbi-based forced-alignment)** 을 사용해 훈련되었습니다(Robinson, 1994). 61개 모델의 한 상태 전이 및 사전 확률에 대한 초기 추정은 훈련 세트에 대한 정확한 전사로 수행되었습니다. 네트워크 출력 확률은 **사전 확률**로 나누어 HMM에 대한 우도를 얻었습니다. 삽입 패널티는 작업에서 최상의 성능을 얻기 위해 선택되었습니다.
>
> BLSTM 아키텍처 및 매개변수는 CTC와 동일했으며, 다음의 예외 사항이 있었습니다: (1) 하이브리드 네트워크의 학습률은 $[\10^−5\]$였고; (2) 삽입된 노이즈의 표준 편차는 0.5였으며; (3) 출력층은 빈(blank) 라벨을 제외한 61개 유닛을 가졌습니다. 노이즈와 학습률은 두 시스템에서 독립적으로 설정되었으며, 매개변수 공간에서의 대략적인 탐색을 통해 선택되었습니다. 하이브리드 네트워크의 가중치는 총 114,461개였으며, HMM은 여기에 추가로 183개의 매개변수를 더했습니다. **가중 오류 실험(weighted error experiment)** 에서는 오류 신호가 길고 짧은 음소에 동일한 가중치를 부여하기 위해 조정되었습니다(Robinson, 1991).
>
> **5.3. Experimental Results**
> **[Table 1]**   <img width="350" alt="image" src="https://github.com/user-attachments/assets/ba6ec37e-ceac-4961-aed3-9ed2b550063c"> <br/>
> Table 1에서 TIMIT 데이터셋에 대한 라벨 오류율(LER: Label Error Rate) 결과를 보여줍니다. CTC와 하이브리드 결과는 각각 5번의 실험에서 얻은 평균값이며, **± 표준 오차(standard error)** 로 나타냅니다. 모든 차이는 유의미한 결과였으며 (p < 0.01), 가중치 오류가 적용된 BLSTM/HMM과 CTC(최적 경로) 간의 차이는 유의미하지 않았습니다. 즉, 대부분의 결과에서 통계적으로 의미 있는 차이가 있었으나, **가중치 오류가 적용된 하이브리드 모델(BLSTM/HMM)** 과 CTC의 최적 경로 디코딩 방법 사이에는 차이가 크지 않다는 것을 의미합니다.
>
> Table 1의 결과는 **접두사 탐색 디코딩(prefix search decoding)** 을 사용한 CTC가 기본 HMM 인식기와 동일한 RNN 아키텍처를 사용한 HMM-RNN 하이브리드 모델보다 더 우수한 성능을 보였음을 보여줍니다. 또한 접두사 탐색이 **최적 경로 디코딩(best path decoding)** 보다 약간 더 나은 성능을 냈음을 나타냅니다.
>
> 최적의 하이브리드 모델 성능은 **가중치 오류 신호(weighted error signal)** 를 사용했을 때 달성되었지만, 이러한 휴리스틱은 CTC에서는 필요하지 않았습니다. CTC의 목적 함수는 라벨의 시퀀스에만 의존하며, 라벨의 지속 시간이나 **세분화(구간화)** 에는 의존하지 않기 때문입니다.
>
> 또한, 입력에 추가된 노이즈가 CTC의 일반화 성능에 더 큰 영향을 미쳤으며, CTC에서는 더 높은 수준의 노이즈가 최적의 성능을 내는 데 적합한 것으로 나타났습니다.


## 6. Discussion and Future Work
> CTC (Connectionist Temporal Classification)의 중요한 차이점 중 하나는 **입력 시퀀스를 명시적으로 구분하지 않는다**는 점입니다. 이 방식은 몇 가지 장점을 제공합니다. 예를 들어, 음성이나 필기처럼 본질적으로 애매한 라벨 경계를 찾을 필요가 없으며, 여러 라벨이 함께 발생하는 경우 이를 그룹화하여 예측할 수 있습니다. 실제로 **라벨 시퀀스만 필요한 경우에는 구분하는 것이 불필요한 모델링 작업**입니다.
>
> 하지만 **구분이 필요한 작업**(예: 단백질 이차 구조 예측)에서는 CTC를 사용하는 것이 문제가 될 수 있습니다. 그러나 그림 1에서 보이듯이, CTC는 자연스럽게 각 라벨 예측을 해당 시퀀스의 부분과 정렬하는 경향이 있습니다. 따라서 **대략적인 구분만으로도 충분한 작업**(예: 키워드 탐지)에서는 CTC가 적합할 수 있습니다.
>
> 또 다른 CTC의 특징은 **라벨 간의 종속성을 명시적으로 모델링하지 않는다**는 점입니다. 그래픽 모델(graphical model)에서는 라벨들이 일반적으로 **k차 마르코프 연쇄(kth order Markov chain)** 를 형성한다고 가정하지만, CTC는 그렇지 않습니다. 그럼에도 불구하고, CTC는 암묵적으로 라벨 간의 종속성을 모델링합니다. 예를 들어, 자주 함께 발생하는 라벨을 **이중 스파이크(double spike)** 로 예측할 수 있습니다(그림 1 참고).
>
> 구조화된 데이터를 처리하는 매우 일반적인 방법 중 하나는 **계층적 시간 분류기(hierarchical temporal classifiers)** 를 사용하는 것입니다. 한 레벨에서의 라벨(예: 문자)이 다음 레벨(예: 단어)의 라벨에 대한 입력이 되는 방식입니다. 계층적 CTC에 대한 초기 실험은 고무적이었으며, 이 방향으로 계속 연구할 계획입니다.
>
> 구조화된 데이터를 처리하는 매우 일반적인 방법 중 하나는 **계층적 시간 분류기(hierarchical temporal classifiers)** 를 사용하는 것입니다. 한 레벨에서의 라벨(예: 문자)이 다음 레벨(예: 단어)의 라벨에 대한 입력이 되는 방식입니다. 계층적 CTC에 대한 초기 실험은 고무적이었으며, 이 방향으로 계속 연구할 계획입니다.


## 7. Conclusions
> 우리는 RNN(순환 신경망)을 사용한 **시간적 분류**를 위한 새로운 일반적인 방법을 소개했습니다. 이 방법은 기존의 **신경망 분류기 프레임워크**에 자연스럽게 통합되며, 동일한 **확률적 원리**에서 유도되었습니다. 이 방법은 **사전 구분된 데이터가 필요하지 않으며**, 네트워크를 **직접 시퀀스 라벨링 작업에 맞게 훈련**할 수 있게 해줍니다. 또한, **특정 작업에 대한 사전 지식이 필요 없이** 실제 세계의 시간적 분류 문제에서 **HMM(Hidden Markov Model)** 과 **HMM-RNN 하이브리드**보다 더 나은 성능을 발휘했습니다.



# 부족한 개념 공부

## 0) Abstract

### 🔎 Unsegmented data
- **개념**
  - 데이터를 명확하게 나누지 않은 상태를 의미
  - 즉, 데이터가 연속적인 흐름으로 존재하며, 특정 구간으로 미리 나누어지지 않은 상태
- **음성인식에서의 개념**
  - 음성 인식에서 입력 신호는 연속적인 음향 신호
  - 신호의 어느 부분이 특정 단어 또는 음소에 해당하는지 명확하지 않음을 의미
  - 음성인식을 예로 들면, 프레임 당 무슨 음운인지는 모르지만 전체 라벨인 단어는 아는 상태를 의미
  - Waveform(파형) 파일에 대한 라벨이 "the sound of"라고 달려 있다고 할 때, 해당 waveform 파일에서 어디까지가 각각 "the", "sound", "of"인지의 경계가 명확하게 분할되어 있지 않은 경우를 생각
- **특징**
  - 이 개념은 특히 음성 인식, 필기 인식, 제스처 인식 등과 같은 순차 데이터 처리에 자주 등장
  - 음성, 영상, 필기, 제스처 등 시간에 따라 변하는 연속적인 데이터는 특정 단위를 나누는 기준을 정하기가 어려움
  - 이미지 데이터: 하나의 이미지에서 구분되지 않은 객체들이 여러 개 있을 때
  - 텍스트 데이터: 자연어 처리에서도 문장이 나뉘어 있지 않고, 구두점 없이 이어져 있는 텍스트
 
### 🔎 Sequence
- 일정한 순서나 연속성을 가진 데이터나 사건의 나열을 의미
- 연속적인 요소들의 나열
  - 해당 요소들은 특정한 순서를 가지고 있음
  - 순서가 시퀀스의 핵심
- **음성인식에서의 개념**
  - 음성 데이터는 시간에 따라 변화하는 연속적인 신호의 시퀀스
  - 이 신호는 음소(소리의 최소 단위)의 연속으로 구성되며, 음성을 인식하려면 시퀀스의 순서와 패턴을 분석해야 함
  - 시간 순서대로 이어진 소리의 연속적인 데이터
  - **음성 신호 자체가 시퀀스**이고, 그 안에는 **주파수, 볼륨, 진폭 같은 소리의 다양한 특성들이 포함**되어 있어
    - 예를 들어, "안녕하세요"라고 말하면 이 말이 끝나는 그 순간까지 **시간에 따라 변하는 신호의 흐름(시퀀스)** 이 기록되는 거 
- **시퀀스처리에 적합한 모델**
  - 순서를 반영하여 처리할 수 있는 알고리즘
  - RNN(순환 신경망), LSTM

### 🔎 Acoustic signal(음향 신호)
- 소리가 물리적인 매체(공기, 물, 고체 등)를 통해 전달될 때 발생하는 신호
- 소리는 기압의 변화로 인해 발생하는 파동인데, 이 파동이 매체를 통해 전달될 때 진동이 발생하고, 이 진동이 시간에 따라 변화하면서 신호로 기록
- 소리가 전달될 때 발생하는 연속적인 신호로, 주파수, 진폭, 시간 등의 요소를 포함한 소리의 물리적 특성을 나타내는 데이터

### 🔎 Post processing(후처리)
- 모델이나 시스템에서 처음으로 생성된 결과물을 추가적으로 가공하는 과정
- **음성인식에서의 개념**
  - 모델이 처음 예측한 결과를 가공하여, 사람이 이해할 수 있는 문장이나 단어로 바꾸는 과정
  - RNN 기반의 음성 인식 시스템이 프레임 단위로 음성 신호를 처리해 음소(소리의 작은 단위)를 예측했다면, 이 음소들을 연결하고, 불필요한 중복을 제거하며, 최종적으로 정확한 단어나 문장으로 바꾸는 작업
  - 음소 연결: 음성 인식 모델이 "안녕하세요"라는 문장을 소리 단위로 인식했을 때, "ㅇ-ㅇ-ㄴ-ㄴ-ㅇ-ㅕ-ㅅ-ㅛ" 같은 음소들로 나올 수 있습니다. 이 음소들을 올바른 순서로 연결하여 실제 문장으로 변환하는 과정
  - 불필요한 중복 제거: 모델이 같은 음소를 여러 번 예측할 수 있습니다. 예를 들어 "ㄴ-ㄴ"이 두 번 나온다면, 이걸 한 번의 "ㄴ"으로 축약하는 과정이 필요
  - 정확한 문장으로 변환: 모든 음소를 적절하게 조합해 문장으로 변환하는 과정도 후처리의 일부입니다. 예를 들어 "ㅇ-ㅕ-ㅅ-ㅛ"를 "여쑤"로 변환

### 🔎 TIMIT
- 음성 인식 연구와 실험에서 자주 사용되는 표준 음성 데이터베이스
- 음성 인식 시스템을 학습시키고 평가하는 데 사용
- TIMIT은 다양한 발음과 억양을 가진 영어 화자들의 녹음된 음성 데이터를 포함
  - 630명의 다른 화자가 각기 다른 문장을 읽는 데이터를 포함
- 각 녹음된 음성 파일에 대해 **정확한 발음 레이블(음소 단위)** 이 포함
  - 각 음성 신호가 어떤 소리(음소)로 구성되어 있는지를 명확하게 레이블링한 것이 특징
- 8개 문장: 각 화자는 10개의 문장을 읽습니다. 이 중 2개는 모든 화자가 동일한 문장이고, 5개는 서로 다른 문장, 3개는 서로 교차된 문장
- 음소 레벨의 주석: TIMIT 데이터는 음소 단위로 주석이 달려 있어, 음성 인식 모델이 음소 수준에서 정확한 학습가능

<br>

## 1) Introduction

### 🔎 Perceptual task
- 사람이 감각을 통해 외부의 자극을 인식하고 처리하는 작업(시각, 청각, 촉각)
- 주로 사람이나 기계가 감각 정보를 바탕으로 주변 환경을 인식하거나, 그 정보를 처리하여 특정한 결론을 도출하는 과정에서 발생하는 작업들을 포함
- **Perceptual task가 중요한 이유**
  - 컴퓨터가 시각이나 청각 정보를 처리해 사람처럼 인식하고 결정을 내리는 능력을 개발하는 것이 목표인 경우가 많기 때문
  - 음성 인식: 음향 신호를 분석해 어떤 말을 하고 있는지 인식하는 것

 
### 🔎 Graphical model
![image](https://github.com/user-attachments/assets/7954c4dc-55d4-4b7a-80f3-1d464339e60f)

- 확률 변수들 간의 관계를 그래프 구조로 표현하는 모델
- 변수들간의 상호 의존 관계를 표현한 확률 모델
- 확률론적인 문제를 해결하기 위해 변수들 사이의 **의존성(dependencies)** 과 **독립성(independence)** 을 시각적으로 표현하고, 그 구조를 기반으로 추론이나 학습을 수행
- Conditional Independence라는 가정
- **구성요소**
  - Node (Vertex): 랜덤 변수(random variable)를 하나의 노드로 표현
  - Edge (Link) : 랜덤 변수 사이의 확률적인 관계
- 그래프 방향성 여부에 따라 2가지 형태 존재
  - **Undirected Graph** : 아래 Nodes와 Edges 그림 처럼 방향성이 없는 그래프
    ![image](https://github.com/user-attachments/assets/16121b9c-3725-456e-be1d-b6bad3e024b9)

    - 예시 : Markov Random Fields, Boltzmann Machine
      
  - **Directed Graph** : 아래 오른쪽 그림처럼, 화살표가 존재하는 그래프
    ![image](https://github.com/user-attachments/assets/1c3370ab-a932-4f1e-913e-1be53c7451f2)

    - 자식 노드 이전에 부모 노드가 존재
    - 예시 : Bayesian Networks, HMM (Hidden Markov Models), Latent Variable Models 
  ![image](https://github.com/user-attachments/assets/24947be2-2e74-41ab-ae93-ebe1728a50e5)

- **주요 유형**
  - **베이즈 네트워크(Bayesian Network)**
    - **방향성 비순환 그래프(DAG, directed acyclic graphs)** 로 구성된 모델
    - 노드들은 확률 변수를 나타내고, 엣지는 변수들 간의 조건부 의존성을 나타냄
  - **마르코프 랜덤 필드(Markov Random Field, MRF)**
    - 비방향 그래프로 구성된 모델
    - 각 변수 간의 상호작용이 엣지로 연결
    - 모델은 주로 공간적인 의존성이나 이웃 관계가 중요한 문제에서 사용(ex. 이미지 처리)

### 🔎 DAG(Directed Acyclic Graph)
- DAG는 cycle이 없는 방향성 그래프
![image](https://github.com/user-attachments/assets/2f827e08-eb26-4d02-8dd4-ed23e9651cca)

### 🔎 Conditional Vs. Marginal independence
- **Marginal independence(주변 독립성)**
  - P(A,B)=P(A)⋅P(B)
  - 주변 독립성은 다른 변수가 없을 때도 **두 변수가 완전히 독립적**임을 의미하며, **조건 없이 독립적**
  - 주사위 던지기: 두 개의 주사위를 던지는 경우, 첫 번째 주사위(A)와 두 번째 주사위(B)의 결과는 주변적으로 독립적
  - 두 개의 주사위가 서로 영향을 미치지 않기 때문에, A와 B는 서로 독립적
- **Conditional independence(조건부 독립성)**
 - P(A,B∣C)=P(A∣C)⋅P(B∣C)
 - 조건부 독립성은 제3의 변수(조건)가 주어졌을 때 두 변수 간의 독립성을 의미하며, **다른 변수가 존재할 때만 두 변수가 독립적**
 - 음성 인식에서: "날씨"라는 변수(C)가 주어졌을 때, "우산을 쓰는지(A)"와 "비가 오는지(B)"는 조건부 독립적
 - 날씨가 비가 오는 날이라면, 이미 날씨 정보를 알기 때문에 우산과 비의 관계는 독립적
 - conditional independent 하다고 independent 한 것은 아니다
- **관계**
  - 조건부 독립일 수 있지만 주변 독립이 아닐 수 있음
    - "환자의 나이(A)"와 "환자가 특정 질병을 가지고 있는지(B)"는 주변적으로 독립적
    - 하지만, "흡연 여부(C)"가 주어졌을 때, 나이와 질병 여부는 조건부 독립적
    - 흡연 여부를 알고 나면, 나이와 질병 간의 관계는 없어질 수 있기 때문 

### 🔎 Bayesian networks
- Bayes 네트워크, 신뢰 네트워크 또는 의사결정 네트워크라고도 함
- Conditional independence(조건부 독립성)
- 변수 집합과 변수의 조건부 종속성을 나타내는 확률적 그래픽 모델
- 결합확률 : p(a,b,c)=p(c|a,b)p(a,b)
- 만약 $\(x_i\), \(i = 1, \ldots, n\)$ 가 이항(binary) 변수라면, $\(p(x_i | x_{i-1}, \ldots, x_1)\)은 \(2^{i-1}\)$개의 모수가 필요
  - 결론적으로 조건부 확률에서 조건으로 주어지는 변수가 많아지면 모수가 **지수적으로 증가**
  - 영향을 주는 변수만을 골라서 조건으로 준다면, 고려해야하는 경우 적어지므로 필요한 모수가 적어질 것
- 예를 들어 베이지안 네트워크는 질병과 증상 간의 확률적 관계를 나타낼 수 있습니다
  - 증상이 주어지면 네트워크를 사용하여 다양한 질병의 존재 확률을 계산 
![image](https://github.com/user-attachments/assets/7b8c42fb-190d-4ea6-9c0e-fdfd087ea5b7)

- 활성 스프링클러 또는 비 두 가지 이벤트로 인해 잔디가 젖을 수 있음
  - 비는 스프링클러 사용에 직접적인 영향을 미침(즉, 비가 올 때 일반적으로 스프링클러는 작동하지 않음)
  - Pr(G,S,R) = Pr(G|S,R)Pr(S|R)Pr(R)
    - G = Grass wet(true/false)
    - S = Sprinkler turned on(true/false)
    - R = Raining(true/false)
    - Rain이 True일 때, Grass Wet이 True일 확률은 높음
    - Sprinkler와 Rain은 서로 독립적이지 않으며, 둘 다 Grass Wet에 영향을 미침
  - 잔디가 젖어 있을 때 비가 올 확률은?
    - $\[\Pr(R = T \mid G = T) = \frac{\Pr(G = T, R = T)}{\Pr(G = T)} = \frac{\sum_{x \in \{T,F\}} \Pr(G = T, S = x, R = T)}{\sum_{x,y \in \{T,F\}} \Pr(G = T, S = x, R = y)}\]$
    - $\[Pr(R = T \mid G = T) = \frac{0.00198_{TTT} + 0.1584_{TFT}}{0.00198_{TTT} + 0.288_{TFF} + 0.1584_{TFT} + 0.0_{TFF}} = \frac{891}{2491} \approx 35.77\%.\]$
  - 결합확률 계산 예시 $\[Pr(G = T, S = T, R = T) = Pr(G = T \mid S = T, R = T) \times Pr(S = T \mid R = T) \times Pr(R = T)\]$
    - $\[= 0.99 \times 0.01 \times 0.2 = 0.00198\]$ 

### 🔎 CRFs(conditional random fields)
- 순차 데이터와 의존 관계를 모델링하는 확률 그래프 모델
- CRF는 주로 자연어 처리(NLP) 분야에서 텍스트 데이터의 특정 부분에 라벨을 할당하는 태깅 작업에 활용(예: POS 태깅, 개체명 인식)

### 🔎 HMM의 State model
- **은닉 상태(hidden states)** 와 관측값(observed values) 간의 관계를 모델링하는 것
- 여기서 상태는 직접 관측할 수 없는 시스템의 내적 상태

- 이미지 처리에서 사용되며, 데이터 간의 상관관계를 잘 반영할 수 있다는 장점
- 입력 시퀀스가 주어졌을 때 각 항목에 가장 적합한 라벨을 예측하는 데 사용
- **마르코프 랜덤 필드(MRF)** 의 확장으로, 조건부 확률을 사용해 전역적인 최적화를 수행
- Undirected Graph, 주어진 관측값에 대해 조건부 확률을 모델링

### 🔎 생성적 접근법 Vs. 판별적 접근법
- **생성적 접근법 (Generative Approaches)**
  - 주어진 입력 데이터(예: 이미지)로부터 데이터셋 내의 다른 데이터를 생성하거나 재구성할 수 있는 모델을 학습시키는 데 초점
  - 데이터의 분포를 모델링하여 데이터를 생성하는 능력을 학습하는 것
  - 데이터와 레이블의 결합 분포를 학습하여, 주어진 입력에 대한 출력뿐만 아니라 데이터 자체를 생성할 수 있는 모델을 학습
  - 데이터 분포를 완벽히 모델링하므로, 데이터를 생성하거나 숨겨진 구조를 분석하는 데 적합
  - 예시: 나이브 베이즈, HMM(은닉 마르코프 모델)
  - Generative Adversarial Networks (GANs): 실제 데이터와 구별할 수 없는 데이터를 생성하도록 설계된 모델
  - Variational Autoencoders (VAEs): 입력 데이터를 잠재 공간의 낮은 차원의 표현으로 압축후, 이 표현에서 원본 데이터를 재생성하는 모델
  - Autoencoders: 입력을 잠재 표현으로 인코딩하고 다시 입력 데이터로 디코딩
- **판별적 접근법 (Discriminative Approaches)**
  - 라벨이 주어지지 않은 데이터로부터 표현을 학습
  - 주어진 입력에서 직접적으로 레이블을 예측하는 것
  - 정확하게 구분하는 데 초점을 맞추므로 예측 성능이 우수하며, 복잡한 의존 관계를 더 잘 학습할 수 있음
  - 로지스틱 회귀, SVM, 조건부 랜덤 필드(CRF)
  - Self-supervised Learning: 데이터 자체에서 라벨을 생성하고, 이를 예측하는 작업을 통해 유용한 표현을 학습
  - Contrastive Learning: 비슷한(긍정적인) 예시들이 서로 가까워지고, 다른(부정적인) 예시들이 멀어지도록 표현을 학습
  - Pretext Tasks: 이미지의 일부분을 회전시키거나, 퍼즐 조각을 재배열하는 등의 작업을 수행하도록 학습

### 🔎 Long-range Sequential structure
- 긴 시퀀스 데이터에서 시간적, 순차적 관계가 멀리 떨어져 있음에도 불구하고 의미 있는 상호작용이나 의존성이 존재하는 구조
- 언어 모델링: 문장의 앞부분과 뒷부분이 멀리 떨어져 있어도, 문맥을 이해하기 위해서는 앞부분의 정보를 기억해야 하는 경우
- 음성 신호 처리: 음성 데이터에서 첫 부분의 발음이 나중의 발음과 연관되어 있거나 의미를 가지는 상황
- 이러한 구조를 학습하기 위해 LSTM이나 Transformer 같은 모델이 효과적

### 🔎 Localized classification
- 데이터의 일부 영역이나 구간에 집중하여 클래스(레이블)를 예측하는 방법을 의미
- 전체 데이터가 아닌 특정 구간 또는 지역적 정보에 기반해 분류 작업을 수행하는 것
- 이미지 처리에서 특정 객체의 주변 픽셀 정보만을 활용하여 그 객체가 어떤 카테고리에 속하는지 판단하는 경우
- 자연어 처리에서 문장의 특정 구간에 기반해 그 구간의 의미를 분류하는 작업

### 🔎 Objective function(목적 함수)
- 신경망의 학습능력을 평가하는 지표
- 기계 학습 모델이 최적화해야 하는 목표를 수치화한 함수
- 모델이 학습할 때, 목적 함수의 값을 최소화하거나 최대화하는 방향으로 매개변수(파라미터)를 조정
- **Objective function >= Cost function >= Loss function**
  - **Cost function(비용 함수)** : 데이터셋 전체에 대해 Loss function의 평균을 나타내며, 모델의 전반적인 성능을 평가
  - **Loss function(손실 함수)**: 단일 데이터 포인트에 대해 모델이 예측한 값과 실제 값 간의 차이 
- 최소화: 대부분의 목적 함수는 **손실 함수(loss function)** 로, 모델이 예측한 값과 실제 값 간의 차이를 최소화하는 것이 목표
- 최대화: 일부 목적 함수는 **보상(reward)** 을 극대화하는 방향으로 사용될

### 🔎 시간 역전파(Backpropagation Through Time, BPTT)
- 과거의 출력을 고려하여 현재의 출력에 대한 오류를 계산하고, 이를 역방향으로 전달하는 방식
- **RNN(Recurrent Neural Network)** 에서 시퀀스 데이터를 학습할 때 사용되는 역전파 알고리즘의 확장
  - RNN 뿐만 아니라 시간적 의존성을 가지는 모든 네트워크에서 적용될 수 있음
  - RNN, LSTM, GRU와 같은 순차적 데이터를 다루는 네트워크에서 BPTT는 필수적인 학습 알고리즘 
- RNN은 시퀀스 데이터의 시간적 의존성을 학습하기 위해 **시간 축을 따라 순차적으로 데이터를 처리**
  - 이 과정에서 오차 신호를 시간을 거슬러 전파하여, 이전 시간 단계에서의 가중치를 업데이트하는 방식
- 현재 시점의 오차가 이전 시점의 가중치에 영향을 미치므로, 역방향으로 오차를 전파하여 가중치를 업데이트
- 그래디언트 소실 문제: 긴 시퀀스에서는 그래디언트가 소실되거나 폭발하는 문제가 발생할 수 있습니다. 이를 해결하기 위해 LSTM 같은 모델이 사용

### 🔎 Temporal classification
- 시간에 따라 변화하는 시퀀스 데이터를 분석하고, 각 시간 단계에 해당하는 레이블을 예측하는 문제를 다룸
- 예를 들어, 음성 인식, 필기 인식, 제스처 인식 등에서 사용
- 이 개념의 핵심은 입력 데이터가 시간 축을 따라 순차적으로 변하기 때문에, 각 시간 구간마다 적절한 레이블을 할당하는 것

### 🔎 Framewise classification
- 시퀀스 데이터를 프레임 단위로 처리하여 각 프레임마다 독립적으로 레이블을 예측하는 방법
- 주로 음성 인식이나 비디오 처리에서 사용
- 입력 시퀀스를 **짧은 고정된 구간(프레임)** 으로 나눈 후, 각각의 프레임을 개별적으로 분류하여 그 시점에서의 상태를 예측

### 🔎 RNN이 시퀀스 레이블링에 직접 적용되지 못하는 이유
- RNN의 장점은 시간에 따라 연속적으로 데이터를 처리하면서 이전의 상태를 기억할 수 있다는 것입니다.
- 하지만, 표준 신경망 목적 함수는 각 시간 단계에서 독립적인 학습만을 지원하기 때문에, 시퀀스 전체를 고려한 레이블링 작업(예: 음성 인식에서 연속된 음소의 라벨링)을 직접적으로 수행하지 못합니다.
- 이로 인해 데이터를 사전 분할하거나, 최종 레이블을 생성하기 위한 후처리가 필요하게 됩니다.

### 🔎 미분이 불가능하면 학습이 불가능한가?
- 미분이 불가능한 경우, 기존의 경사 하강법을 사용할 수 없으므로 표준적인 방법으로 학습하기는 어렵습니다.
- 경사 하강법은 미분 가능한 함수에 의존하여, 네트워크의 가중치를 점진적으로 조정해가는 방식이기 때문입니다.
- 유전 알고리즘(genetic algorithms)이나 강화 학습(reinforcement learning) 같은 방법들은 미분이 불가능한 함수에서도 사용할 수 있습니다.
- 또는 근사 미분 기법(예: 유한 차분법)을 사용할 수 있지만, 이는 성능이 떨어질 수 있습니다.
- 미분 가능하다는 것은 네트워크의 가중치를 어떻게 업데이트할지 결정하는데 필수적입니다.
- 미분 가능하다는 것은, 네트워크의 출력값과 목표값(레이블) 간의 차이를 기반으로 네트워크의 각 가중치(weight)를 어떻게 업데이트할지를 계산할 수 있음을 의미합니다.
- 가중치는 입력 데이터의 특정 특징이 출력에 얼마나 영향을 줄지를 조정하는 요소로, 학습 과정에서 점진적으로 최적화되어 모델의 예측 성능을 높입니다.

<br>

## 2) Temporal Classification

### 🔎 Label Error Rate (LER)
- 시퀀스 분류 작업에서 예측된 레이블과 실제 레이블 간의 차이를 측정하는 지표
- 편집 거리(Edit Distance), 즉 예측된 시퀀스를 실제 시퀀스로 변환하기 위해 필요한 삽입, 삭제, 대체의 최소 작업 수를 기반으로 계산
  - 두 시퀀스 사이의 차이를 측정하는 방법으로, 한 시퀀스를 다른 시퀀스로 바꾸기 위해 필요한 최소한의 편집 작업(삽입, 삭제, 대체)을 계산
  - 정규화: 전체 레이블의 길이를 기준으로 편집 거리를 정규화하여 오류율 도출 

<br>

## 3) Connectionist Temporal Classification

### 🔎 Softmax
- 머신러닝과 딥러닝에서 분류 문제를 다룰 때 자주 사용되는 활성화 함수
- 주로 다중 클래스 분류에서 각 클래스에 대한 확률 값을 계산하는 데 사용
- 입력 벡터의 값을 확률 분포로 변환하며, 각 클래스의 확률 값은 0과 1 사이에 위치하며, 전체 클래스의 확률 합은 1
- 이를 통해 모델은 각 클래스가 입력 데이터에 속할 확률을 계산 가능
- **수식**
  - **$\[\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}\]$**
  - \( z_i \): 입력 벡터의 i번째 값
  - \( K \): 총 클래스의 수
  - \( e^{z_i} \): \( z_i \)의 지수 값
- 장점
  - 결과를 확률로 변환하여 모델의 예측을 해석하기 쉽게 만든다
  - 하나의 예측에서 여러 클래스를 처리하는 데 적합
- 단점
  - 입력 값 간의 차이가 크면, Softmax는 큰 값을 가진 클래스의 확률을 매우 높게 만들고 나머지 클래스의 확률을 거의 0에 가깝게 만들어진다  

### 🔎 Best Path Decoding(최적 경로 디코딩)
- **Connectionist Temporal Classification (CTC)** 에서 사용되는 디코딩 방법 중 하나
- 각 시간 단계에서 가장 높은 확률을 가진 라벨을 선택하여 시퀀스를 구성
- CTC 네트워크의 출력이 시간적으로 길어져도, 각 시간 프레임에서 최적의 라벨만 고르면 되므로 효율적
- 각 시간 단계에서 선택된 최적의 라벨을 연결하여 최종 라벨 시퀀스를 생성
- 중복된 라벨이나 blank 레이블이 포함된 경우, 이를 제거하거나 조정하여 최종 출력을 완성
- 단점
  - 각 시간 단계에서의 출력이 독립적으로 선택되기 때문에 전체 시퀀스에서 최적의 라벨링을 항상 보장하지는 못함
  - 중간에 빈(blank) 라벨이 자주 발생하는 경우, 중요한 라벨이 잘못 선택될 수 있음 

### 🔎 Prefix Search Decoding(접두사 탐색 디코딩)
- **Connectionist Temporal Classification (CTC)** 에서 사용하는 디코딩 방법 중 하나로, **최적 경로 디코딩(Best Path Decoding)** 의 단점을 보완하기 위한 방법
- 입력 시퀀스에서 가능한 라벨링 접두사(prefix)를 확장해가면서 가장 높은 확률의 라벨 시퀀스 찾음
- 각 라벨의 확률 분포를 단계별로 확장해 나가는 방식
- Best Path Decoding은 각 시간 프레임에서 개별적으로 최적의 라벨을 선택하지만, Prefix Search Decoding은 시퀀스 전체를 고려하여 최적의 라벨 시퀀스를 선택
-  CTC 네트워크의 출력은 보통 **'blank(빈칸)'** 과 같은 값들이 포함되는데, 이 값들은 접두사 확장 과정에서 걸러짐
-  모든 가능한 접두사를 확장하면서 그 확률을 계산해야 하기 때문에, 입력 시퀀스가 길어질수록 확장해야 할 접두사의 수가 기하급수적으로 증가

### 🔎 heuristic
- 문제를 해결하거나 의사 결정을 내리는 데 사용되는 경험적 방법 또는 간단한 규칙을 의미
- 빠르고 간단하게 해결책을 찾기 위한 방법으로, 완벽하거나 최적의 결과를 보장하지는 않지만, 제한된 시간이나 정보 내에서 실용적이고 유용한 결과를 얻기 위한 방법으로 사용
- 경험 기반: Heuristic은 과거의 경험이나 관찰을 바탕으로 문제를 해결
  - 복잡한 문제를 풀 때 모든 가능한 해를 탐색하는 대신, 자주 효과가 있었던 경험적 규칙이나 직관에 의존
- 빠른 문제 해결: 최적의 해결책을 찾는 것이 아니라, 적당한 시간 안에 충분히 좋은 해결책을 찾는 데 중점
- 불완전성: Heuristic은 항상 최적의 결과를 보장하지는 않음
  - 많은 경우에 충분히 좋은 결과를 제공하여 실용적
- 탐색 효율성: Heuristic은 탐색 공간을 줄여줌
  - 모든 가능한 해를 탐색하지 않고, 가능성이 높은 해들만을 선택해 효율적으로 문제를 풀 수 있음    

<br>

## 4) Training the Network

### 🔎 Gradient descent(경사 하강법)

### 🔎 Maximum likelihood(
- **정확한 라벨(목표 라벨)** 에 도달할 가능성을 최대화하는 방식으로 모델을 학습시키는 방법
- 쉽게 말해, **목표 라벨이 실제로 나올 가능성을 최대한 높이는 방향으로 네트워크를 훈련시키는 것**

### 🔎 Log likelihood(
- **로그 우도**는 확률의 로그 값을 의미하며, 학습 과정에서 **이 값을 최대화**하는 것이 목적
- **목적 함수**를 최소화하면, 결과적으로 로그 우도 값이 커지며, 이는 **목표 라벨이 나올 가능성을 높이는 방향**으로 학습이 진행된다는 의미

### 🔎 Underflow
- 수치 계산에서 발생하는 문제로, 매우 작은 숫자를 표현하려고 할 때 발생하는 현상
- 컴퓨터의 부동 소수점 표현 방식에서 숫자의 범위는 제한적이며, 너무 작은 값은 컴퓨터가 정확하게 표현할 수 없을 정도로 작아질 수 있음
- 매우 작은 값을 계산하게 되면, 그 숫자가 부동 소수점 표현 방식의 하한보다 작아져 0으로 처리될 수 있음
  - 즉, 결과적으로 소실이 발생하여 잘못된 결과를 얻을 수 있는 상황을 underflow라고 함
- 해결방법
  - 로그 변환(log transformation): 작은 확률 값을 다룰 때, 값 자체를 사용하는 대신, 그 값의 로그 값을 사용하면 underflow 문제를 완화
  - 정규화(Normalization): 계산 중간중간에 값을 정규화(값을 일정 범위 내로 조정)하여 작은 값을 크게 만들어 underflow를 방지
  - 재스케일링(Rescaling): 아주 작은 값을 곱하거나 더하는 계산을 하기 전에 값을 스케일링(배율 변경)하여 underflow를 방지  
