# 트랜스포머 구조∙활용에 따른 언어모델의 분화
- 인코더만 활용
  - BERT (트랜스포머의 인코더 12개로 구성 - 자연어 이해에 강점을 보이는 모델)
- 인코더 + 디코더 구조
  - BART
  - T5
- 디코더만 활용
  - GPT
  - XLNet

# Transformer 소개
- 구글이 자연어처리를 위해 2017년 발표한 모델
- ChatGPT 역시 트랜스포머 기반 모델
- 자연어처리 뿐만 아니라 컴퓨터 비전이나 음성 인식 등 다른 분야에도 적용
- 인코더와 디코더를 모두 어텐션으로 구현한 모델
- Seq2Se에 비해 문장의 길이에 대한 제약이 없어짐
- 인코더가 인풋 문장을 보다 잘 이해하고 디코더가 자신이 앞서 생성한 단어들에 대해서도 보다 더 잘 이해할 수 있는 모델

# 구조와 원리
- **3가지 종류의 Attention 존재**
  - Encoder Self-Attention
  - Masked Decoder Self-Attention
  - Encoder-Decoder Attention
- 입력 문장을 토큰화해 사전을 만들고 토큰을 정수에 매핑시켜 임베딩 층을 통과하면 모델이 학습하기 위한 토큰들의 임베딩 값이 만들어 짐
  - 트랜스포머는 단어를 표현하는 임베딩 벡터와 모델 내 입출력 값이 모두 같은 512 차원
- 첫 번째 인코딩 츨에서는 **입력된 문장의 토큰들끼리 유사도 계산**
  - 512차원을 n개 head로 나눠서 학습함 (= **Multi-head Attention**)
 
# 기존 모델과의 차이점
- Transformer가 등장하기 전 NLP 분야에서는 주로 RNN, LSTM, GRU 등의 모델이 사용됨
- **RNN**
  - 순차적 데이터를 처리하기 위해 설계된 신경망
  - 먼 과거의 정보를 효과적으로 활용하지 못하는 한계 존재
- **LSTM**
  - RNN의 장거리 의존성 한계를 해결하기 위해 개발된 모델
  - 정보를 선택적으로 기억하고 잊을 수 있는 구조로 긴 시계열 데이터의 예측 정확도를 높임
- **GRU**
  - LSTM을 단순화한 모델
  - 비슷한 성능을 보이면서 계산 효율성을 개선함
- **Transformer**
  - 기존의 모델들은 순차적으로 데이터를 처리하나 transformer의 경우 문장의 모든 단어를 동시에 처리할 수 있음 **(병렬 처리)**
  - Self-Attention 메커니즘을 통해 문장 내 모든 단어 간의 관계를 직접적으로 계산 **(장거리 의존성 문제 해결)**
  - 병렬 처리가 가능해 문장 길이에 덜 민감, 다만 Self-Attention 연산의 복잡도는 문장 길이의 제곱에 비례 **(계산 복잡도 완화)**
  - 기존의 RNN 계열 모델은 이전 상태를 기억하기 위한 내부 메모리를 사용했으나 transformer의 경우 별도의 메모리 구조 없이 어텐션 메커니즘으로 정보 처리 **(메모리 사용 절약)**
 
# Transformers in Speech Processing
- 전체 문장을 한 번에 분석하므로 음성을 이해하는 데 장점 존재
