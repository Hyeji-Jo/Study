# 트랜스포머 구조∙활용에 따른 언어모델의 분화
- 2021년 기준 최신 고성능 모델들은 Transformer 아키텍처를 기반으로 함
- 인코더만 활용
  - BERT (트랜스포머의 인코더 12개로 구성 - 자연어 이해에 강점을 보이는 모델)
- 인코더 + 디코더 구조
  - BART
  - T5
- 디코더만 활용
  - GPT
  - XLNet

# Seq2Seq 모델의 한계
- seq2seq
  - 인코더-디코더 구조로 구성
  - 인코더 : 입력 시퀀스를 하나의 벡터 표현으로 압축
  - 디코더 : 해당 벡터 표현을 통해 출력 시퀀스 생성
- 단점
  - 인코더가 입력 시퀀스를 **하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨**
  - 해당 단점을 보정하기 위해 **Attention 활용**
  - Attention을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 **Attention만으로 인코더와 디코더를 만드는법 생각**
  <img width="1191" alt="image" src="https://github.com/user-attachments/assets/ebde630c-641a-4b26-b3d6-45ee3e1cdc44">
  

# 인코더 + 디코더
- 인코더
  - 입력 : 텍스트 또는 음성의 특징(피처) 벡터
  - 출력 : 입력의 숨겨진 표현(hidden representation)
  - 구성 : Self-Attention과 피드포워드 뉴럴 네트워크
- 디코더
  - 입력 : 인코더의 출력과 이전 디코더 출력(디코더는 순차적으로 작동)
  - 출력 : 최종 결과물(ex) 번역된 문장)
  - 구성 : Self-Attention, Encoder-Decoder Attention, 피드 포워드 뉴럴 네트워크

# Transformer 소개
- 구글이 자연어처리를 위해 2017년 발표한 모델
- ChatGPT 역시 트랜스포머 기반 모델
- 자연어처리 뿐만 아니라 컴퓨터 비전이나 음성 인식 등 다른 분야에도 적용
- 인코더와 디코더를 모두 어텐션으로 구현한 모델
- Seq2Se에 비해 문장의 길이에 대한 제약이 없어짐
- 인코더가 인풋 문장을 보다 잘 이해하고 디코더가 자신이 앞서 생성한 단어들에 대해서도 보다 더 잘 이해할 수 있는 모델

# 구조와 원리
- **3가지 종류의 Attention 존재**
  - Encoder Self-Attention
    - 입력 데이터의 각 부분이 서로 어떻게 관련되어 있는지 파악
    - 모델이 문장이나 음성 데이터의 맥락을 이해하는 데 중요한 역할 
  - Masked Decoder Self-Attention
  - Encoder-Decoder Attention
- 입력 문장을 토큰화해 사전을 만들고 토큰을 정수에 매핑시켜 임베딩 층을 통과하면 모델이 학습하기 위한 토큰들의 임베딩 값이 만들어 짐
  - 트랜스포머는 단어를 표현하는 임베딩 벡터와 모델 내 입출력 값이 모두 같은 512 차원
- 첫 번째 인코딩 츨에서는 **입력된 문장의 토큰들끼리 유사도 계산**
  - 512차원을 n개 head로 나눠서 학습함 (= **Multi-head Attention**)
 
# 기존 모델과의 차이점
- Transformer가 등장하기 전 NLP 분야에서는 주로 RNN, LSTM, GRU 등의 모델이 사용됨
- **RNN**
  - 순차적 데이터를 처리하기 위해 설계된 신경망
  - 먼 과거의 정보를 효과적으로 활용하지 못하는 한계 존재
- **LSTM**
  - RNN의 장거리 의존성 한계를 해결하기 위해 개발된 모델
  - 정보를 선택적으로 기억하고 잊을 수 있는 구조로 긴 시계열 데이터의 예측 정확도를 높임
- **GRU**
  - LSTM을 단순화한 모델
  - 비슷한 성능을 보이면서 계산 효율성을 개선함
- **Transformer**
  - 기존의 모델들은 순차적으로 데이터를 처리하나 transformer의 경우 문장의 모든 단어를 동시에 처리할 수 있음 **(병렬 처리)**
  - Self-Attention 메커니즘을 통해 문장 내 모든 단어 간의 관계를 직접적으로 계산 **(장거리 의존성 문제 해결)**
  - 병렬 처리가 가능해 문장 길이에 덜 민감, 다만 Self-Attention 연산의 복잡도는 문장 길이의 제곱에 비례 **(계산 복잡도 완화)**
  - 기존의 RNN 계열 모델은 이전 상태를 기억하기 위한 내부 메모리를 사용했으나 transformer의 경우 별도의 메모리 구조 없이 어텐션 메커니즘으로 정보 처리 **(메모리 사용 절약)**
 
# Transformers in Speech Recognition
- 전체 문장을 한 번에 분석하므로 음성을 이해하는 데 장점 존재
- 음성 신호를 텍스트로 변환하는데 사용
  1. 음성 입력 : 음성 신호 입력 받기
  2. 전처리 : 음성 신호를 작은 청크(chunk)로 나누고, 각 청크의 특징(ex) 스팩트로그램) 추출
  3. 트랜스포머 모델 적용
    - 인코더 : 음성 특징을 입력으로 받아 숨겨진 표현을 생성
    - 디코더 : 숨겨진 표현을 기반으로 텍스트 생성    
