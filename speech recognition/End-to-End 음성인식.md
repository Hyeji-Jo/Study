개요  
기승전결  
누가언제어떻게왜 만들어졌는지  
어떠한 종류가 있고  
그래서 다른 모델들과의 차이는 무엇이고  
그런거 등등  


# 1. 과거 음성인식 시스템의 발전과 한계
## HMM 기반 음성인식 (1980~2010년대 초반)
- **히든 마르코프 모델(HMM)** 은 음성을 통계적으로 분석하는 방식
- 순차데이터를 확률적으로 모델링 하는 생성 모델
  
- **개념**
  - observation(관측치) 뒤에 hidden(은닉)되어 있는 state(상태)를 추청하는 것
  - 우리가 보유하고 있는 observation(데이터)는 true hidden state(실제 은닉 상태들)이 노이즈가 낀 형태로 실현된 것이라고 보는 것 **(= noisy channel)**
  - ex. P(x=아이스크림 구매|q=더운 날씨)

- **주요 구성 요소(Parameters)**
  - **전이 확률(transition probability) A, 방출 확률(emission probablity) B, 초기 상태 분포(Initial State Distribution) π** 3가지 요소로 구성됨
  - 모든 전이 확률의 합 = **1**, 방출 확률의 합 = **1**, 초기 상태 분포의 합 = **1**
    - **전이 확률** : 숨겨진 상태(hidden state)에서 다른 상태로 이동할 확률을 의미 ex)"h" 다음에 "e"가 나올 확률을 의미
    - **방출 확률** : 숨겨진 상태가 실제로 관찰되는 데이터를 생성할 확률 ex)"h"가 특정 주파수의 음성 신호를 방출할 확률
    - **초기 상태 분포** : 첫 번째 숨겨진 상태가 무엇일지에 대한 확률 분포 ex) "h"-0.3, "s"-0.2, "a"-0.5

- 특정 음소(phoneme)를 인식하고 그 음소들이 조합되어 단어와 문장이 형성
- 이 방식에서는 음성인식 시스템이 **음향 모델(Acoustic Model), 발음 사전(Pronunciation Dictionary), 언어 모델(Language Model)의 세 가지 모듈로** 나뉘어 처리
  
- **문제점**
  - 모듈마다 독립적으로 훈련되어야 하고, 음성 데이터를 텍스트로 변환하는 여러 단계를 거쳐야 했습니다.
  - 이로 인해 시스템이 복잡하고 최적화가 어렵고, 새로운 언어나 방언에 대한 확장성이 떨어졌습니다.
  - 모듈 간 상호작용 부족, 복잡한 최적화 과정, 높은 계산 비용.

### Markov model
- 마르코프 모델은 state로 이루어진 Sequence를 상태 전이 확률 행렬로 표현하는 것
- **Markov 가정** : 시간 **t에서 관측은 가장 최근 r개의 관측에만 의존**한다는 가정
  - 한 상태에서 다른 상태로의 전이는 이전 상태의 긴 이력을 필요치 않다는 가정 
- **Hidden Markov Model** : 관측이 불가능한 process를 관측이 가능한 다른 process로 추정하는 이중 확률처리 모델
  - 관측이 가능한 Observable과 관측이 불가능한 Hidden state 구분하기 



## GMM-HMM 기반 ASR
- **GMM(Gaussian Mixture Model)** 은 음성 데이터에서 음향 모델을 추출하는 과정에서 사용
- 각 음소의 통계적 특성을 학습하여 음성 신호를 해석
- 문제점
  - GMM-HMM 시스템은 고정된 수학적 가정을 사용해 특정 주파수 패턴을 기반으로 분석하기 때문에,
  - 실제로 다양한 음성 신호를 정확하게 처리하기에는 한계가 있었습니다. 특히 노이즈나 발음의 변동성에 취약했습니다. 

## DNN-HMM 기반 ASR
- 전통적인 HMM 구조에 **심층 신경망(DNN)** 을 결합하여 음성 신호에서 패턴을 더 잘 학습할 수 있도록 개선한 방식
- 기존 GMM-HMM보다 성능이 뛰어나며 더 복잡한 음성 패턴을 학습
- 문제점
  - 여전히 다단계 모듈 구조를 사용하기 때문에 최적화와 훈련이 복잡

## End-to-End ASR



## Hybrid ASR
- 기존 HMM 기반 구조에 신경망을 결합한 하이브리드 시스템
- HMM, CTC, DNN 등 여러 기법을 결합하여 성능을 높이려는 접근 방식으로, 전통적인 ASR과 End-to-end ASR의 장점을 결합



# 2. End-to-End ASR 등장 배경(핵심 아이디어, 누가 어떻게 만들었는지)
- 2010년대에 들어서면서 **심층 신경망(DNN)**이 음성인식에 도입되기 시작했지만, 여전히 GMM-HMM 구조와 결합된 형태였습니다.
- 하지만 더 나은 성능을 위해 단일 신경망으로 음성인식을 처리하는 End-to-end 접근법이 필요하게 되었습니다.
- 음성 데이터를 바로 텍스트로 변환하는 하나의 신경망 모델을 사용해 여러 모듈을 통합하는 방식
- 이 방식은 복잡한 중간 단계를 없애고, 음성에서 바로 텍스트로 변환하는 과정을 단순화



# 3. End-to-End ASR의 종류(각 종류별 차이점)
- CTC 기반 ASR
  - Connectionist Temporal Classification(CTC)을 사용하여 음성 입력과 텍스트 출력 사이의 관계를 직접 학습
  - 음성 길이가 다를 때 발생하는 문제를 해결하며, 중간 단계를 생략할 수 있습
- Attention 기반 ASR (Seq2Seq)
  - Attention 메커니즘을 사용하여 음성의 중요한 부분을 선택하고, 입력과 출력 간의 관계를 더욱 정교하게 학습
  - 음성의 특정 부분에 집중하면서도 텍스트로 변환하는 능력이 뛰어
- Transformer 기반 ASR
  - 음성 데이터를 처리하기 위해 Self-Attention 메커니즘을 사용하는 최신 End-to-end ASR 모델
  - 실시간 음성 처리에 매우 적합하며, 대규모 음성 데이터에서 높은 성능을 발휘 


# ASR
- automatic speech recognition (자동음성인식)
- 음성 신호를 입력으로 받아들여 텍스트로 변환하는 기술
- 구성단계  
  - 푸리에 변환(Fourier transform)을 사용하여 음성 신호를 주파수 영역으로 변환한 후, 주파수 스펙트럼에서 특징적인 정보를 추출
  - 음성 신호의 특징 벡터를 입력으로 받아들여 음소(phoneme) 단위로 분할(음소 인식 모델을 사용하여 음성 신호를 음소 단위로 변환)
  - 음소 시퀀스를 텍스트로 변환(언어 모델과 결합하여 음성 신호를 텍스트로 해석)
