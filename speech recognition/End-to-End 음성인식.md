개요  
기승전결  
누가언제어떻게왜 만들어졌는지  
어떠한 종류가 있고  
그래서 다른 모델들과의 차이는 무엇이고  
그런거 등등  


# 1. 과거 음성인식 시스템의 발전과 한계
## HMM 기반 음성인식 (1980~2010년대 초반)
- **히든 마르코프 모델(HMM)** 은 음성을 통계적으로 분석하는 방식
- 특정 음소(phoneme)를 인식하고 그 음소들이 조합되어 단어와 문장이 형성
- 이 방식에서는 음성인식 시스템이 **음향 모델(Acoustic Model), 발음 사전(Pronunciation Dictionary), 언어 모델(Language Model)의 세 가지 모듈로** 나뉘어 처리
- 문제점
  - 모듈마다 독립적으로 훈련되어야 하고, 음성 데이터를 텍스트로 변환하는 여러 단계를 거쳐야 했습니다.
  - 이로 인해 시스템이 복잡하고 최적화가 어렵고, 새로운 언어나 방언에 대한 확장성이 떨어졌습니다.
  - 모듈 간 상호작용 부족, 복잡한 최적화 과정, 높은 계산 비용.


## GMM-HMM 기반 ASR
- **GMM(Gaussian Mixture Model)** 은 음성 데이터에서 음향 모델을 추출하는 과정에서 사용
- 각 음소의 통계적 특성을 학습하여 음성 신호를 해석
- 문제점
  - GMM-HMM 시스템은 고정된 수학적 가정을 사용해 특정 주파수 패턴을 기반으로 분석하기 때문에,
  - 실제로 다양한 음성 신호를 정확하게 처리하기에는 한계가 있었습니다. 특히 노이즈나 발음의 변동성에 취약했습니다. 

## DNN-HMM 기반 ASR
- 전통적인 HMM 구조에 **심층 신경망(DNN)** 을 결합하여 음성 신호에서 패턴을 더 잘 학습할 수 있도록 개선한 방식
- 기존 GMM-HMM보다 성능이 뛰어나며 더 복잡한 음성 패턴을 학습
- 문제점
  - 여전히 다단계 모듈 구조를 사용하기 때문에 최적화와 훈련이 복잡

## End-to-End ASR



## Hybrid ASR
- 기존 HMM 기반 구조에 신경망을 결합한 하이브리드 시스템
- HMM, CTC, DNN 등 여러 기법을 결합하여 성능을 높이려는 접근 방식으로, 전통적인 ASR과 End-to-end ASR의 장점을 결합



# 2. End-to-End ASR 등장 배경(핵심 아이디어, 누가 어떻게 만들었는지)
- 2010년대에 들어서면서 **심층 신경망(DNN)**이 음성인식에 도입되기 시작했지만, 여전히 GMM-HMM 구조와 결합된 형태였습니다.
- 하지만 더 나은 성능을 위해 단일 신경망으로 음성인식을 처리하는 End-to-end 접근법이 필요하게 되었습니다.
- 음성 데이터를 바로 텍스트로 변환하는 하나의 신경망 모델을 사용해 여러 모듈을 통합하는 방식
- 이 방식은 복잡한 중간 단계를 없애고, 음성에서 바로 텍스트로 변환하는 과정을 단순화



# 3. End-to-End ASR의 종류(각 종류별 차이점)
- CTC 기반 ASR
  - Connectionist Temporal Classification(CTC)을 사용하여 음성 입력과 텍스트 출력 사이의 관계를 직접 학습
  - 음성 길이가 다를 때 발생하는 문제를 해결하며, 중간 단계를 생략할 수 있습
- Attention 기반 ASR (Seq2Seq)
  - Attention 메커니즘을 사용하여 음성의 중요한 부분을 선택하고, 입력과 출력 간의 관계를 더욱 정교하게 학습
  - 음성의 특정 부분에 집중하면서도 텍스트로 변환하는 능력이 뛰어
- Transformer 기반 ASR
  - 음성 데이터를 처리하기 위해 Self-Attention 메커니즘을 사용하는 최신 End-to-end ASR 모델
  - 실시간 음성 처리에 매우 적합하며, 대규모 음성 데이터에서 높은 성능을 발휘 


# ASR
- automatic speech recognition (자동음성인식)
- 음성 신호를 입력으로 받아들여 텍스트로 변환하는 기술
- 구성단계  
  - 푸리에 변환(Fourier transform)을 사용하여 음성 신호를 주파수 영역으로 변환한 후, 주파수 스펙트럼에서 특징적인 정보를 추출
  - 음성 신호의 특징 벡터를 입력으로 받아들여 음소(phoneme) 단위로 분할(음소 인식 모델을 사용하여 음성 신호를 음소 단위로 변환)
  - 음소 시퀀스를 텍스트로 변환(언어 모델과 결합하여 음성 신호를 텍스트로 해석)
