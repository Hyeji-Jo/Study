개요  
기승전결  
누가언제어떻게왜 만들어졌는지  
어떠한 종류가 있고  
그래서 다른 모델들과의 차이는 무엇이고  
그런거 등등  

### Q&A
1. 음성에서의 sequence는 무엇을 의미하는가?
  - 음성 데이터에서의 sequence는 시간에 따라 연속적으로 변하는 신호의 흐름
  - 음성 신호는 연속적인 시간 흐름에 따른 여러 개의 특징들(예: 주파수, 진폭 등)을 포함하고 있기 때문에, 이를 **순차 데이터(sequential data)** 로 간주
  - 이런 연속적인 데이터를 처리하기 위해 HMM 같은 모델들이 사용되었고, 각 시간 단위의 상태나 음소를 예측하는 데 활용
  - 예를 들어, "hello"라는 단어는 "h", "e", "l", "o"라는 음소들이 시간 순서대로 나오는 것이고, 각 음소는 짧은 시간 동안 연속적인 신호로 표현
  - 이 시퀀스 정보는 음성의 맥락을 이해하고 정확한 발화를 인식하는 데 매우 중요

2. hmm, Gmm-HMM은 음향 모델내에 속하는 것인지?
  - HMM과 GMM-HMM은 모두 음향 모델의 핵심적인 방법론
3. 음소의 정의는?
  - **음소(Phoneme)** 는 말소리의 최소 단위
  - "bit"와 "pit"이라는 단어는 첫 소리인 /b/와 /p/가 다름으로써 의미가 달라짐 이때의 **/b/와 /p/ 같은 소리 단위가 바로 음소**
4. 왜 HMM이 대세로 굳었는지? 어떻게 나오게 됬는지?
  - 음성 신호의 시간적인 특성을 잘 모델링할 수 있는 **순차 데이터 처리 능력 덕분**


# 1. 과거 음성인식 시스템의 발전과 한계
<img width="1019" alt="image" src="https://github.com/user-attachments/assets/63a14901-fa96-441d-82cb-78e10d7b2224">

![image](https://github.com/user-attachments/assets/d6f01631-7fd1-4051-9d13-10486e305fcd)


- **언어 모델(Language Model = LM)**
  - 단어간의 결합 확률에 관한 지식
  - 단어 시퀀스의 확률을 모델링하여 문법적 일관성과 문맥을 고려
  - 어떤 단어가 다음에 올 가능성이 높은지를 예측하는 기능을 담당
  - 일반적으로 **n-그램 모델**을 사용하여 $\( P(w_{1}, w_{2}, \dots, w_{n}) \)$ 형태로 표현
    - n-gram 모델이 간단하고 효율적이며, 연산 비용이 상대적으로 낮다는 장점이있어서 사용
    - n-gram 모델은 과거의 단어 몇 개에만 의존하기 때문
    - **한계** : 문맥을 길게 고려하지 않기 때문에, 짧은 범위 내의 단어 관계만을 처리 가능하며 학습하지 않은 패턴이나 단어 조합은 모델이 예측할 수 없음
- **발음 사전(Pronunciation Dictionary)**
  - 단어와 음소간의 관계에 대한 지식
  - 단어를 음소의 시퀀스로 변환하여 음향 모델과 언어 모델을 연결하는 역할
  - 미리 만들어 두고 계속 사용
  - 특정 단어가 어떤 음소로 발음되는지 정의한 것으로 매번 새로 학습하거나 변경하지 않아도 사용 가능
- **음향 모델(Acoustic Model= AM)**
  - 음소와 소리의 관계에 대한 지식
  - 음성 신호와 음소(phoneme) 또는 음향 단위 간의 확률적 관계를 모델링
  - 관측값은 일반적으로 **MFCC**나 **필터뱅크** 같은 음성 특징 벡터
  - HMM
  - GMM-HMM
  - DNN-HMM

## 1) HMM 기반 음성인식 (1980~2010년대 초반)
![image](https://github.com/user-attachments/assets/ba275934-bb00-4283-8134-52f57f4ba13c)
![image](https://github.com/user-attachments/assets/ab5e010e-5ead-4475-a97d-d5c4f205cf76)

- **히든 마르코프 모델(HMM)** 은 음성을 통계적으로 분석하는 방식
- 순차데이터를 확률적으로 모델링 하는 생성 모델
- 특정 음소(phoneme)를 인식하고 그 음소들이 조합되어 단어와 문장이 형성
- 이 방식에서는 음성인식 시스템이 **음향 모델(Acoustic Model), 발음 사전(Pronunciation Dictionary), 언어 모델(Language Model)의 세 가지 모듈로** 나뉘어 처리

### Markov model
- 마르코프 모델은 state로 이루어진 Sequence를 상태 전이 확률 행렬로 표현하는 것
- **Markov 가정** : 시간 **t에서 관측은 가장 최근 r개의 관측에만 의존**한다는 가정
  - 한 상태에서 다른 상태로의 전이는 이전 상태의 긴 이력을 필요치 않다는 가정
  - $\[P(S_{t} | S_{t-1}, S_{t-2}, \dots, S_{1}) = P(S_{t} | S_{t-1})\]$
- **Hidden Markov Model** : 관측이 불가능한 process를 관측이 가능한 다른 process로 추정하는 이중 확률처리 모델
  - 관측이 가능한 Observable과 관측이 불가능한 Hidden state 구분하기
  
### 개념
- observation(관측치) 뒤에 hidden(은닉)되어 있는 state(상태)를 추청하는 것
- 우리가 보유하고 있는 observation(데이터)는 true hidden state(실제 은닉 상태들)이 노이즈가 낀 형태로 실현된 것이라고 보는 것 **(= noisy channel)**
- ex. P(x=아이스크림 구매|q=더운 날씨)
- 음성 인식에서는 **시간 동질 마르코프 연쇄(Time-homogeneous Markov Chain)** 를 사용
  - 이는 확률 분포가 시간과 무관하다는 의미
  - 3 states & left-to-right transition : **Left-to-right transition** 구조를 사용하며, 3개의 상태로 구성
  ![image](https://github.com/user-attachments/assets/4a259f9b-a70d-49ee-a44e-9f60316343df)

### 주요 구성 요소(Parameters)
- **전이 확률(transition probability) A, 방출 확률(emission probablity) B, 초기 상태 분포(Initial State Distribution) π** 3가지 요소로 구성됨
- 모든 전이 확률의 합 = **1**, 방출 확률의 합 = **1**, 초기 상태 분포의 합 = **1**
  - **전이 확률** : 숨겨진 상태(hidden state)에서 다른 상태로 이동할 확률을 의미
    - ex)"h" 다음에 "e"가 나올 확률을 의미
    - $\( A_{ij} = P(S_{t} = j | S_{t-1} = i) \)$
  - **방출 확률** : 숨겨진 상태가 실제로 관찰되는 데이터를 생성할 확률
    - ex)"h"가 특정 주파수의 음성 신호를 방출할 확률
    - $\( B_{j}(o_{t}) = P(O_{t} = o_{t} | S_{t} = j) \)$
  - **초기 상태 분포** : 첫 번째 숨겨진 상태가 무엇일지에 대한 확률 분포
    - ex) "h"-0.3, "s"-0.2, "a"-0.5
    - $\( \pi_{i} = P(S_{1} = i) \)$


### HMM 알고리즘
- **Forward Algorithm**
  - 주어진 모델에서 관측된 시퀀스가 나올 확률을 계산
  - HMM의 상태와 전이 확률을 기반으로, 각 시간 단계에서 관측된 데이터가 발생할 확률을 계산
  - 초기 상태에서 시작하여 각 상태에 대한 확률을 누적하여 최종적인 관측 시퀀스의 확률을 얻음
  - 특정 음성이 주어졌을 때, 그 음성이 특정 상태(예: 음소)에서 나올 확률을 계산하고자 할 때 사용
- **Viterbi Algorithm**
  - 가장 가능성 있는 숨겨진 상태 시퀀스를 찾는 알고리즘, 실제 음소들이 발음된 시퀀스를 추정하는 데 사용
  - 주어진 관측 시퀀스에 대해, 각 시간 단계에서 가장 높은 확률을 가지는 상태를 선택(동적 프로그래밍 사용)
  - 최종적으로, 선택된 상태 시퀀스가 가장 높은 확률을 가진 숨겨진 상태 경로를 제공
  - 실제 발음된 음소의 시퀀스를 추정하고자 할 때, 즉 어떤 단어가 발음되었는지 알아내고자 할 때 사용
- **Baum-Welch Algorithm**
  - 모델의 파라미터를 학습하는 데 사용되는 알고리즘으로, EM(Expectation-Maximization) 알고리즘의 한 형태
  - HMM의 파라미터(전이 확률, 방출 확률 등)를 학습
  - EM(Expectation-Maximization) 알고리즘을 기반으로 하며, 관측된 데이터와 모델의 파라미터를 반복적으로 업데이트하여 최적의 파라미터를 찾음
  - E 단계에서 현재 파라미터를 사용하여 각 상태의 확률 분포를 계산하고, M 단계에서 이러한 확률을 기반으로 모델의 파라미터를 업데이트


### HMM의 한계
- 모듈마다 독립적으로 훈련되어 복잡하고 최적화가 어렵습니다.
- 새로운 언어나 방언에 대한 확장성이 떨어집니다.
- 상태 지속 시간이 지수 분포를 따른다는 가정이 실제 음성에서는 맞지 않을 수 있습니다.
- 발음 변이, 억양, 발화 속도 등의 다양한 음성 변형을 처리하기 어려운 점이 있습니다.
- HMM 자체는 상태 전이에 대한 확률 모델을 제공하지만, 각 상태에서 관측 데이터(음성 신호의 특징 벡터)가 발생하는 확률을 더 복잡하게 모델링하는 능력이 부족
- 음성 신호는 고차원적이고 복잡한 패턴을 포함하는데, 단순한 통계적 모델로는 이를 충분히 설명하기 어려움


## 2) Hybrid ASR
- 기존 HMM 기반 구조에 신경망을 결합한 하이브리드 시스템
- HMM, CTC, DNN 등 여러 기법을 결합하여 성능을 높이려는 접근 방식으로, 전통적인 ASR과 End-to-end ASR의 장점을 결합
- **HMM(Hidden Markov Model)** 을 상태 전이(state transition) 모델로 사용하고, 음향 모델(Acoustic Model) 부분을 다른 기술(GMM, DNN, LSTM 등)과 결합하는 형태가 기본
- HMM은 특히 음성의 속도 변화에 어느 정도 불변한 성질을 가지는데, 이는 빠른 발음과 느린 발음을 하나의 모델로 표현할 수 있도록 함
- HMM은 **관측 데이터(observations)** 의 복잡성을 충분히 설명하지 못하는 한계가 존재
  - 음성 신호는 매우 비선형적이고 복잡한 특성을 지니는데, HMM은 단순한 확률 분포로는 이를 충분히 모델링할 수 없어서 등장한게 하이브리드 모델 

### 1. GMM-HMM (Gaussian Mixture Model - Hidden Markov Model)
- **특징**
  - 음향 모델로 **GMM(가우시안 혼합 모델)** 을 사용
  - **각 음소**(음성의 기본 단위)를 가우시안 혼합 분포로 모델링하여 HMM의 각 상태에서의 방출 확률을 계산
  - HMM은 음소가 시간에 따라 어떻게 전이되는지를 모델링하며, GMM은 각 음소의 통계적 특성을 학습하여 관측된 음성 신호의 음향적 특성을 설명
  - **EM 알고리즘**을 사용하여 GMM의 파라미터(평균, 공분산 등)를 최적화

- **장점**
  - **전통적이고 안정적인 모델**로 많은 음성 인식 시스템에서 활용되었음
  - **수학적 기반**이 명확하여 직관적이며, 적은 데이터에서도 성능이 우수

- **단점**
  - **비선형적인 패턴**을 충분히 설명하지 못함
  - **고차원 음성 데이터**를 학습하는 데 한계가 있음
  - GMM의 가우시안 분포는 모든 음향 패턴을 완벽히 설명하지 못하고, 다양한 상황에서 **성능이 떨어짐**
  - **노이즈**나 발음의 변동성에 취약

### 2. DNN-HMM (Deep Neural Network - Hidden Markov Model)
- **특징**
  - 음향 모델로 **DNN(심층 신경망)** 을 사용
  - DNN은 **비선형적인 음성 신호의 패턴을 학습**하는 데 뛰어나며, HMM의 각 상태에서의 방출 확률을 더 정교하게 계산
  - 다층의 뉴런(neuron)을 통해 입력 데이터의 **복잡한 패턴**을 학습하고, 높은 차원의 음향 특징도 잘 처리
  - HMM은 여전히 음소 간의 **시간적 전이**를 모델링하는 데 사용

- **장점**
  - GMM-HMM보다 **더 높은 표현력**을 지님. 특히 복잡한 음성 신호 패턴을 학습하는 데 적합
  - **대용량 데이터**를 사용하여 모델 성능을 극대화할 수 있음
  - 다양한 데이터와 상황에서 **더 우수한 성능**을 발휘하며, 더 복잡한 음성 데이터를 잘 학습

- **단점**
  - **많은 양의 데이터**와 학습 시간이 필요함
  - **계산 비용**이 GMM-HMM보다 훨씬 높음
  - **과적합**(overfitting)에 주의해야 함
  - 기존 GMM-HMM 대비 **구조가 복잡**하고, 이해와 구현이 더 어려움
 
### 3. LSTM-HMM (Long Short-Term Memory - Hidden Markov Model)
- **특징**
  - 음향 모델로 **LSTM**을 사용
  - LSTM은 **RNN의 변형**으로, 시간에 따른 데이터의 장기 의존성을 학습하는 데 적합
  - **음성 신호의 연속성**과 **긴 시퀀스의 의존성**을 더 잘 학습할 수 있도록 설계되었으며, 음소의 시간적 패턴을 더 정확하게 모델링
  - HMM은 음소의 시간적 전이를 설명하는 데 사용되며, LSTM은 각 상태에서의 음성 특징을 더 잘 학습함

- **장점**
  - 시간적으로 **긴 시퀀스**에서도 성능이 뛰어나며, **장기 의존성(long-term dependencies)** 문제를 해결
  - 음성 신호의 **복잡한 시간적 패턴**을 더 잘 모델링함
  - **복잡한 음성 데이터**와 다양한 상황에서도 더 정확한 예측 가능

- **단점**
  - **학습 비용**이 매우 높고, 훈련 시간이 길어짐
  - **메모리 사용량**이 커서 계산 자원이 많이 필요함
  - 다른 모델들보다 더 복잡한 구조로 인해 **구현과 최적화**가 어려움
  - GMM-HMM이나 DNN-HMM보다 **훈련 데이터에 민감**함

### 4. 모델 간 차이점 요약

| 모델 | 음향 모델 | 장점 | 단점 |
| --- | --- | --- | --- |
| **GMM-HMM** | Gaussian Mixture Model | - 단순하고 안정적<br>- 적은 데이터에서도 성능 우수 | - 비선형적 패턴 학습 한계<br>- 고차원 데이터에 취약 |
| **DNN-HMM** | Deep Neural Network | - 비선형적 패턴을 잘 학습<br>- 대규모 데이터에서 성능 우수 | - 계산 비용이 높음<br>- 과적합 가능성 |
| **LSTM-HMM** | Long Short-Term Memory | - 장기 의존성 문제 해결<br>- 긴 시퀀스에서 뛰어난 성능 | - 학습 및 메모리 비용이 큼<br>- 계산 자원 요구 |

### 결론
- **GMM-HMM**은 전통적인 모델로, 적은 데이터나 간단한 음성 인식 시스템에 적합하지만 **복잡한 음성 신호**에는 한계가 있음.
- **DNN-HMM**은 더 복잡한 음성 패턴을 학습할 수 있으며 **대용량 데이터**에서 성능이 우수하지만, **계산 자원이 많이 필요**하고 **구조가 복잡**함.
- **LSTM-HMM**은 시간적 의존성을 더 잘 처리하는 모델로, **긴 시퀀스 데이터**에서도 높은 성능을 발휘하지만, 그만큼 **계산 자원**과 **시간**이 많이 요구됨.


## 2-1) Hybrid ASR - GMM-HMM 기반 ASR
![image](https://github.com/user-attachments/assets/b3d157c1-cdf5-48e3-bb62-81f64fc9c882)

- **GMM(Gaussian Mixture Model)** 은 음성 데이터에서 음향 모델을 추출하는 과정에서 사용
- 각 음소의 통계적 특성을 학습하여 음성 신호를 해석

### GMM
- **음성 신호의 음향 특성을 모델링하는 데 사용되는 통계적 기법**
- 음성 신호는 매우 복잡하고 **비선형적인 패턴**을 포함하고 있기 때문에, 이를 더 세밀하게 설명할 수 있는 모델이 필요
- **여러 개의 가우시안 분포를 섞은 혼합 분포(Mixture of Gaussians)** 로 음성 신호의 복잡한 패턴을 설명
- **Expectation-Maximization (EM) 알고리즘**
  - **E-단계 (Expectation)**: 현재 파라미터를 기반으로 각 데이터 포인트가 각 가우시안에 속할 확률 계산
  - **M-단계 (Maximization)**: E-단계에서 계산된 확률을 사용하여 모델 파라미터를 업데이트, 이 과정을 반복하여 파라미터가 수렴할 때까지 진행 


### 배경
- HMM의 성능을 강화하고 음성 데이터를 더 잘 모델링하기 위해 등장
- 음성의 연속적인 특성과 비선형적인 음성 신호의 복잡성을 모두 다룰 수 있는 강력한 모델

### 개념
- **GMM-HMM(Gaussian Mixture Model-Hidden Markov Model)**
- HMM은 시퀀스 데이터의 시간적 변화를 모델링하는 데 사용되며, 각 상태는 특정 음성(또는 소리)의 단위(예: 음소)를 나타냄
- GMM은 각 HMM 상태에 대응하는 음성 특성(예: 주파수 분포)을 모델링하는 데 사용
- GMM은 HMM의 **방출 확률(Observation Probability)** 을 모델링하는 데 사용

### 구성 요소(Parameters)
- **HMM**의 주요 구성 요소는 여전히 전이 확률(transition probability) A, 초기 상태 분포(Initial State Distribution) π와 함께, GMM을 통해 방출 확률(emission probability) B가 모델링
- **GMM**의 파라미터는 각 가우시안의 평균 벡터(μ), 공분산 행렬(Σ), 혼합 계수(π)를 포함
  - 혼합 계수: 각 가우시안의 기여도를 나타내며, 모든 혼합 계수의 합은 1
  - 가우시안 분포: 방출 확률은 GMM을 사용하여 다음과 같이 표현
    - $B_j(o_t) = \sum_{k=1}^{K} \pi_k^{(j)} \cdot \mathcal{N}(o_t | \mu_k^{(j)}, \Sigma_k^{(j)})$  

### 알고리즘
- GMM-HMM 모델에서는 HMM의 알고리즘(Forward, Viterbi, Baum-Welch 알고리즘)을 사용하여 음성 신호를 인식하고 모델의 파라미터를 학습
- **Forward-Backward 알고리즘**: 주어진 모델에서 관측된 시퀀스가 나올 확률을 계산하며, GMM을 통해 각 상태에서의 방출 확률을 고려
- **Viterbi 알고리즘**: 가장 가능성 있는 숨겨진 상태 시퀀스를 찾는 알고리즘으로, GMM을 사용하여 각 상태에서의 관측값에 대한 최대 가능성 평가
- **Baum-Welch 알고리즘**: GMM의 파라미터(평균, 공분산, 혼합 계수)를 포함하여 HMM의 전이 확률과 초기 상태 분포를 학습


### 구조와 동작 원리
1. **음성 신호 처리 및 특징 추출**  
   - 음성 신호를 입력받아 이를 처리하여 유용한 정보를 추출하는 단계
   - 음성 신호는 시간 영역에서 주파수 영역으로 변환
   - **MFCC (Mel-Frequency Cepstral Coefficients)** 또는 **PLP (Perceptual Linear Predictive)** 등의 음성 특징 벡터가 이 단계에서 추출
2. **음향 모델 (Acoustic Model)**  
   - 음성 특징 벡터를 입력으로 받아, 그것이 어떤 음소(phoneme)에 해당하는지를 확률적으로 예측하는 단계
   - HMM은 시간적으로 음소가 어떻게 진행되는지를 설명
   - 각 HMM의 상태는 음성 신호에서 나타나는 주파수 특징을 확률적으로 설명하는데 이때 GMM이 사용됨
   - GMM은 여러 개의 가우시안 분포를 혼합하여 음성 특징 벡터가 주어진 상태에서 발생할 확률을 모델링
3. **발음 사전 (Pronunciation Dictionary)**  
   - 텍스트의 각 단어가 어떻게 음소들의 시퀀스로 변환되는지를 정의하는 역할
   - **발음 규칙 정의**: 발음 사전에는 각 단어에 대한 음소 시퀀스 정보가 포함됨  
     예: "speech"라는 단어는 ["s", "p", "ee", "ch"]라는 음소로 이루어짐
   - **모델의 연계**: 발음 사전은 음향 모델과 언어 모델을 연결
4. **언어 모델 (Language Model)**  
   - 인식된 음소나 단어들이 실제로 어떤 문장을 구성할지를 확률적으로 판단
   - **n-gram 모델**: 각 단어가 문장에서 나올 확률을 계산
     - 예: 2-gram 모델은 "the" 다음에 "cat"이 나올 확률을 계산
   - **문맥 반영**: 언어 모델은 문맥을 반영하여 인식된 단어의 순서를 결정


### 한계
- **GMM-HMM 시스템의 고정된 가정**: 고정된 수학적 가정을 사용해 특정 주파수 패턴을 분석
  - 다양한 음성 신호를 정확하게 처리하기에 한계가 있으며, 노이즈나 발음의 변동성에 취약
- **시간적 정보의 한계**: HMM은 각 상태 간의 전이만을 고려하며, 복잡한 시간적 상관관계를 완전히 다루지 못함
- **계산 비용이 높음**: 많은 파라미터를 포함하고 있어 학습이 느리고 계산 비용이 높음
- **GMM의 가정이 제한적**: 각 상태에서 관찰값이 가우시안 분포를 따른다는 가정은 모든 음성 데이터에 적합하지 않을 수 있음

  

## 2-2) Hybrid ASR - DNN-HMM 기반 ASR
- 전통적인 HMM 구조에 **심층 신경망(DNN)** 을 결합하여 음성 신호에서 패턴을 더 잘 학습할 수 있도록 개선한 방식
- 기존 GMM-HMM보다 성능이 뛰어나며 더 복잡한 음성 패턴을 학습
- **DNN (Deep Neural Network)** 은 음성 데이터에서 음향 모델을 추출하는 과정에서 사용
- GMM을 대신해 DNN을 사용하여 더 복잡한 음성 신호의 패턴을 학습

### DNN
- **심층 신경망(Deep Neural Network)** 은 다층 퍼셉트론(MLP) 구조를 기반으로 하는 **지도 학습(Supervised Learning)** 모델
- 다층 네트워크를 통해 비선형적인 패턴을 학습할 수 있음
- **GMM의 한계를 극복**하기 위해 도입되었으며, 음성 신호의 복잡한 패턴을 더욱 잘 설명
- 음성 특징 벡터(예: MFCC)를 입력받아 **음향 모델(Accoustic Model)**로 학습

### 배경
- GMM-HMM 모델의 한계로 인해 음성 데이터의 복잡한 패턴을 더 잘 모델링할 수 있는 DNN이 등장
- DNN-HMM 모델은 GMM-HMM보다 **더 높은 표현력**을 가지며, 다양한 음성 신호의 비선형적 특성을 더 잘 학습할 수 있음

### 개념
- **DNN-HMM (Deep Neural Network-Hidden Markov Model)** 
- HMM은 시퀀스 데이터의 시간적 변화를 모델링하는 데 사용되며, 각 상태는 특정 음성(또는 소리)의 단위(예: 음소)를 나타냄
- DNN은 각 HMM 상태에 대응하는 음성 특성을 모델링하여 GMM보다 더 복잡한 패턴을 학습할 수 있음
- **DNN은 HMM의 방출 확률(Observation Probability)을 모델링하는 역할**을 수행

### 구성 요소(Parameters)
- **HMM**의 주요 구성 요소는 전이 확률(transition probability) A, 초기 상태 분포(Initial State Distribution) π, 그리고 **DNN**을 통해 방출 확률(emission probability) B가 모델링
- **DNN**의 파라미터는 각 은닉층의 가중치와 편향(bias)을 포함
  - DNN은 입력으로 음성 특징 벡터(MFCC 또는 다른 음성 특징)를 받고, 이를 통해 HMM 상태의 방출 확률을 예측

### 알고리즘
- DNN-HMM 모델에서는 **Backpropagation** 알고리즘을 사용하여 DNN의 가중치를 학습
- HMM의 알고리즘(Forward, Viterbi, Baum-Welch 알고리즘)은 그대로 사용되며, 방출 확률을 예측하는 부분만 DNN이 담당
- **Forward-Backward 알고리즘**: 주어진 모델에서 관측된 시퀀스가 나올 확률을 계산하며, DNN을 통해 각 상태에서의 방출 확률을 계산
- **Viterbi 알고리즘**: 가장 가능성 있는 숨겨진 상태 시퀀스를 찾는 알고리즘으로, DNN을 사용하여 각 상태에서의 관측값에 대한 최대 가능성 평가
- **Baum-Welch 알고리즘**: DNN의 파라미터를 학습하기 위해 **사후 확률(Posteriors)** 을 사용

### 구조와 동작 원리
1. **음성 신호 처리 및 특징 추출**  
   - 음성 신호를 입력받아 이를 처리하여 유용한 정보를 추출하는 단계
   - 음성 신호는 시간 영역에서 주파수 영역으로 변환
   - **MFCC (Mel-Frequency Cepstral Coefficients)** 또는 **PLP (Perceptual Linear Predictive)** 등의 음성 특징 벡터가 이 단계에서 추출
2. **음향 모델 (Acoustic Model)**  
   - 음성 특징 벡터를 입력으로 받아, 그것이 어떤 음소(phoneme)에 해당하는지를 예측하는 단계
   - DNN은 입력받은 특징 벡터를 기반으로 각 음소에 대한 방출 확률을 계산
   - HMM은 시간적으로 음소가 어떻게 진행되는지를 설명하며, DNN은 각 상태의 방출 확률을 제공
3. **발음 사전 (Pronunciation Dictionary)**  
   - 텍스트의 각 단어가 어떻게 음소들의 시퀀스로 변환되는지를 정의하는 역할
   - **발음 규칙 정의**: 발음 사전에는 각 단어에 대한 음소 시퀀스 정보가 포함됨  
     예: "speech"라는 단어는 ["s", "p", "ee", "ch"]라는 음소로 이루어짐
   - **모델의 연계**: 발음 사전은 음향 모델과 언어 모델을 연결
4. **언어 모델 (Language Model)**  
   - 인식된 음소나 단어들이 실제로 어떤 문장을 구성할지를 확률적으로 판단
   - **n-gram 모델**: 각 단어가 문장에서 나올 확률을 계산
     - 예: 2-gram 모델은 "the" 다음에 "cat"이 나올 확률을 계산
   - **문맥 반영**: 언어 모델은 문맥을 반영하여 인식된 단어의 순서를 결정

### DNN-HMM의 장점
- **비선형적 데이터 학습 능력**: DNN은 GMM보다 복잡한 음성 데이터의 패턴을 더 잘 학습할 수 있음
- **더 높은 표현력**: DNN은 다층 구조를 사용하여 데이터의 다양한 패턴과 변동성을 포착
- **성능 개선**: DNN-HMM은 GMM-HMM보다 높은 인식률을 제공하며, 다양한 환경에서 더 좋은 성능을 보임

### 한계
- **계산 비용**: DNN은 복잡한 네트워크 구조로 인해 학습 시간이 길고 계산 비용이 높음
- **데이터 요구량**: DNN은 대량의 데이터가 필요하며, 데이터가 적을 경우 과적합(overfitting)의 문제가 발생할 수 있음
- **시간적 정보 부족**: DNN은 HMM에 통합되어 시간적 정보를 모델링하지만, RNN(Recurrent Neural Networks) 같은 시간적 모델보다 시퀀스의 시간적 정보 처리가 부족함

  
## 2-3) Hybrid ASR - LSTM-HMM 기반 ASR

- **LSTM(Long Short-Term Memory)** 은 음성 데이터에서 음향 모델을 추출하는 과정에서 RNN(Recurrent Neural Network)의 한 종류로, 긴 시퀀스 내의 시간적 의존성을 더 잘 학습할 수 있도록 설계됨.
- 음성의 연속적인 패턴을 학습하고, 시퀀스 간의 의존성을 잘 처리함으로써 더 나은 성능을 보임.

### LSTM
- **장기 의존성(Long-Term Dependencies)** 문제를 해결하는 RNN의 변형
- 음성 신호는 길고 복잡한 패턴을 포함하고 있어, 기존 RNN은 시간적으로 먼 정보와 가까운 정보를 잘 학습하지 못하는 문제가 있었음
- **LSTM**은 이러한 한계를 극복하기 위해 게이트 메커니즘(입력 게이트, 출력 게이트, 망각 게이트)을 도입하여, 정보의 흐름을 더 효율적으로 관리함
- **기억 셀(Cell State)**과 **게이트(Gate)** 메커니즘을 통해 정보를 선택적으로 유지하거나 잊어버릴 수 있음
  - **입력 게이트**: 새로운 정보를 얼마나 받아들일지 결정
  - **망각 게이트**: 이전 정보를 얼마나 버릴지 결정
  - **출력 게이트**: 현재 정보를 얼마나 출력할지 결정

### 배경
- **RNN** 기반의 모델들은 시간적 순서가 중요한 데이터를 처리하는 데 적합하지만, 긴 시퀀스를 다루는 데는 한계가 있었음
- LSTM은 긴 시퀀스 데이터의 학습을 개선하기 위해 등장했으며, 음성 신호의 시간적 의존성을 잘 모델링할 수 있음
- **HMM의 시간적 모델링 능력**을 보완하여 더 정밀한 음성 모델을 구현하고자 함

### 개념
- **LSTM-HMM(LSTM-Hidden Markov Model)**
  - HMM은 시퀀스 데이터의 시간적 변화를 모델링하며, 각 상태는 특정 음성(음소)을 나타냄
  - LSTM은 HMM의 각 상태에 해당하는 음성 신호의 시간적 특징을 더 잘 학습하고, 음향 모델로서 더 나은 예측을 수행
  - LSTM은 긴 시퀀스에서도 시간적 정보를 잃지 않고 학습함으로써, 음성의 패턴을 더 잘 예측

### 구성 요소(Parameters)
- **HMM**의 주요 구성 요소는 여전히 전이 확률(transition probability) A, 초기 상태 분포(Initial State Distribution) π이며, 방출 확률(emission probability) B는 LSTM을 통해 모델링
- **LSTM**의 파라미터는 각 셀의 가중치(입력 게이트, 망각 게이트, 출력 게이트에 해당하는 가중치와 편향)를 포함
  - **가중치(W), 편향(b)**: 네트워크에서 정보 흐름을 조정하는 데 사용
  - **기억 셀(Cell State)**: 시퀀스의 중요한 정보를 유지하는 역할을 함

### 알고리즘
- LSTM-HMM 모델에서는 **Forward**, **Viterbi**, **Baum-Welch 알고리즘**을 사용하여 음성 신호를 인식하고 모델의 파라미터를 학습
  - Forward Algorithm: 주어진 모델에서 관측된 시퀀스가 나올 확률을 계산
  - Viterbi Algorithm: 가장 가능성 있는 숨겨진 상태 시퀀스를 찾음
  - Baum-Welch Algorithm: HMM의 전이 확률과 초기 상태 분포 학습
- LSTM은 음성 신호의 시간적 의존성을 모델링하여 더 정확한 방출 확률을 제공

### 구조와 동작원리
1. **음성 신호처리 및 특징 추출**
   - 음성 신호를 입력받아 주파수 영역으로 변환하여 MFCC(Mel-Frequency Cepstral Coefficients) 등의 음성 특징 벡터를 추출
2. **음향 모델(Acoustic Model)**
   - HMM은 시간적으로 음소가 어떻게 전이하는지를 설명하며, 각 상태에서 발생하는 음성 신호를 LSTM이 모델링
   - LSTM은 긴 시퀀스 내에서도 시간적 의존성을 유지하여 음성 신호의 패턴을 학습
3. **발음 사전(Pronunciation Dictionary)**
   - 텍스트의 각 단어가 음소의 시퀀스로 변환되는 규칙을 정의
4. **언어 모델(Language Model)**
   - 인식된 음소나 단어들이 실제로 어떤 문장을 구성할지를 확률적으로 판단
   - n-gram 모델을 사용하여 단어의 순서를 결정

### 한계
- LSTM-HMM은 더 나은 성능을 보이지만, 여전히 **HMM의 한계**(독립 가정 등)를 가짐
- LSTM의 **학습 비용**이 높고, **훈련 데이터**가 충분하지 않으면 성능이 저하될 수 있음
- 전통적인 HMM 구조를 유지하면서 LSTM을 결합했기 때문에 완전한 End-to-End 방식보다 더 복잡하고 계산 자원이 많이 요구됨

  
## 3) End-to-End ASR
<img width="818" alt="image" src="https://github.com/user-attachments/assets/0092af4a-42e2-4bc2-86e5-98070c8b8542">

![image](https://github.com/user-attachments/assets/dea3f1b8-350b-4bde-addb-1f1aa3e72363)

### 개념
- **End-to-End ASR**는 음성 신호 입력을 텍스트 출력으로 직접 매핑하는 방식의 음성 인식 시스템
- 전통적인 ASR 시스템에서 사용되는 **음향 모델, 언어 모델, 발음 사전** 등의 구성 요소를 통합하여 **단일 신경망 모델**로 구현
- 학습 데이터로부터 직접적으로 입력과 출력을 연결하여, 중간 단계의 필요성을 줄이고 효율성을 극대화

### 구조
1. **음성 신호 입력**: 입력 음성 신호를 처리하여 특징 벡터(예: MFCC, Spectrogram 등)로 변환
2. **신경망 모델**: 입력된 특징 벡터를 통해 텍스트 출력을 생성하는 신경망 구조
   - **CNN(Convolutional Neural Networks)**, **RNN(Recurrent Neural Networks)**, **LSTM(Long Short-Term Memory)** 또는 **Transformer** 모델 등이 사용
3. **출력 생성**: 모델의 최종 출력은 주어진 음성 신호에 대한 텍스트(단어, 문장 등)

### 특징
- **단일 모델**: 기존 ASR 시스템과 달리 여러 단계를 단일 신경망으로 통합
- **자연어 처리와의 통합**: 음성 인식 뿐만 아니라, 텍스트 후처리와 같은 자연어 처리 기술을 통합 가능
- **딥러닝 활용**: 최신 딥러닝 기법을 활용하여 고차원의 음성 데이터에서 직접적으로 학습

### 장점
- **단순한 파이프라인**: 여러 단계를 거치지 않아 시스템 구조가 단순해지고, 개발과 유지 보수가 용이
- **효율적인 학습**: 대량의 데이터를 통해 직접적으로 입력-출력 매핑을 학습하므로, 과거의 정보가 필요 없음
- **비지도 학습 가능성**: 라벨링이 부족한 데이터를 활용할 수 있는 가능성
- **성능 향상**: 최신 딥러닝 모델을 사용하여 전통적인 ASR 시스템보다 우수한 성능을 발휘할 수 있음

### 단점
- **데이터 요구량**: 대량의 레이블이 있는 학습 데이터가 필요하여, 데이터 수집이 어려운 경우 성능 저하 가능성
- **해석의 어려움**: 내부 작동 방식이 복잡하여, 모델의 작동 원리를 이해하고 설명하기 어려움
- **노이즈와 변동성 문제**: 다양한 발음이나 배경 소음에 대한 저항력이 떨어질 수 있음
- **테스트와 일반화**: 특정 도메인이나 언어에 최적화된 모델이 다른 상황에서 잘 작동하지 않을 수 있음

### 결론
- **End-to-End ASR**는 음성 인식의 미래로 간주되며, 특히 대량의 데이터가 활용될 수 있는 환경에서 높은 성능을 발휘하는 경향이 있음.
- 전통적인 ASR 시스템이 갖는 복잡성을 줄이고, 최신 딥러닝 기술을 통해 보다 직관적이고 효율적인 음성 인식 솔루션을 제공.





# 2. End-to-End ASR 등장 배경(핵심 아이디어, 누가 어떻게 만들었는지)
- 2010년대에 들어서면서 **심층 신경망(DNN)** 이 음성인식에 도입되기 시작했지만, 여전히 GMM-HMM 구조와 결합된 형태였습니다.
- 하지만 더 나은 성능을 위해 단일 신경망으로 음성인식을 처리하는 End-to-end 접근법이 필요하게 되었습니다.
- 음성 데이터를 바로 텍스트로 변환하는 하나의 신경망 모델을 사용해 여러 모듈을 통합하는 방식
- 이 방식은 복잡한 중간 단계를 없애고, 음성에서 바로 텍스트로 변환하는 과정을 단순화

#### 1. 전통적인 ASR 시스템의 한계
- **복잡한 구조**: 전통적인 음성 인식 시스템은 음향 모델, 언어 모델, 발음 사전 등 여러 구성 요소로 나뉘어 있으며, 각 단계에서의 오류가 전체 성능에 영향을 미침.
- **의존성 문제**: 각 단계가 서로 독립적으로 작동하기 때문에, 한 단계의 성능 저하가 최종 인식 결과에 심각한 영향을 미칠 수 있음.
- **데이터 처리의 비효율성**: 각 단계에서 처리된 결과를 결합해야 하므로, 데이터 전송 및 처리의 비효율성이 발생.

#### 2. 딥러닝 기술의 발전
- **신경망의 성공**: 딥러닝 기술, 특히 CNN(Convolutional Neural Network)과 RNN(Recurrent Neural Network)의 발전으로 복잡한 데이터 패턴을 효과적으로 학습할 수 있는 가능성이 열림.
- **대량의 데이터 활용**: 대량의 음성 데이터와 텍스트 데이터가 축적되면서, 딥러닝 모델이 직접적으로 입력-출력 관계를 학습할 수 있는 환경 조성.

#### 3. 통합 모델의 필요성
- **단순화된 모델**: 기존의 ASR 시스템에서 여러 단계를 하나의 통합된 모델로 결합함으로써 시스템 복잡성을 줄이고 효율성을 높일 필요성이 대두됨.
- **실시간 인식**: 사용자의 요구에 따라 실시간으로 음성을 인식하고 처리할 수 있는 시스템의 필요성이 증가하면서, End-to-End 모델이 주목받기 시작.

#### 4. 자연어 처리의 발전
- **자연어 처리와의 통합**: 음성 인식과 자연어 처리 기술이 통합되면서, End-to-End ASR은 텍스트 생성 및 후처리와 같은 자연어 처리 작업도 효과적으로 수행할 수 있는 가능성을 제공.
- **학습의 일관성**: End-to-End 시스템은 입력에서 출력까지의 일관된 학습이 가능하여, 문맥 정보를 활용한 자연스러운 텍스트 생성이 가능해짐.

#### 5. 연구와 산업의 관심 증가
- **효율성**: 많은 연구자와 기업들이 End-to-End ASR의 효율성과 성능 향상 가능성에 관심을 가지게 됨.
- **경쟁력 확보**: 기업들이 다양한 산업 분야에서의 음성 인식 응용 프로그램 개발을 위해 End-to-End ASR을 채택함으로써, 경쟁력을 확보하고자 함.

### 결론
- End-to-End ASR은 전통적인 음성 인식 시스템의 복잡성을 해결하고, 딥러닝 기술의 발전과 자연어 처리의 통합에 따라 등장한 혁신적인 접근 방식입니다.
- 이 시스템은 음성 인식의 효율성을 높이고 사용자 경험을 개선하는 데 중요한 역할을 하고 있습니다.


# 3. End-to-End ASR의 종류(각 종류별 차이점)
## 1) An Overview of End-to-End Automatic Speech Recognition(2019)
- End-to-End 모델은 3가지 분류
  - CTC(Connectionist Temporal Classification) 기반 모델
  - RNN-Transducer 모델
  - Attention 기반 모델

- **LVCSR(Large Vocabulary Continuous Speech Recognition)**
  - 대규모 어휘 연속 음성 인식을 의미
  - 일상적인 대화나 연설처럼 흐름이 끊기지 않는 긴 문장 내에서 많은 단어를 인식해야 함
  - 소규모 어휘를 처리하는 시스템과는 달리 어휘 크기, 연속성, 발음의 변이와 같은 도전 과제를 해결 


- 구조
  - 입력 시퀀스 (X = {x1,···, xT}): 음성 입력.
  - 인코더 (Encoder): 입력된 음성 시퀀스를 특징 시퀀스로 변환.
  - 얼라이너 (Aligner): 특징 시퀀스와 언어를 정렬.
  - 디코더 (Decoder): 최종 결과로 단어나 철자 시퀀스를 출력.


## 2) End-to-End Speech Recognition: A Survey
- "End-to-End"라는 용어는 "과정의 모든 단계를 포함한다"는 의미를 지니며
- E2E 모델은 하나의 신경망 구조로 통합되어 있으며, 다양한 언어나 도메인에서 더 빠른 개발이 가능하고, 메모리 사용량과 전력 소비를 줄이는 데 유리
- 사전 학습된 모듈 없이 처음부터 끝까지 통합된 학습을 수행하며, 하나의 목표에 맞춰 단일 패스로 인식하는 시스템
- **E2E의 여러 해석**
  - 이론적으로는 음향 모델과 언어 모델의 구분이 없어지지만, 실제로는 외부 언어 모델을 사용해 성능을 향상시키는 경우가 많아 완전한 E2E 모델이라 하기 어려움
  - 외부의 지식을 사용하지 않고 순수하게 데이터를 통해 처음부터 학습하는 방식으로 해석 가능하나 대규모의 사전 학습된 모델이나 추가적인 학습 전략(예: 자가 지도 학습)을 사용하는 경우가 많음
  - 전체 단어 또는 문자 단위의 직접적인 어휘 모델링을 사용하지만, 일부 언어에서는 훈련 데이터가 부족해 이러한 모델링이 어렵다는 문제점 존재
  - E2E 시스템은 모든 구성 요소를 통합하여 단일 모델로 학습하는 점에서 유리하지만, 실질적으로는 외부 자원 및 중간 과정의 도움을 받는 경우가 많아 완벽한 E2E 모델이라고 보기 어려울 때가 많음

- 음성 인식은 음성 신호를 입력(X)으로 받고 이를 단어나 문자 등의 시퀀스(C)로 변환하는 시퀀스 분류 문제
- E2E 모델은 이 입력과 출력 간의 관계, 즉 **정렬 문제(alignment problem)** 를 처리하는 방식에 따라 분류
- **E2E 모델 분류의 기초**
  - **명시적 정렬(explicit alignment)**
    - 입력과 출력 사이의 정렬을 명시적인 **잠재 변수(latent variable)** 로 모델링
    - 입력 음성과 출력 텍스트 간의 정렬을 계산
    - CTC (Connectionist Temporal Classification), RNN-T (Recurrent Neural Network Transducer)
    - 기존 과거의 HMM, Hybrid 모델도 여기에 해당
  - **암시적 정렬(implicit alignment)**
    - 입력과 출력 간의 정렬을 명시적으로 정의하지 않고 모델이 스스로 정렬을 학습
    - Attention 메커니즘을 통해 입력의 특정 부분과 출력 라벨 간의 관계를 학습
    - 명시적 정렬 모델보다 우수한 성능을 보여주나 실시간 처리에서는 사용하기 어렵다는 한계 존재
      - 스트리밍 모델을 위해 스트리밍 가능한 암시적 정렬 모델 개발됨
      - **Neural Transducer (NT)** 와 **Monotonic Attention**
      - Neural Transducer나 Monotonic Attention 같은 모델들은 **명시적 정렬 모델과 암시적 정렬 모델의 장점을 결합한 하이브리드 형태**
    - Attention-based Encoder-Decoder (AED), 또는 LAS (Listen, Attend and Spell) 모델
- **E2E 모델은 크게 Encoder 모듈과 Decoder 모듈로 나뉨**
  - **Encoder 모듈**
    - 입력 음성 시퀀스를 더 높은 차원의 표현으로 변환
    - 입력 음성 신호는 주로 D-차원의 음향 프레임으로 표현(길이가 가변적인 입력 시퀀스)
  - **Decoder 모듈**
    - Encoder의 출력을 기반으로 출력 라벨을 생성
    - 출력 라벨(문자, 단어, 서브워드 등)의 확률 분포를 계산
    - 이전에 예측된 라벨들(출력의 이전 부분)에 의존하여 다음 라벨을 예측(조건부 확률 형태)

- E2E ASR에서는 발음 사전이 필요하지 않음
  - 대신, 모델이 음소 대신 문자(character) 또는 서브워드(subword) 단위로 음성 데이터를 직접 학습하여 단어를 예측
- 언어 모델이 내부적으로 통합되어 있으며, 음향 모델과 함께 학습
  - 하지만, 여전히 외부 언어 모델(예: shallow fusion)을 추가적으로 사용가능
- 외부 언어모델 통합 방법
  - 문맥적 오류를 보정하고, 더 자연스러운 텍스트 시퀀스를 생성하는 데 도움
  - **Shallow Fusion, Deep Fusion, Cold Fusion**
  - Shallow Fusion
    - 음성 신호에서 생성한 출력에 외부 언어 모델을 결합
    - 텍스트 시퀀스를 예측할 때, 외부 언어 모델에서 제공하는 확률 분포와 결합
    - 디코딩 과정에서 언어 모델의 확률을 가중치로 적용하여, 텍스트 시퀀스의 선택을 조정
  - Deep Fusion
    -  **은닉 상태(hidden states)** 와 **언어 모델의 상태**를 결합
    -  학습 과정에서 E2E 모델과 언어 모델이 동시에 작동
  - Cold Fusion
    - E2E 모델에 **사전 통합(pre-integration)** 하여 학습 과정에서부터 두 모델을 결합
    - 언어 모델의 출력을 E2E 모델에 보조 정보로 제공
    - 훈련 중에 언어 모델과 음향 모델을 동시에 학습하게 하여, 언어 모델의 영향을 더 자연스럽게 반영


- "End-to-End Models for Speech Recognition: History and Methodology" (Survey, 2019)
- "A Comprehensive Review of End-to-End Speech Recognition" (Survey, 2021)

- CTC 기반 ASR
  - Connectionist Temporal Classification(CTC)을 사용하여 음성 입력과 텍스트 출력 사이의 관계를 직접 학습
  - 음성 길이가 다를 때 발생하는 문제를 해결하며, 중간 단계를 생략할 수 있습
- Attention 기반 ASR (Seq2Seq)
  - Attention 메커니즘을 사용하여 음성의 중요한 부분을 선택하고, 입력과 출력 간의 관계를 더욱 정교하게 학습
  - 음성의 특정 부분에 집중하면서도 텍스트로 변환하는 능력이 뛰어
- Transformer 기반 ASR
  - 음성 데이터를 처리하기 위해 Self-Attention 메커니즘을 사용하는 최신 End-to-end ASR 모델
  - 실시간 음성 처리에 매우 적합하며, 대규모 음성 데이터에서 높은 성능을 발휘 

## 1. CTC (Connectionist Temporal Classification) - 2006
-"Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks" (CTC, 2006)

- https://zerojsh00.github.io/posts/Connectionist-Temporal-Classification/

### 특징
- 입력과 출력 시퀀스의 길이가 다르거나 정렬되지 않은 경우에도 학습 가능
- 음성 신호에서 음소 또는 문자 시퀀스를 직접 예측

### 장점
- 데이터 정렬 필요 없음
- 음소 단위로 학습할 수 있어 유연성 높음

### 단점
- 출력 단어의 순서를 보장하지 않음
- 모델이 장기 의존성 문제를 해결하는 데 한계

### 배경
- RNN과 CNN이 발전하면서 음성 인식의 성능을 향상시키기 위해 등장
- 음성 인식에서 시퀀스 예측 문제를 효과적으로 해결하기 위한 방법론으로 자리잡음

### 구조
- 입력: 음성 특징 벡터 (예: MFCC)
- 출력: 문자 시퀀스
- 출력 레이어에 Softmax를 적용하여 각 타임스텝에서의 확률 분포를 생성


## 2. RNN-T (Recurrent Neural Network Transducer) - 2012
- "Sequence Transduction with Recurrent Neural Networks" (RNN-T, 2012)
### 특징
- 입력 시퀀스의 처리와 출력 시퀀스의 생성을 동시에 수행
- 예측된 출력과 함께 입력을 처리하는 구조

### 장점
- 입력과 출력을 동기화할 수 있어 실시간 인식 가능
- 유연한 구조로 다양한 입력에 대응 가능

### 단점
- 훈련이 상대적으로 복잡하고 시간이 많이 소요됨
- 긴 시퀀스를 처리할 때 기울기 소실 문제에 직면할 수 있음

### 배경
- CTC의 한계를 극복하기 위해 발전
- 시퀀스 간의 동기화를 통해 실시간 음성 인식의 필요성을 충족

### 구조
- 입력: 음성 특징 벡터
- RNN을 사용하여 현재 입력과 이전 출력 정보를 결합하여 다음 출력을 생성


## 3. LAS (Listen, Attend and Spell) - 2015
- "Listen, Attend and Spell" (LAS, 2015)
### 특징
- Attention 메커니즘을 활용하여 입력 시퀀스에서 필요한 정보에 집중
- 주어진 정보를 바탕으로 음소 또는 문자를 생성하는 구조

### 장점
- 긴 시퀀스에서의 문맥 정보를 효과적으로 반영
- 입력의 중요도를 다르게 평가할 수 있어 성능 향상 가능

### 단점
- 복잡한 구조로 인해 계산 비용이 높음
- 학습 데이터가 부족할 경우 성능 저하 가능

### 배경
- Sequence-to-Sequence 모델에서 영감을 받아 음성 인식에 적용
- Attention 기법이 발전하면서 인식 성능이 향상됨

### 구조
- Encoder-Decoder 구조로 구성
- Encoder: 음성 신호를 인코딩
- Decoder: 인코딩된 정보를 기반으로 문자를 생성


## 4. Hybrid CTC/LAS - 2017
- "Hybrid CTC/Attention Architecture for End-to-End Speech Recognition" (2017)
### 특징
- CTC와 LAS의 장점을 결합하여 음성 인식 성능을 극대화
- 초기 예측을 CTC로 수행하고, 세밀한 예측을 LAS로 수행

### 장점
- 두 모델의 장점을 활용하여 높은 정확도 달성
- 데이터의 다양한 특성에 적응 가능

### 단점
- 구조가 복잡하여 구현 및 훈련이 어려움
- 두 모델 간의 하이퍼파라미터 조정이 필요할 수 있음

### 배경
- CTC와 LAS 각각의 한계를 인식하고 이를 보완하기 위해 개발
- 복잡한 음성 인식 문제를 해결하기 위한 진화된 접근법

### 구조
- CTC와 LAS의 두 가지 구성 요소를 포함
- CTC를 통해 초기 예측을 수행하고, LAS로 더욱 세밀한 결과를 생성



# ASR
- automatic speech recognition (자동음성인식)
- 음성 신호를 입력으로 받아들여 텍스트로 변환하는 기술
- 구성단계  
  - 푸리에 변환(Fourier transform)을 사용하여 음성 신호를 주파수 영역으로 변환한 후, 주파수 스펙트럼에서 특징적인 정보를 추출
  - 음성 신호의 특징 벡터를 입력으로 받아들여 음소(phoneme) 단위로 분할(음소 인식 모델을 사용하여 음성 신호를 음소 단위로 변환)
  - 음소 시퀀스를 텍스트로 변환(언어 모델과 결합하여 음성 신호를 텍스트로 해석)
 
# 명시적 정렬 vs 암묵적 정렬
음성 인식에서 **Sequence Alignment** 방식은 크게 **명시적 정렬(Explicit Alignment)**과 **암묵적 정렬(Implicit Alignment)**으로 나눌 수 있습니다. 각 모델이 이 정렬 방식을 어떻게 처리하는지에 따라 구분될 수 있습니다.

## 1. 명시적 정렬(Explicit Alignment)

명시적 정렬은 **입력 시퀀스**와 **출력 시퀀스** 간의 시간적 순서가 명확하게 정해진 경우를 의미합니다. 즉, 입력과 출력이 정해진 순서에 따라 대응됩니다. **CTC**와 **RNN-T** 모델이 여기에 속합니다.

### CTC (Connectionist Temporal Classification)
- **정렬 방식**: CTC는 입력과 출력의 순차적 관계를 명시적으로 유지합니다. 각 입력 프레임은 출력에 대응되거나, 대응되지 않는 경우에는 **블랭크(blank)** 토큰이 사용됩니다.
- **특징**:
  - 입력 시퀀스의 프레임이 출력에 대응하는 경로를 찾기 위해 여러 가능한 경로를 계산하며, 최종적으로 최적 경로를 선택합니다.
  - 입력과 출력 간의 시간적 순서가 유지되므로 **명시적 정렬** 방식으로 간주됩니다.
  
### RNN-T (Recurrent Neural Network Transducer)
- **정렬 방식**: RNN-T는 입력 시퀀스가 실시간으로 처리되면서 출력이 순차적으로 생성됩니다. 이전 출력 상태와 현재 입력 프레임을 기반으로 다음 출력을 예측합니다.
- **특징**:
  - 실시간 음성 인식에 적합하며, 입력과 출력 간의 정렬이 명확하게 유지됩니다.
  - 출력이 이전 출력 상태에 의존하므로 **명시적 정렬** 방식으로 분류할 수 있습니다.

## 2. 암묵적 정렬(Implicit Alignment)

암묵적 정렬은 **입력과 출력 사이의 정렬 정보가 명시적으로 주어지지 않고**, 모델이 **자동으로 정렬을 학습**하는 방식입니다. **LAS**와 **Transformer** 모델이 여기에 속합니다.

### LAS (Listen, Attend and Spell)
- **정렬 방식**: LAS는 **Attention 메커니즘**을 사용하여 입력의 특정 부분에 집중하고, 해당 입력에 대응하는 출력을 암묵적으로 생성합니다.
- **특징**:
  - 입력과 출력 간의 정렬 정보가 명확히 제공되지 않고, 모델이 학습을 통해 정렬을 찾아내기 때문에 **암묵적 정렬** 방식입니다.
  - **Encoder-Decoder 구조**를 사용하여 입력을 벡터로 변환한 후, Decoder가 출력을 생성합니다.

### Transformer
- **정렬 방식**: **Self-Attention 메커니즘**을 통해 입력 시퀀스의 모든 요소가 상호작용하며, 출력과의 관계를 암묵적으로 학습합니다.
- **특징**:
  - Transformer는 **병렬 처리**가 가능하고, 각 입력이 모든 출력과 상호작용하는 구조를 가지므로 **암묵적 정렬** 방식으로 동작합니다.

---

## 요약

- **명시적 정렬**: CTC, RNN-T
  - 입력과 출력의 시간적 순서가 명확하게 정렬됨.
  - 실시간 처리와 순차적 출력을 유지함.

- **암묵적 정렬**: LAS, Transformer
  - 정렬 정보가 주어지지 않고 모델이 자동으로 정렬을 학습함.
  - Attention 메커니즘을 사용하여 입력과 출력 사이의 관계를 학습.

