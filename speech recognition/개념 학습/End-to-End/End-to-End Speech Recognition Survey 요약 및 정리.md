# 논문 정리 : End-to-End Speech Recognition: A Survey (T-ASLP 2024)

## Abstract
- **딥러닝이 도입된 ASR** 모델이 도입되지 않은 모델과 비교해 **단어 오류율(WER)이 50%이상 감소**
  - **WER** : 음성인식 시스템이 인식한 텍스트가 정답과 얼마나 다른지를 측정하는 평가 지표
    - WER = (I + D + S) / N
    - 추가, 삭제, 대체된 단어 수를 전체 정답 단어 수로 나누어 계산
      - **I (Insertions)**: 추가된 단어 수 (인식 결과에만 존재)
      - **D (Deletions)**: 빠뜨린 단어 수 (정답에 있었는데 인식 결과엔 없음)
      - **S (Substitutions)**: 잘못 인식된 단어 수 (ex. "love" → "like")
      - **N**: 정답 문장의 총 단어 수
    - 낮을수록 좋은 값
  - **CER** : 음성 인식의 오류율을 **문자 단위**로 계산하는 지표
    - 한국어, 중국어 등 공백이 없는 언어에서도 유용
    - WER에 비해 좀 더 세밀한 오류 파악 가능
- 현재는 **End-to-End(E2E)모델이 음성인식의 주 방식**




## 1. Introduction
### 전통적인 ASR 시스템 구조
- 전통적인 통계적 ASR 아키텍처는 네 가지 주요 요소로 구성
  - **음향 특징 추출 (Feature Extraction)** : 음성 오디오 신호로부터
  - **음향 모델링 (Acoustic Modeling)** : 주로 HMM, GMM
    - 은닉 마르코프 모델(HMM) : 화자의 발화 속도 변화를 반영
  - **언어 모델링 (Language Modeling)** : **카운트 기반 방식(예: N-gram)**
  - **탐색/디코딩 (Search)** : 베이즈 결정 규칙(bayes decision rule)에 기반한 탐색 과정


### 딥러닝 도입 시작
- 음향 모델에서 **가우시안 혼합 모델(GMM)** 대신 신경망 모델 사용
- 언어 모델도 **딥러닝 기반 언어 모델로 대체**
- 하지만 여전히 파이프라인 구조 자체는 유지


### 여전히 많은 별도의 구성 요소와 전문 지식에 의존
- 음성 신호 전처리
- 녹음 환경 변화에 대한 견고성 확보 기법
- 음소 목록과 발음 사전
- 음소 클러스터링
- OOV(사전에 없는 단어) 처리
- 다양한 적응/정규화 방법
- 복잡한 학습 스케줄 (예: 시퀀스 기반 판별 학습 포함) 등


### 시퀀스-투-시퀀스(sequence-to-sequence)
- 음성 전처리 + 특징 추출 + 음향 모델링 → 통합
- 점점 **sequence-to-sequence** 형태로 발전
- 기존 음성처리 특화 설계 대신 **범용 딥러닝 모델로 처리**


### End-to-End 정의
- **Cambridge Dictionary**에 따르면, **“end-to-end”**라는 형용사는 **“하나의 과정의 모든 단계를 포함하는 것”** 으로 정의
  - 하나의 과정으로 끝까지 처리되는 음성 인식 모델
- 단순히 구조만 하나가 아닌 모델 구성, 학습 방식, 데이터 활용 등 모든 측면에서 통합된 구조 의미
  - **a) Joint Modeling (통합 모델링)** : 기존의 모듈화된 모델을 하나의 신경망으로 통합
  - **b) Single-Pass Search (단일 탐색 과정)** : 전체 정보를 기반으로 한 번에 결과 예측
    - Search는 모델이 **어떤 출력을 내릴지 탐색**하는 과정 == **디코딩(decoding)**
    - 전통모델 처럼 여러 모듈 따로 돌아가게 하지 않고, **하나의 디코더 블록이 전체 역할을 맡음**
  - **c) Joint Training (통합 학습)** : 하나의 목적함수로 전체 모델 학습 (ex. WER 최소화)
  - **d) Training Data (단일 종류의 학습 데이터)** : 전사된 음성 데이터만으로 학습
    - **전사(transcription)** : 음성 데이터를 듣고, 그 내용을 문자로 적은 것
    - **전사된 음성 데이터** : 음성과 해당 텍스트가 짝을 이루는 데이터
  - **e) Training from Scratch (처음부터 학습)** : 사전 정렬이나 초기 모델 없이 처음부터 학습
    - 과거 음향 모델은 단어의 음소가 몇 초 구간에 위치하는지 미리 알고 있어야 했음 -> **Forced Alignment**로 구함
    - 그런 정렬 정보 없이도 **모델이 직접 정렬도 학습**
  - **f) Secondary Knowledge Sources (2차 지식원 회피)** : 발음 사전, 음소 집합 같은 외부 지식 사용 안함
    - 모델이 직접 문자나 음소 단위의 분포를 학습하고 정렬까지 담당 
    - **일관성 문제, 비용, 오류 가능성** 등을 줄여줌
  - **g) Vocabulary Modeling (어휘 모델링 방식)** : 발음 사전을 사용하지 않으며, 문자/서브워드 기반 사용
    - 전체 단어 기반 모델은 **엄청난 양의 전사 데이터를 필요로 하므로 현실적으로 어려움 존재**
  - **h) Generic vs. Informed Modeling (범용 vs. 특화 모델링)** : 도메인 지식 없이도 데이터로부터 학습
    - **예를 들어, ASR의 순차적(monotonic) 정렬 특성을 attention 기반 모델에서는 학습을 통해, HMM에서는 시스템 구조로 직접 구현**
- 특히 전통적인 ASR의 발음사전, 정렬 정보, 모듈별 최적화 과정을 피하고, 오직 데이터를 기반으로 전체 시스템을 하나의 목표 아래 학습하는 것이 핵심


### ASR 시스템 개발의 목표
- 주 목표
  - 단어 오류율(WER: Word Error Rate) 최소화
- 부차적 목표
  - 디코더의 시간/메모리 복잡도 줄이기
  - 모델링을 쉽게 하고, 다양한 도메인/언어에 적용할 수 있는 범용성 확보 


### End-to-End 이점
- **구조적 단순화와 범용성 (Simplicity & Generality)**
  - 하나의 신경망 구조로 전체 시스템 구성
  - **디코딩**이 단일 모델 안에서 처리되어 **복잡한 모델 통합 불필요**
  - **새로운 언어나 도메인에 대해 빠른 개발 주기 달성**
- **효율성과 경량화 (Efficiency & Lightweight Design)**
  - 메모리, 연산 자원이 줄어들어 임베디드 환경에서의 ASR 적용에도 유리
- **학습 최적화와 안정성 (Learning Optimization & Robustness)**
  - 통합 학습(joint training)으로 잘못된 국소 최적화 지점 피할 수 있음
- **외부 리소스 없이 데이터 기반 학습 (Data-Driven & Resource-Free)**
  - 발음사전과 같은 2차 지식원 불필요
  - 2차 지식원이 잘못된 경우 생기는 오류 방지
  - 충분한 작업-특화 데이터(task-specific data)만 있으면 학습 가능


### 논문의 구성
- 제2장: E2E 음성 인식의 역사적 발전 과정과, **입출력 정렬 방식(alignment)** 관점의 분류
- 제3장: 기본 E2E 모델에 대한 개선 (구조, 문맥, loss 등)
- 제4장: E2E 모델 학습 방법
- 제5장: 다양한 E2E 디코딩 전략
- 제6장: E2E ASR에서의 **언어 모델(LM)** 의 역할과 통합
- 제7장: 최신 E2E 모델과 전통적인 HMM 기반 ASR의 비교
- 제8장: E2E 및 전통 ASR 접근법에 대한 성능 비교
- 제9장: E2E ASR의 실제 적용 사례
- 제10장: E2E ASR 향후 연구 방향
- 제11장: 결론



## 2. End-to-End 음성 인식 모델의 분류 체계

### 논문 전반 표기법
- **X : 입력 음성 발화**
  - 길이 **T′** 의 **D** 차원 음향 프레임 시퀀스 (예: log-mel features)로 표현
    - D는 각 프레임에서 추출되는 음향 특성 벡터의 차원 수 - **입력 음성의 “한 프레임당 정보량”을 몇 차원으로 표현할 것인지 결정하는 값**
    - 일반적으로는 40~80 차원이 자주 사용
  - X = $(x₁, …, x_{T′})$,   where   $x_t$ ∈ $ℝᴰ$
- **C : 입력 발화에 대응하는 단어 또는 문자 시퀀스**
  - 길이 $L$ 의 라벨(label) 시퀀스로 분해할 수 있으며, 라벨 단위는 문자, 단어, subword (예: BPE, word-piece 등)로 구성
    - **문자 단위는 너무 길고, 단어 단위는 희귀어 문제(OOV)가 심한데, Subword는 둘의 장점을 잘 절충해서 처리할 수 있기 때문**
    - **BPE (Byte Pair Encoding)**
      - 빈도 높은 문자 쌍부터 반복적으로 병합해서 단위를 만들어냄
      - 병합 규칙이 단순함 (문자 쌍 병합)
      - 예측 단위는 subword → 예: “reading” → “read”, “ing” 
    - **WordPiece**
      - Google에서 만든 방식 (BERT, T5 등에서 사용)
      - 기본 원리는 BPE와 비슷하지만, 병합 시 **likelihood**를 기반으로 선택함 (확률 모델 기반)
        - **“주어진 관측값이 어떤 확률 모델 하에서 얼마나 일어날 법한가“를 나타내는 수치**
        - **주어진 데이터가 특정 모델 하에서 발생할 가능성의 척도**
      - 더 언어학적 의미 단위에 가까운 subword를 생성 가능
  - C = $(c₁, …, c_L)$,   where   $c_j$ ∈ $𝒞$
  - ex) 문자 단위: C = (h, e, l, l, o), → L = 5
- **H(X) : 인코더 모듈**
  - 모든 **E2E 모델은 공통적으로 인코더 구조**를 포함
  - 인코더는 입력 음향 시퀀스 **$X$를 고차원 표현 $H(X)$**로 변환
  - X의 원시 특징을 요약하고, 모델이 C를 예측하는 데 필요한 정보를 담는것
  - H(X) = $(h_1, ..., h_T)$ (길이 T) 보통 T ≤ T′
    - 인코더는 **다운샘플링(stride, pooling)** 이나 **요약(압축)** 을 통해 입력보다 짧은 길이의 표현 T를 만들어내는 경우가 많음
    - 중요한 정보만 남기고 요약하기 위해
    - 입력이 너무 길면 연산량이 많아져 메모리, 학습시간 증가
  - 인코더는 RNN, CNN, Transformer 등 다양한 신경망 구조로 구현될 수 있으며, 논문에서는 이러한 구조의 세부 구현에는 구애받지 않고 일반적으로 설명
- **𝓣 : 훈련 데이터 집합**
  - 훈련 데이터는 **(음성, 전사)** 쌍으로 구성된 N개의 학습 샘플
  - $`𝒯 = { (Xᵢ, Cᵢ) ∣ i = 1, …, N }`$
 
- 모델의 학습 목표는, **입력 음성 $X$가 주어졌을 때 출력 시퀀스 $C$가 나올 조건부 확률**을 예측하는 것
- $$P(C|X) = P(C|H(X))$$
  - 입력 음성 X가 주어졌을 때, 그 음성을 인코더를 통해 벡터 표현 H(X)로 바꿔서, 최종적으로 그에 해당하는 텍스트 C를 생성할 확률을 구하겠다
  - 직접 X에서 바로 예측하는 게 아니라, 먼저 인코더를 통해 고차원 표현 H(X)를 만들고, 그 표현을 바탕으로 C를 예측하는 구조로 이루어져 있다는 걸 의미



### 정렬(Alignment)의 필요성
- End-to-End ASR 모델의 핵심 목표는 **입력 음성 X로부터 출력 텍스트 C를 직접 예측하는 확률 분포 P(C|X)를 학습하는 것**
  - 음성(X)은 길고 연속적인 시퀀스이고, 텍스트(C)는 상대적으로 짧은 **불연속적인 단위(문자, 단어 등)** 로 이루어져 있음
  - 이 둘을 연결하기 위해 **어떤 프레임이 어떤 문자/단어에 대응되는지 정렬하는 과정(Alignment)** 이 필요


### 정렬(Alignment)의 발전
- 전통적인 ASR에서는 프레임 단위 정렬을 **HMM(Hidden Markov Model)** 로 모델링하고, **GMM(Gaussian Mixture Model)** 또는 **신경망**을 통해 음향 프레임의 출력 분포를 학습
  - 프레임 단위 정렬 : 음성 시퀀스의 각 시간 단위(프레임)가 어떤 음소 또는 문자에 대응되는지를 명확히 지정하는 것
    - 예를 들어 “hello”라는 단어를 사람이 말하면, 약 1초 정도의 음성이 나온다고 가정
    - 이 1초를 10ms 단위로 쪼개면 총 100개의 프레임 생성
    - → **이 100개의 프레임 중 1~10은 h, 11~20은 e, …** 처럼 **각 프레임이 어떤 라벨에 해당되는지를 정렬**
  - 음향 프레임의 출력 분포 학습 : 이 프레임이 어떤 소리를 의미할 확률이 높은지 학습
    - 프레임 xₜ가 있을 때 → 이게 음소 /h/ 일 확률, /l/일 확률 등
    - 각 프레임을 입력받아서, **그 프레임이 어떤 소리에 해당될 확률 분포를 출력하는 모델** 
- 신경망 기반 음향 모델 학습을 위해, GMM-HMM을 기반으로 한 **강제 정렬(Forced Alignment) 기법** 이 널리 사용됨
- 최근에는 초기 정렬 없이 **전체 시퀀스를 직접 학습하는 방법(Sequence-level training)** 도 사용 가능


### E2E의 정렬
- 일반적으로 **문자 또는 (sub-)word 단위**로 라벨 정의
  - 전통적인 ASR 시스템에서의 **음소 단위 정렬**은 자연스러운데, **문자 단위 정렬은 직관적이지 않음**
  - **음소**는 발음과 직접 연결되기 때문에 **프레임과의 정렬이 비교적 자연스러움**
  - 하지만 E2E는 문자나 서브워드 단위로 정렬하기에 어떤 프레임이 ‘a’, ‘b’에 대응되는지 모호
- **명시적(Explicit) 정렬**
  - 정렬을 **잠재 변수(latent variable)** 로 명시하고, 학습 시 이를 주변화(marginalization) 처리
    - 잠재 변수 : **직접 관측할 수는 없지만, 모델 내부에서 반드시 추정해야 하는 변수**
    - 주변화 : **가능한 모든 정렬 경우의 수에 대해 확률을 더해서 전체 확률을 계산**
  - **CTC (Connectionist Temporal Classification)**
  - **RNN-T (Recurrent Neural Network Transducer)**
  - **RNA (Recurrent Neural Aligner)**
  - **HAT (Hybrid Auto-Regressive Transducer)**
  - 후속 모델일수록 **독립 가정(independence assumption)** 을 줄이고, **모델 성능이나 표현력이 향상**
    - 독립 가정 : “각 시간 프레임의 예측은 서로 독립적이다”라고 가정
      - “프레임 1에서 h 예측”이 “프레임 2에서 e 예측”에 아무 영향도 안 준다고 가정
    - 독립 가정이 줄어들면 좋은점
      - 자연어는 연속성(context) 이 중요한데, 독립 가정이 있으면 문맥을 반영하지 못함
      - **프레임 간 의존성(문맥)을 모델이 더 잘 학습할 수 있어 → 성능 향상**
- **암시적(Implicit) 정렬**
  - **정렬을 명시하지 않고, attention 메커니즘**을 통해 동적으로 정렬 관계를 학습
  - 전체 음향 시퀀스와 개별 라벨 간의 매핑 관계 학습
  - **Attention-based Encoder-Decoder (LAS 등)**
- **중간적인 접근 방식**
  - 정렬을 명시적으로 구성하면서도, attention 매커니즘을 활용해 유연성 확보
    - **attention 메커니즘을 활용해 주변 음향 정보를 참고하여 예측을 정제(refine)**
  - **Neural Transducer**
  - 모노토닉 정렬(monotonic alignment) 기반
    - **monotonic = 일방향으로만 진행하는** -> 시간 순서를 앞으로만 진행하는 정렬 방식
      - 자연스러운 말하기는 **시간적으로 순차적**이기 때문에, 모델도 정렬을 **시간 순서대로만 하도록 제한하면 더 현실적인 학습**
    - MoChA (Monotonic Chunkwise Attention)
    - MILK (Monotonic Infinite Lookback) 


### A. 명시적 정렬 기반 E2E 접근법 (Explicit Alignment E2E Approaches)
- 입력 음성의 인코더 출력 시퀀스와 정답 텍스트 사이의 정렬 관계를 **잠재 변수(latent variable)** 로 정의
  - **T 길이의 인코더 출력 H(X)** 와 **L 길이의 출력 시퀀스 C** 사이의 정렬 정보를 **명시적인 정렬 변수 A**로 표현
  - CTC, RNN-T, RNA 각각의 모델별 정렬 A를 정의하는 방식이 다름
  - 해당 차이는 모델의 내부 **조건부 독립 가정(conditional independence assumption)**, 그리고 학습 및 디코딩 방식에도 영향
- 공통 구조 : ⟨b⟩ - blank 기호 도입
  - 출력 기호 집합 확장 : $C_b = C \cup \{⟨b⟩\}$
  - 문자와 문자 사이의 공백 혹은 타이밍 조절 역할 수행
  - 각 모델마다 그 의미나 사용 방식은 조금씩 차이 존재
- 주변화(Marginalization)
  - 모델은 하나의 정렬이 아닌, **여러 가능한 정렬 시퀀스(A)** 를 고려
  - 특정 훈련 예제 (X, C)에 대해, 유효한 정렬 집합 ${A}_{(T,C)}$ 를 정의
  - 이를 기반으로 전체 조건부 확률 P(C|X) 주변화
  - $`P(C|X) = P(C|H(X)) = \sum_{A} P(C | A, H(X)) \cdot P(A | H(X)) <br>
= \sum_{A \in \mathcal{A}_{(T = |H(X)|, C)}} P(A | H(X)) \tag{1}`$
    - $`\sum_{A} P(C | A, H(X)) \cdot P(A | H(X))`$ -> 가능한 모든 정렬 시퀀스 A를 다 고려해서 평균을 냄(sum A)
      - P(A | H(X)) -> 인코더 출력 H(X)가 주어졌을 때 **정렬 A가 일어날 확률**
      - P(C | A, H(X)) -> 정렬 A가 주어졌을 때, 실제로 그 정렬이 **정확히 C를 만들었는지 여부**
  - 여기서 조건 : $`P(C | A, H(X)) = 1 \quad \text{if} \quad A \in \mathcal{A}_{(T, C)}`$
    - 즉, **특정 정렬 A가 라벨 시퀀스 C를 정확히 생성할 수 있을 경우에만 1의 확률을 부여**
    - 하나의 정렬 A에서 라벨 시퀀스 C로의 매핑은 **유일하게 정의**


#### 1) CTC (Connectionist Temporal Classification)
- 입력된 **음성 시퀀스 X를 출력 텍스트 시퀀스 C로 직접 매핑**하는 모델
- **Blank 라벨 ⟨b⟩**
  - $`C_b = C \cup \{\langle b \rangle\}`$
  - 인코더 출력 H(X)와 라벨 시퀀스 C 사이의 정렬을 명시적으로 모델링하기 위해
  - 반복되는 라벨을 구분하기 위해 <b> 추가
    - [e, ⟨b⟩, e]처럼 넣어줘야 “두 개의 e”로 인식 가능 
- **정렬 A의 정의**
  - $`A \in C_b^*`$ - 전체 길이는 T (인코더 출력 길이)와 동일
    - 인코더 출력 H(X)는 길이 T의 시퀀스 == 정렬 A도 길이 T인 라벨 시퀀스
    - 인코더 출력이 T 프레임이니까, 각 프레임마다 어떤 라벨이 대응될지 정렬 시퀀스를 만들 수 있음
  - 가능한 정렬 집합 : $`\mathcal{A}_{\text{CTC}}(X, C) = \{ A = (a_1, a_2, …, a_T) \mid a_t \in C_b \}`$
- **유효한 정렬**
  - **연속된 동일 라벨을 먼저 병합(collapse)한 후, 모든 ⟨b⟩ 기호 제거했을때, A가 정확히 C가 되는 경우만 유효**
  - 예를 들어, T = 10, C = (s, e, e) 라고 할 때, $`A = (s, \langle b \rangle, \langle b \rangle, e, e, \langle b \rangle, e, e, \langle b \rangle, \langle b \rangle)`$
  <img width="603" alt="image" src="https://github.com/user-attachments/assets/440025e6-6530-4535-9c77-3a44baa330e7" />  

- **CTC 확률 계산**
  - 가능한 모든 정렬에 대해 **확률을 합산** - **주변화(marginalization)**
  - $`P_{\text{CTC}}(C | X) = \sum_{A \in \mathcal{A}_{\text{CTC}}(X, C)} P(A | H(X))`$
    - $`= \sum_{A \in \mathcal{A}{\text{CTC}}(X, C)} \prod{t=1}^{T} P(a_t \mid a_{t-1}, …, a_1, H(X))`$
    - $`= \sum_{A \in \mathcal{A}{\text{CTC}}(X, C)} \prod{t=1}^{T} P(a_t \mid h_t) \tag{2}`$
- **강한 독립 가정**
  - 각 시간 t의 출력은 다른 시간 스텝과 독립적이라고 가정
    - 오직 해당 시점의 인코더 출력 $h_t$ 만을 기반으로 결정
  - 이 가정 덕분에 학습과 추론이 단순해지지만, 긴 문맥 정보나 복잡한 의존 관계를 반영하는 데는 한계 존재

- **모델 구조**  
  <img width="586" alt="image" src="https://github.com/user-attachments/assets/0776e2ca-66d2-4474-b64d-a9d55195e3f9" />   

  - 각 시점 t에서 $P(a_t | X)$ 를 모델링하는 신경망 구조로 구성
  - **인코더(Encoder)**
    - 입력 음성 X → 인코더 → 고차원 시퀀스 표현 $`H(X) = (h_1, …, h_T)`$
      - $`h_t \in \mathbb{R}^k`$
    - **CNN, RNN (예: LSTM)**
  - **출력(Softmax layer)**
    - 각 시간 t에서 : $`P(a_t = c \mid X) = P(a_t = c \mid H(X))`$
    - 각 프레임에서 ⟨b⟩ 포함 전체 라벨 집합 $`C_b`$ 에 대한 확률 분포 출력
    - 매 스텝마다 ⟨b⟩ 혹은 실제 라벨 중 하나를 선택하여 출력


#### 2) RNN-T
- 기존 CTC가 갖는 **강한 조건부 독립 가정(각 시간 스텝 출력이 독립)을 완화**하여 개선한 모델
- **Blank 라벨 ⟨b⟩**
  - CTC : 반복 라벨을 구분하기 위한 중간 기호
  - 각 인코더 프레임 ʰₜ 에 대해, 모델은 0개 이상의 라벨을 출력하고, 마지막에 blank로 종료
  - 즉, 반복되는 라벨을 위한 별도 조치가 필요 없음
- **유효한 정렬**
  - T + L 길이의 시퀀스 A로 정의되며, blank를 제거했을 때 C
    - 모든 ⟨b⟩ 기호를 제거하면 정확히 C가 되는 시퀀스만 유효한 정렬로 간주 
  - 𝒜ᴿᴺᴺᵀ(X, C) = { A = (a₁, a₂, ..., aₜ₊ₗ) }
  - 출력 위치 τ에서, $$i_τ$$ 는 정렬 시퀀스 A의 처음부터 τ - 1까지 등장한 non-blank 라벨의 수를 나타냄
    - 이때, 그 구간에 포함된 blank의 수는 (τ - i_τ - 1)
  - 예를 들어 T = 7, C = (s, e, e)일 때 A = (⟨b⟩, s, ⟨b⟩, ⟨b⟩, ⟨b⟩, e, e, ⟨b⟩, ⟨b⟩, ⟨b⟩)
    - 이는 유효한 정렬에 포함  

   
  <img width="586" alt="image" src="https://github.com/user-attachments/assets/ed774ebb-60e9-4164-b86f-1da32f4dcd25" />  

- **RNN-T 확률 계산**  
  - 𝑃ᴿᴺᴺᵀ(C ∣ X) = ∑ₐ ∈ 𝒜ᴿᴺᴺᵀ(X,C) 𝑃(𝒜 ∣ H(X))
  - = ∑ₐ ∈ 𝒜ᴿᴺᴺᵀ(X,C) ∏ₜ₌₁ᵀ₊ᴸ 𝑃(aₜ ∣ aₜ₋₁, …, a₁, H(X))
  - = ∑ₐ ∈ 𝒜ᴿᴺᴺᵀ(X,C) ∏ₜ₌₁ᵀ₊ᴸ 𝑃(aₜ ∣ cᵢₜ, …, c₀, ʰₜ₋ᵢₜ)
  - = ∑ₐ ∈ 𝒜ᴿᴺᴺᵀ(X,C) ∏ₜ₌₁ᵀ₊ᴸ 𝑃(aₜ ∣ 𝐩ᵢₜ, ʰₜ₋ᵢₜ)
    - 여기서 aₜ : 정렬된 시퀀스 A의 τ번째 심볼 (blank 포함 가능)
      - 이전까지 예측된 non-blank 라벨 시퀀스에 조건부 종속
      - 하지만 정렬이 이루어진 프레임의 위치 (즉, 언제 출력되었는지)는 고려하지 않음
  	- 𝐩ᵢₜ : prediction network의 출력 (non-blank 라벨들에 기반)
    - ʰₜ₋ᵢₜ: 인코더에서 나온 acoustic context vector
    - P = (𝐩₁, 𝐩₂, …, 𝐩ᴸ) 은 prediction network의 출력 시퀀스
      - 이전까지 예측된 non-blank 라벨들의 시퀀스를 요약한 벡터들
  - RNN-T는 CTC보다 약한 독립 가정 사용

- **모델 구조**  
  <img width="580" alt="image" src="https://github.com/user-attachments/assets/9951c35a-c928-4cde-bccf-b2adc764af85" />  

  - 이 prediction network는 또 하나의 신경망으로 구현
    - 각 출력은 다음과 같이 정의된다:  𝐩ⱼ = NN(c₀, c₁, …, cⱼ₋₁)
    - 여기서 c₀ 은 문장의 시작을 나타내는 특수 라벨 ⟨sos⟩


- **RNN-T의 일반화**
  - [43]의 연구에서는 RNN-T 모델을 더 일반화된 형태로 확장
  - 정렬 가능한 프레임 단위 시퀀스를 임의의 그래프 구조로 표현 가능
  - 조건부 확률 계산을 위한 forward-backward 알고리즘을 일반화하여 적용
  - 이 구조에서는 CTC처럼 각 프레임마다 하나의 출력만 발생시키는 방식도 포함 가능
  - **Prediction Network를 통한 문맥 반영은 그대로 유지**
 

#### 3) RNA
- **RNA는 RNN-T의 일반화 모델**로, RNN-T보다 간단한 정렬 구조와 계산 효율성을 제공
  - 조건부 독립에 대한 가정 중 하나를 제거
- **Blank 라벨 ⟨b⟩**
  - CTC 모델과 동일한 의미 - 반복 라벨을 구분하기 위한 중간 기호
  - 각 프레임에서 모델은 다음 프레임으로 진행하기 전에 blank 또는 non-blank 레이블 중 하나만 출력
    - 즉, 정렬 시퀀스 A는 항상 **입력 길이 T와 동일한 길이**
  - But, CTC와 달리 RNN-T 처럼 모델은 각 non-blank 레이블의 단일 인스턴스만 출력
    - RNN-T에서 blank 심볼을 출력하면 모델이 다음 프레임으로 진행
    - RNA에서 모델은 단일 blank 또는 non-blank 레이블을 출력한 후 다음 프레임으로 진행
    - 각 프레임에서 단일 non-blank 레이블을 출력하도록 모델을 제한
      - 각 프레임에서 모델 확장 수를 제한하여 계산 효율성이 향상되고 디코딩 프로세스가 단순화 됨 
- **유효한 정렬**
  - 𝒜ᴿᴺᴬ(X, C) = (a₁, ⋯, aₜ)
    - 길이 T
  - T - L개의 blank 심볼
  - A에서 blank를 모두 제거했을 때 정확히 라벨 시퀀스 C와 일치해야 함
  - 예를 들어, T = 8이고 C = (s, e, e)인 경우
    - 정렬 A 예시: 𝒜 = (⟨b⟩, s, ⟨b⟩, e, ⟨b⟩, ⟨b⟩, e, ⟨b⟩) ∈ 𝒜ᴿᴺᴬ(X, C)  
  ![image](https://github.com/user-attachments/assets/ed1c8f2d-2663-4f36-9403-3fb65cba8aee)  


- **RNA 확률 계산**
  - 𝑃ᴿᴺᴬ(C | X) = ∑ₐ∈𝒜ᴿᴺᴬ(X, C) 𝑃(A | ℋ(X))  
    - = ∑ₐ∈𝒜ᴿᴺᴬ(X, C) ∏ₜ₌₁ᵀ 𝑃(aₜ | aₜ₋₁, …, a₁, ℋ(X))  
    - = ∑ₐ∈𝒜ᴿᴺᴬ(X, C) ∏ₜ₌₁ᵀ 𝑃(aₜ | qₜ₋₁, hₜ)
  - qₜ₋₁ = NN(aₜ₋₁, …, a₁) 는 전체 부분 정렬을 요약하는 신경망의 출력

- **모델 구조**  
  ![image](https://github.com/user-attachments/assets/5133da2a-79f1-443b-ad74-e3abc4e20353)  

  - non-blank 레이블의 시퀀스(RNN-T에서와 같이)와 이러한 레이블이 방출되는 특정 프레임 모두에 대해 조건화
    - 로그-우도(및 해당 기울기)의 정확한 계산은 해결하기 어려움
    - 단일 레이블만 출력할 수 있다는 제약 조건을 활용하면서 **가장 가능성 높은 정렬 경로만 사용**하는 근사 학습 방식 사용


### B. 암시적 정렬 기반 E2E 접근법 (Implicit Alignment E2E Approaches)
- CTC, RNN-T, RNA는 정렬 정보를 명시적으로 모델링해 스트리밍 처리가 용이함
  - 최종 프레임을 처리할 때까지 symbols를 계속 출력하고, 그 시점에서 디코딩 프로세스가 완료
- 반면, Attention-based Encoder-Decoder (AED, 예: **LAS**)는 **정렬 정보를 attention으로 암시적(implicitly)** 처리함
  - 디코더가 출력할 때마다 **인코더 출력 전체에서 attention mechanism을 통해 정렬 정보를 학습**함
- 출력 집합에 문장 종료 symbol인 〈eos〉를 추가 = C ∪ {⟨eos⟩}
![image](https://github.com/user-attachments/assets/d2ed31ad-9a53-4327-81fe-21c03ddd370b)  

#### 1) 암시적 정렬 방식의 작동 원리
- 인코더는 음성 입력 X = (x₁, …, xₜ′)를 고차원 시퀀스로 변환: H(X) = (h₁, …, hₜ)
- 디코더는 각 step마다 이전까지 출력된 심볼들을 요약한 디코더 상태 sᵢ와,
- attention을 통해 인코더 출력 H(X) 에서 현재 출력과 가장 관련 있는 부분을 요약한 context vector vᵢ 를 기반으로 다음 symbol cᵢ를 출력

- **디코딩**
  - 문장 시작 기호 c₀ = ⟨sos⟩ 을 입력으로 받아 디코딩 시작
  - 디코더는 매 스텝에서 다음을 수행:
    - 이전 상태 sᵢ₋₁, context vᵢ₋₁, 이전 출력 cᵢ₋₁ → 디코더 상태 sᵢ 생성
      - sᵢ = Decoder(vᵢ₋₁, sᵢ₋₁, cᵢ₋₁)
    - 현재 상태 sᵢ를 기준으로 인코더 출력 전체에 attention 적용 → context vector vᵢ 계산
    - sᵢ, vᵢ 기반으로 현재 출력 cᵢ 예측
  - 디코더는 문장 종료 기호  ⟨eos⟩ 가 출력될 때까지 반복 수행

#### 2) 출력 확률 계산 방식
  - 정답 시퀀스 C = (c₁, …, cᴸ)에 문장 종료 심볼 ⟨eos⟩를 붙인 시퀀스를 다음과 같이 정의
    - Cₑ = (c₁, …, cᴸ, ⟨eos⟩)
    - (L + 1)의 ground-truth symbol sequence
  - **조건부 확률 예측**
    - P(Cₑ | X) = P(Cₑ | H(X)) = ∏ᵢ₌₁⁽ᴸ⁺¹⁾ P(cᵢ | cᵢ₋₁, …, c₀ = ⟨sos⟩, H(X))
      - = ∏ᵢ₌₁⁽ᴸ⁺¹⁾ P(cᵢ | cᵢ₋₁, …, c₀ = ⟨sos⟩, vᵢ)
      - = ∏ᵢ₌₁⁽ᴸ⁺¹⁾ P(cᵢ | sᵢ, vᵢ)
    - sᵢ: 이전 출력들을 기반으로 생성된 디코더 상태
    - vᵢ: attention을 통해 현재 시점에서 참조해야 할 인코더 출력의 요약
    
#### 3) 모델 특징
  - **정렬 A를 명시적으로 정의하지 않음**
  - attention 메커니즘이 soft하게 정렬을 유도
  - 학습 시 alignment marginalization 불필요 → 학습 및 구현이 단순
  - **입력 전체를 디코더가 한꺼번에 참조 가능** → 문맥 반영에 유리
  - 출력은 <sos> 부터 시작해 <eos>이 나올 때까지 순차 생성
  - **디코더가 순차적으로 동작하므로 좌-우 비대칭성 존재**
    - 디코더는 과거 정보만 사용 → 향후 context는 활용 불가
  - 전체 입력을 먼저 받아야 하므로 스트리밍 적용이 어렵다

#### 4) AED 모델에서 Context Vector 계산하기
![image](https://github.com/user-attachments/assets/d9e7e4ed-423b-46fe-bfe9-96fa554df522)  

- **Context Vector vᵢ**
  - 디코더가 i-번째 출력 심볼 cᵢ를 예측할 때,
    - 디코더 상태 sᵢ (이전까지의 출력들을 요약한 내부 상태)와
    - 인코더 출력 H(X) 의 정보 중, 현재 예측에 가장 관련 있는 정보만 요약한 벡터가 필요
  - 이 요약 벡터가 바로 Context Vector vᵢ
  - “지금 이 타이밍에 인코더 출력 중 어디를 보고 cᵢ를 결정해야 할까?”라는 질문에 답하는 벡터
- **Attention Weight 계산 과정**
  - 인코더 출력:  H(X) = (h₁, h₂, …, hₜ)  (총 T개 프레임)
  - 디코더 상태: sᵢ ← c₁, …, cᵢ₋₁ 까지 출력한 후의 상태
  - Attention 함수: atten(hₜ, sᵢ) ∈ ℝ
    - 디코더 상태 sᵢ와 인코더 프레임 hₜ 간의 “유사도” 또는 “관련성 점수”
  - Softmax를 통해 각 프레임의 중요도(가중치)를 계산: αₜ,ᵢ = exp(atten(hₜ, sᵢ)) / ∑ₜ₌₁ᵗ exp(atten(hₜ′, sᵢ))
    - αₜ,ᵢ는 디코딩 시점 i에서 인코더 프레임 hₜ의 중요도를 나타냄
    - ∑ₜ αₜ,ᵢ = 1
- **Context Vector 계산**
  - attention weight αₜ,ᵢ를 사용하여 인코더 출력들의 가중합을 계산
  - vᵢ = ∑ₜ αₜ,ᵢ ⋅ hₜ
    - 디코더가 다음 출력 cᵢ를 생성할 때 참조하는 요약 정보
- Attention 종류
  - Dot-Product Attention
    - atten(hₜ, sᵢ) = hₜᵀ · sᵢ
    - 두 벡터의 내적을 사용
  - Additive Attention
    - atten(hₜ, sᵢ) = vᵀ · tanh(W₁hₜ + W₂sᵢ)
    - 두 벡터를 선형 변환 후, tanh를 거쳐 벡터 유사도를 계산
  - Gaussian Attention
    - 고정된 시간적 중심값 주변을 중심으로 연속적인 가중치를 부여하는 방식 (주로 음성 분야에서 사용됨)
  - Multi-Head Attention (MHA)
    - 단일 attention head는 **한 가지 관점에서만** 정보를 요약
    - 여러 개의 attention head를 사용해 **다양한 관점에서 정보를 병렬**로 요약
      - 각 head k에서 context vector vᵢᵏ를 계산
      - 이들을 단순 연결(concatenation): vᵢ = [vᵢ¹ ; vᵢ² ; … ; vᵢᴷ]
      - 이를 통해 모델은 시공간적 정보나 다양한 부분적 정렬을 동시에 처리 가능
    - 이 구조는 Transformer 디코더에서도 사용됨


### C. 정렬 모델링을 사용한 Attention 기반 E2E 접근법 (Attention-based E2E Approaches with Alignment Modeling)
- **기본 AED 모델의 한계**
  - 기본 AED (Attention-based Encoder-Decoder) 모델은 명시적 정렬 모델을 사용하지 않고, attention을 통해 암시적으로 정렬을 학습
  - 매우 강력한 성능을 보이며, **종종 CTC나 RNN-T 기반 모델을 정확도 측면에서 능가**
  - **하지만 두 가지 주요 단점이 존재**
    - **비스트리밍 모델** → 모든 음성 입력을 모두 본 뒤에야 디코딩 시작 가능
    - **긴 음성에 약함** → 긴 오디오에 대해 별도의 처리 없이 그대로 사용하면 성능이 저하될 수 있음
- **스트리밍 가능한 Attention 모델 구축 시도**
  - AED를 스트리밍 환경에 적용하려는 시도는 **“로컬 정렬 가능성”** 에서 출발
    - 음성은 기계번역과 달리 **입력과 출력 사이의 위치적 관계가 비교적 정렬되어 있음**
  - 따라서, 전체 인코더 출력을 보지 않고도 정렬된 일부 구간만 보고 디코딩 가능
 
#### 1) 대표 모델 및 방식 
- **Neural Transducer (NT)**
  - 입력 인코더 프레임을 길이 W의 겹치지 않는 Tᵂ 개의 청크로 명시적으로 분할
    - W: H₁ᵂ = [h₁, ..., hᵂ]; ..., Hₜᵂᵂ = [hₜᵂ₊₁, ..., hₜᵂ·ᵂ],
    - where Tᵂ = ⌈T / W⌉, and hₜ = 0 if t > T
      - W는 청크 길이 (chunk width)
      - H₁ᵂ, Hₜᵂᵂ: 각각 첫 번째와 마지막 청크의 인코더 출력
      - Tᵂ = ⌈T / W⌉: 총 청크 개수 (ceil 연산)
      - hₜ = 0 if t > T: T를 넘는 프레임에 대해서는 zero padding 처리 
  - 모든 인코딩된 프레임을 검사하는 AED 모델과 달리, NT 모델은 한 번에 하나의 청크만 처리하도록 제한
    - 각 청크 단위로 디코딩을 수행하고, 특수 심볼 (e.g., end-of-chunk) 출력 시 다음 청크로 넘어감
  - 초기 학습 시 정렬이 없다면  Tᵂ 개의 청크에 동일하게 분산된 대략적인 초기 정렬을 사용
    - 반복적으로 정렬을 보정 (HMM 기반 시스템의 강제 정렬과 유사)
	- 장점: 스트리밍 가능, 훈련 시 alignment 불필요
    
- **Hard Monotonic Attention - vanilla AED model**
  - 인코더 출력을 왼쪽에서 오른쪽으로 스캔하며, 출력을 생성할 시점을 명시적으로 예측
	- 이 결정은 Bernoulli(베르누이) 랜덤 변수를 통해 이루어짐
  - 선택된 프레임에서만 출력을 생성하고, 이후 반복 → 하드 단조 정렬(hard monotonic alignment) 실현

- **MoChA (Monotonic Chunkwise Attention)**
  - 위 방식의 개선형
  - 출력 프레임을 선택한 후, 해당 프레임의 왼쪽 구간(W 프레임)에 soft attention 적용 → context vector 생성
	- 2단계 구조:
    - 출력이 생성되어야 하는 프레임 식별
    - **선택된 프레임의 왼쪽에 있는 프레임에 대해 AED 모델을 적용**
   
- MILK (Monotonic Infinite Lookback)
  - MoChA보다 넓은 문맥 참조 가능
  - 선택된 프레임 이전의 모든 프레임(h₁ ~ h_τ) 에 대해 attention 적용
    - **각 단계에서 선택된 프레임 τ의 왼쪽에 있는 모든 프레임**(예: h₁ ~ h_τ)에 대해 **컨텍스트 벡터를 계산**
  - 더 넓은 문맥을 반영하되, 여전히 스트리밍 가능
 
- Triggered Attention
  - CTC 네트워크의 출력을 디코더의 attention 활성화 시점으로 사용
  - 즉, 디코더가 언제 작동해야 하는지를 CTC가 trigger(신호) 를 줌
  - 디코더는 지연이 제한된 상태로 작동 가능 → 실시간 응답 가능
 
#### 2) Segmental Attention 및 기타 확장 모델
- 긴 시퀀스에 대한 일반화를 개선
- **Segmental Attention**
  - 입력을 의미 있는 segment 단위로 나눠 attention 적용
  - 긴 문장에서도 더 안정적이며, 일반화 성능이 향상됨
 
- **Segmental E2E 모델**
  - 트랜스듀서 또는 HMM 기반 접근과도 연결 가능
  - attention 기반 방식과 HMM 또는 neural transducer 방식 간의 이론적 연결고리 존재  



## 3. 기본 E2E 모델의 구조 개선
- 지금까지 설명한 vanilla E2E 모델에 대해 자세히 설명
  - Vanilla : 가장 기본적인 형태의 모델, 아무런 추가적인 기법이나 최적화 없이 원형 그대로의 모델
- **Combinations of Models** : 다양한 상호 보완적인 E2E 모델을 결합하는 다양한 방법을 설명
- **Incorporating Context(문맥 정보 통합)** : 사용자 이름, 지명, 특수 명사 등 드물게 등장하는 고유 엔티티에 대해 인식 정확도를 높이기 위한 개선
- **Encoder/Decoder Structure** : TPU나 GPU에서 병렬 처리 최적화를 위해 설계된 구조 소개
- **Integrated Endpointing** : 모델의 대기 시간을 개선하는 방법에 대해 논의

### A. 모델 결합(Combinations of Models)
- **이유**
  - E2E 음성 인식 모델은 각각의 장점과 단점이 다름
  - 상황별로 강점을 가지는 모델이 다르기 때문에, 서로 보완(complementary)
- **CTC + Attention 모델 결합**
  - Attention 모델 단독 사용 시 발화 전체를 자유롭게 참조하기 때문에 **잡음이 많거나 긴 문장에서는 정렬이 불안정**
  - CTC는 시간 순서를 강제(왼→오) → 정렬이 안정적
  - CTC + Attention 모델을 동시에 학습하는 Multi-Task Learning (MTL) 구조 제안
  - WSJ, CHiME 데이터셋 기준 **WER (Word Error Rate) 5–14% 상대적 개선**
- **RNN-T + AED 모델 결합**
  - RNN-T : 스트리밍 가능 (왼→오 방향 디코딩), **길거나 시끄러운 발화에 강함**
  - AED : 전체 컨텍스트 사용 가능 → **문맥 이해가 필요한 발화 (예: 숫자 1.15)** 에 강함
  - 결합 방식
    - RNN-T로 1차 패스 디코딩
    - AED로 2차 패스에서 재점수화(rescoring)
  - RNN-T와 AED가 인코더를 공유 → 연산량 감소
  - **음성 검색 태스크에서 WER 17–22% 상대적 감소**
  - 하지만 실시간 응답을 위해선, 2차 패스도 스트리밍 가능해야 함

### B. 컨텍스트 통합(Incorporating Context)
- **이유**
  - 사용자 이름, 앱 이름, 연락처, 장소 등은 **고유명사(Proper Nouns)** 로 구성된 경우가 많음
  - 이러한 단어는 **빈도수가 낮고 발음이 유사한 경우도 많아** E2E ASR 모델에서 **인식이 어렵다**
  - 특히, **E2E 모델은 빔 서치 중 후보 개수가 작아지기 쉬워** 고유명사 인식 성능이 떨어짐
  - 따라서 E2E 모델에 문맥 정보를 **외부에서 주입하거나 내부적으로 반영**하는 것이 매우 중요함
- **Shallow Fusion Context Biasing**
  - **컨텍스트 정보(예: 앱 이름, 연락처 이름 등)를 FST 형태로 구성**
    - finite state transduce
  - 이 FST는 후보 구문(phrase bias)을 표현하고, 이를 **빔 서치 과정 중 E2E 모델의 score와 보간**
  - 단순하고 명시적이며, 시스템 동작 제어가 쉬움
  - 단점: 모델이 직접 학습한 정보는 아니며, 별도로 정제된 FST 필요
- **Neural 방식 – Embedding 기반 문맥 삽입**
  - 문맥 구문(phrase)을 **벡터로 임베딩**하여 모델 입력으로 넣는 방식
    - bias phrase들을 각각 벡터로 변환
    - 이 벡터들을 디코더에 함께 입력 → 디코더가 attention을 통해 특정 구문에 집중할 수 있음
    - **문맥 구문에 대한 확률이 높아지게 유도**
  - 학습 과정에서 모델이 직접 문맥의 역할을 배움
  - external FST 없이도 end-to-end 구조 유지 가능
- **대화 컨텍스트 기반 Biasing**
  - **이전 turns(말 차례)** 에 등장한 단어나 주제 정보를 context로 활용
  - 모델이 현재 입력을 디코딩할 때, **이전 대화 흐름에 따라 가중치를 조정**
  - 예시
    - 사용자: “알람 6시로 맞춰줘”
    - 다음 발화: “7시로 바꿔줘” → “바꿔줘”의 의미 해석에 이전 발화 문맥이 필요
- **음운/형태 정보 기반 임베딩**
  - 문맥 구문을 단순 문자열이 아니라 **음소/형태소 수준 정보로 임베딩**
  - **발음 유사성, 철자 구조 등을 모델이 더 잘 학습**하도록 돕는다
  - 발음이 비슷한 고유명사를 더 잘 구분 가능
- **발화 기반 i-vector 스타일 문맥 삽입**
  - 각 발화에 대해 **발화 전반을 요약하는 컨텍스트 벡터** 생성
  - 서브 네트워크에서 frame-level hidden states를 풀링
  - i-vector처럼 발화 단위로 요약된 context vector 생성
  - 이를 통해 발화 특유의 톤, 말투, 주제 등을 **전체적으로 고려할 수 있음**


### C. 인코더/디코더 구조(Encoder/Decoder Structure)
- **초기 E2E 모델 구조: LSTM 기반**
  - 초창기 E2E ASR 모델은 **인코더와 디코더 모두 LSTM 기반 RNN 구조**를 사용
    - LSTM (Long Short-Term Memory)은 시퀀스 데이터를 처리하기에 적합
    - 시간 흐름에 따른 정보를 순차적으로 처리하는 데 강점
  - 단점
    - 입력 시퀀스를 순차적으로 처리해야 하므로 **병렬 연산이 어렵다**
    - 음성 프레임 수가 많을 경우, **모델 학습과 추론 속도가 느림**
    - 고성능 연산을 지원하는 현대 하드웨어의 **병렬성(parallelism)을 활용하지 못함**
- **하드웨어 변화와 요구사항**
  - **온디바이스(모바일/엣지 디바이스) 환경**에서도 고속 음성 인식을 요구
  - 대표적으로 Google의 **Edge TPU**와 같은 칩은 **수천 개의 병렬 처리 코어**를 가지고 있음
  - 하지만 기존 RNN 기반 구조는 이런 **병렬 코어를 충분히 활용하지 못함**
  - **더 병렬 친화적이고, 연산 구조가 단순하며, 모바일 디바이스에서 실시간 추론이 가능한 인코더/디코더 구조**
- **새로운 구조로의 전환 동기**
  - 위의 배경에서 등장한 모델 : **Conformer, Transformer Encoder**
    - Conformer: CNN + Transformer 구조 (시간적 정보 + 병렬성 결합)
    - Transformer Encoder: 완전 병렬 처리 가능, 글로벌 컨텍스트 확보에 유리
  - 인코더는 더 이상 시간 순서를 따라야 할 이유가 없으므로 병렬화가 가능한 구조로 개선됨
  - 디코더도 LSTM → Transformer 디코더로 변화
    - 특히 multi-head attention은 이전 출력과 인코더 출력을 병렬로 다룰 수 있게 함
  - E2E 모델의 대부분의 성능이 인코더에 있음


### D. 통합 엔드포인트(Integrated Endpointing)
- **배경: Endpointing이란?**
  - **Endpointing**: 사용자의 **음성이 끝났음을 감지**하고, 그 시점에서 **음성 인식 결과를 서버로 전송**해 처리하는 과정
    - 즉, **“언제 음성이 끝났다고 판단할지”** 를 결정하는 메커니즘
  - **스트리밍 ASR 시스템**에서 매우 중요한 기능
    - 빠르고 정확한 endpointing이 없으면 **대기 시간(latency) 증가**
    - 너무 일찍 종료하면 **인식 누락**, 너무 늦으면 **불필요한 지연**
   
- **기존 방식: 외부 VAD (Voice Activity Detector)**
  - 전통적으로는 ASR 모델 외부에 존재하는 **VAD(음성 활동 검출기)** 를 통해 endpointing 수행
    - 음향 에너지 기반으로 **말하는 중인지 아닌지를 판단**
  - 정확한 타이밍 판단이 어렵고
  - **ASR 모델의 문맥 이해를 반영하지 못함**
    
- **최근 방식: ASR 모델에 endpointing 통합**
  - **ASR 모델 내부**에서 endpointing을 직접 수행하도록 개선 중
  - 대표적으로 스트리밍 **RNN-T 기반 연구들**
    - **〈eos〉 (end-of-speech)** 토큰을 명시적으로 예측하도록 학습
    - 예: 모델 출력 → “What’s the weather ⟨eos⟩”
  - 장점
    - **음향 정보 + 언어 정보 모두를 기반으로 종료 판단 가능**
    - **VAD보다 더 지능적인** 종료 타이밍 판단 가능
    - 실제 실험에서 **WER 손실 없이 대기 시간(latency) 단축** 가능

- **다른 연구들: CTC blank symbol 활용**
  - **CTC 모델의 blank** 기호를 활용해 endpointing을 시도
    - blank가 반복될 경우 “음성이 멈췄다”고 판단
    - 이 방식도 추가적인 모듈 없이 **경량화된 endpointing 구현 가능**
   

## 4. E2E 모델 훈련
- **딥러닝 기반 방식**
  - End-to-End ASR 모델의 학습은 기본적으로 **딥러닝 모델 훈련 체계**를 따름
    - 기본적 방식: **확률 모델의 로그우도 최대화 (log-likelihood maximization)**
    - 사용되는 알고리즘: **역전파(Backpropagation) + 확률적 경사 하강법 (SGD, Adam 등)**
- **학습의 난점: 순차성 + 정렬**
  - 음성 인식은 본질적으로 **순차적 데이터(sequence)** 를 다룸
  - 입력 음성 시퀀스 X = (x_1, …, x_T) 와 출력 텍스트 C = (c_1, …, c_L) 사이의 **길이가 다름**
    - **정렬(alignment)** 이 명확하지 않음 (어느 프레임이 어떤 글자에 대응되는지 모름)
  - 전통 ASR은 이를 HMM 등을 통해 처리했지만, E2E는 **이 정렬을 모델이 직접 학습**해야 하므로 **학습 난이도↑**
- **학습 기준 (Training Objective)**
  - 학습 데이터셋: 𝒯 = { (𝑋ᵢ, 𝐶ᵢ) }ⁿᵢ₌₁
    - X_i: i번째 입력 음성 시퀀스
	  - C_i: i번째 정답 텍스트 시퀀스
  - 학습 목표는 전체 조건부 로그우도 합을 최대화
    - 즉, 손실 함수 L을 다음과 같이 정의: 𝐿 = − ∑ⁿᵢ₌₁ log 𝑃(𝐶ᵢ | 𝑋ᵢ)
  - 또는, **전체 데이터에서 예측한 문장의 확률을 높이는 방향**으로 훈련
- **E2E 모델의 강점: 독립 가정 없음**
  - 전통 ASR (예: GMM-HMM)은 프레임 간 조건부 독립 가정을 전제로 함
    - 즉, 시간 t의 프레임은 이전/이후 프레임과 독립적으로 처리됨
  - **E2E는 이런 독립 가정을 하지 않음**
    - 예: attention 기반 AED는 **전체 입력 시퀀스를 고려해 현재 출력을 예측**
    - 문맥 정보 활용 가능 → **긴 종속성(long dependency)** 처리에 강함
   
### A. 통합 엔드포인트(Alignment in Training)


### B. 통합 엔드포인트(Training with External Language Models)


### C. 통합 엔드포인트(Minimum Error Training)


### D. 통합 엔드포인트(Pretraining)


### E. 통합 엔드포인트(Training Schedules and Curricula)


### F. 통합 엔드포인트(Optimization and Regularization)


### G. 통합 엔드포인트(Data Augmentation)
