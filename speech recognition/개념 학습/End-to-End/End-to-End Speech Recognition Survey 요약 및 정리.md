# 논문 정리 : End-to-End Speech Recognition: A Survey (T-ASLP 2024)

## Abstract
- **딥러닝이 도입된 ASR** 모델이 도입되지 않은 모델과 비교해 **단어 오류율(WER)이 50%이상 감소**
  - **WER** : 음성인식 시스템이 인식한 텍스트가 정답과 얼마나 다른지를 측정하는 평가 지표
    - WER = (I + D + S) / N
    - 추가, 삭제, 대체된 단어 수를 전체 정답 단어 수로 나누어 계산
      - **I (Insertions)**: 추가된 단어 수 (인식 결과에만 존재)
      - **D (Deletions)**: 빠뜨린 단어 수 (정답에 있었는데 인식 결과엔 없음)
      - **S (Substitutions)**: 잘못 인식된 단어 수 (ex. "love" → "like")
      - **N**: 정답 문장의 총 단어 수
    - 낮을수록 좋은 값
  - **CER** : 음성 인식의 오류율을 **문자 단위**로 계산하는 지표
    - 한국어, 중국어 등 공백이 없는 언어에서도 유용
    - WER에 비해 좀 더 세밀한 오류 파악 가능
- 현재는 **End-to-End(E2E)모델이 음성인식의 주 방식**




## 1. Introduction
### 전통적인 ASR 시스템 구조
- 전통적인 통계적 ASR 아키텍처는 네 가지 주요 요소로 구성
  - **음향 특징 추출 (Feature Extraction)** : 음성 오디오 신호로부터
  - **음향 모델링 (Acoustic Modeling)** : 주로 HMM, GMM
    - 은닉 마르코프 모델(HMM) : 화자의 발화 속도 변화를 반영
  - **언어 모델링 (Language Modeling)** : **카운트 기반 방식(예: N-gram)**
  - **탐색/디코딩 (Search)** : 베이즈 결정 규칙(bayes decision rule)에 기반한 탐색 과정


### 딥러닝 도입 시작
- 음향 모델에서 **가우시안 혼합 모델(GMM)** 대신 신경망 모델 사용
- 언어 모델도 **딥러닝 기반 언어 모델로 대체**
- 하지만 여전히 파이프라인 구조 자체는 유지


### 여전히 많은 별도의 구성 요소와 전문 지식에 의존
- 음성 신호 전처리
- 녹음 환경 변화에 대한 견고성 확보 기법
- 음소 목록과 발음 사전
- 음소 클러스터링
- OOV(사전에 없는 단어) 처리
- 다양한 적응/정규화 방법
- 복잡한 학습 스케줄 (예: 시퀀스 기반 판별 학습 포함) 등


### 시퀀스-투-시퀀스(sequence-to-sequence)
- 음성 전처리 + 특징 추출 + 음향 모델링 → 통합
- 점점 **sequence-to-sequence** 형태로 발전
- 기존 음성처리 특화 설계 대신 **범용 딥러닝 모델로 처리**


### End-to-End 정의
- **Cambridge Dictionary**에 따르면, **“end-to-end”**라는 형용사는 **“하나의 과정의 모든 단계를 포함하는 것”** 으로 정의
  - 하나의 과정으로 끝까지 처리되는 음성 인식 모델
- 단순히 구조만 하나가 아닌 모델 구성, 학습 방식, 데이터 활용 등 모든 측면에서 통합된 구조 의미
  - **a) Joint Modeling (통합 모델링)** : 기존의 모듈화된 모델을 하나의 신경망으로 통합
  - **b) Single-Pass Search (단일 탐색 과정)** : 전체 정보를 기반으로 한 번에 결과 예측
    - Search는 모델이 **어떤 출력을 내릴지 탐색**하는 과정 == **디코딩(decoding)**
    - 전통모델 처럼 여러 모듈 따로 돌아가게 하지 않고, **하나의 디코더 블록이 전체 역할을 맡음**
  - **c) Joint Training (통합 학습)** : 하나의 목적함수로 전체 모델 학습 (ex. WER 최소화)
  - **d) Training Data (단일 종류의 학습 데이터)** : 전사된 음성 데이터만으로 학습
    - **전사(transcription)** : 음성 데이터를 듣고, 그 내용을 문자로 적은 것
    - **전사된 음성 데이터** : 음성과 해당 텍스트가 짝을 이루는 데이터
  - **e) Training from Scratch (처음부터 학습)** : 사전 정렬이나 초기 모델 없이 처음부터 학습
    - 과거 음향 모델은 단어의 음소가 몇 초 구간에 위치하는지 미리 알고 있어야 했음 -> **Forced Alignment**로 구함
    - 그런 정렬 정보 없이도 **모델이 직접 정렬도 학습**
  - **f) Secondary Knowledge Sources (2차 지식원 회피)** : 발음 사전, 음소 집합 같은 외부 지식 사용 안함
    - 모델이 직접 문자나 음소 단위의 분포를 학습하고 정렬까지 담당 
    - **일관성 문제, 비용, 오류 가능성** 등을 줄여줌
  - **g) Vocabulary Modeling (어휘 모델링 방식)** : 발음 사전을 사용하지 않으며, 문자/서브워드 기반 사용
    - 전체 단어 기반 모델은 **엄청난 양의 전사 데이터를 필요로 하므로 현실적으로 어려움 존재**
  - **h) Generic vs. Informed Modeling (범용 vs. 특화 모델링)** : 도메인 지식 없이도 데이터로부터 학습
    - **예를 들어, ASR의 순차적(monotonic) 정렬 특성을 attention 기반 모델에서는 학습을 통해, HMM에서는 시스템 구조로 직접 구현**
- 특히 전통적인 ASR의 발음사전, 정렬 정보, 모듈별 최적화 과정을 피하고, 오직 데이터를 기반으로 전체 시스템을 하나의 목표 아래 학습하는 것이 핵심


### ASR 시스템 개발의 목표
- 주 목표
  - 단어 오류율(WER: Word Error Rate) 최소화
- 부차적 목표
  - 디코더의 시간/메모리 복잡도 줄이기
  - 모델링을 쉽게 하고, 다양한 도메인/언어에 적용할 수 있는 범용성 확보 


### End-to-End 이점
- **구조적 단순화와 범용성 (Simplicity & Generality)**
  - 하나의 신경망 구조로 전체 시스템 구성
  - **디코딩**이 단일 모델 안에서 처리되어 **복잡한 모델 통합 불필요**
  - **새로운 언어나 도메인에 대해 빠른 개발 주기 달성**
- **효율성과 경량화 (Efficiency & Lightweight Design)**
  - 메모리, 연산 자원이 줄어들어 임베디드 환경에서의 ASR 적용에도 유리
- **학습 최적화와 안정성 (Learning Optimization & Robustness)**
  - 통합 학습(joint training)으로 잘못된 국소 최적화 지점 피할 수 있음
- **외부 리소스 없이 데이터 기반 학습 (Data-Driven & Resource-Free)**
  - 발음사전과 같은 2차 지식원 불필요
  - 2차 지식원이 잘못된 경우 생기는 오류 방지
  - 충분한 작업-특화 데이터(task-specific data)만 있으면 학습 가능


### 논문의 구성
- 제2장: E2E 음성 인식의 역사적 발전 과정과, **입출력 정렬 방식(alignment)** 관점의 분류
- 제3장: 기본 E2E 모델에 대한 개선 (구조, 문맥, loss 등)
- 제4장: E2E 모델 학습 방법
- 제5장: 다양한 E2E 디코딩 전략
- 제6장: E2E ASR에서의 **언어 모델(LM)** 의 역할과 통합
- 제7장: 최신 E2E 모델과 전통적인 HMM 기반 ASR의 비교
- 제8장: E2E 및 전통 ASR 접근법에 대한 성능 비교
- 제9장: E2E ASR의 실제 적용 사례
- 제10장: E2E ASR 향후 연구 방향
- 제11장: 결론



## 2. End-to-End 음성 인식 모델의 분류 체계

### 논문 전반 표기법
- **X : 입력 음성 발화**
  - 길이 **T′** 의 **D** 차원 음향 프레임 시퀀스 (예: log-mel features)로 표현
    - D는 각 프레임에서 추출되는 음향 특성 벡터의 차원 수 - **입력 음성의 “한 프레임당 정보량”을 몇 차원으로 표현할 것인지 결정하는 값**
    - 일반적으로는 40~80 차원이 자주 사용
  - X = $(x₁, …, x_{T′})$,   where   $x_t$ ∈ $ℝᴰ$
- **C : 입력 발화에 대응하는 단어 또는 문자 시퀀스**
  - 길이 $L$ 의 라벨(label) 시퀀스로 분해할 수 있으며, 라벨 단위는 문자, 단어, subword (예: BPE, word-piece 등)로 구성
    - **문자 단위는 너무 길고, 단어 단위는 희귀어 문제(OOV)가 심한데, Subword는 둘의 장점을 잘 절충해서 처리할 수 있기 때문**
    - **BPE (Byte Pair Encoding)**
      - 빈도 높은 문자 쌍부터 반복적으로 병합해서 단위를 만들어냄
      - 병합 규칙이 단순함 (문자 쌍 병합)
      - 예측 단위는 subword → 예: “reading” → “read”, “ing” 
    - **WordPiece**
      - Google에서 만든 방식 (BERT, T5 등에서 사용)
      - 기본 원리는 BPE와 비슷하지만, 병합 시 **likelihood**를 기반으로 선택함 (확률 모델 기반)
        - **“주어진 관측값이 어떤 확률 모델 하에서 얼마나 일어날 법한가“를 나타내는 수치**
        - **주어진 데이터가 특정 모델 하에서 발생할 가능성의 척도**
      - 더 언어학적 의미 단위에 가까운 subword를 생성 가능
  - C = $(c₁, …, c_L)$,   where   $c_j$ ∈ $𝒞$
  - ex) 문자 단위: C = (h, e, l, l, o), → L = 5
- **H(X) : 인코더 모듈**
  - 모든 **E2E 모델은 공통적으로 인코더 구조**를 포함
  - 인코더는 입력 음향 시퀀스 **$X$를 고차원 표현 $H(X)$**로 변환
  - X의 원시 특징을 요약하고, 모델이 C를 예측하는 데 필요한 정보를 담는것
  - H(X) = $(h_1, ..., h_T)$ (길이 T) 보통 T ≤ T′
    - 인코더는 **다운샘플링(stride, pooling)** 이나 **요약(압축)** 을 통해 입력보다 짧은 길이의 표현 T를 만들어내는 경우가 많음
    - 중요한 정보만 남기고 요약하기 위해
    - 입력이 너무 길면 연산량이 많아져 메모리, 학습시간 증가
  - 인코더는 RNN, CNN, Transformer 등 다양한 신경망 구조로 구현될 수 있으며, 논문에서는 이러한 구조의 세부 구현에는 구애받지 않고 일반적으로 설명
- **𝓣 : 훈련 데이터 집합**
  - 훈련 데이터는 **(음성, 전사)** 쌍으로 구성된 N개의 학습 샘플
  - $`𝒯 = { (Xᵢ, Cᵢ) ∣ i = 1, …, N }`$
 
- 모델의 학습 목표는, **입력 음성 $X$가 주어졌을 때 출력 시퀀스 $C$가 나올 조건부 확률**을 예측하는 것
- $$P(C|X) = P(C|H(X))$$
  - 입력 음성 X가 주어졌을 때, 그 음성을 인코더를 통해 벡터 표현 H(X)로 바꿔서, 최종적으로 그에 해당하는 텍스트 C를 생성할 확률을 구하겠다
  - 직접 X에서 바로 예측하는 게 아니라, 먼저 인코더를 통해 고차원 표현 H(X)를 만들고, 그 표현을 바탕으로 C를 예측하는 구조로 이루어져 있다는 걸 의미



### 정렬(Alignment)의 필요성
- End-to-End ASR 모델의 핵심 목표는 **입력 음성 X로부터 출력 텍스트 C를 직접 예측하는 확률 분포 P(C|X)를 학습하는 것**
  - 음성(X)은 길고 연속적인 시퀀스이고, 텍스트(C)는 상대적으로 짧은 **불연속적인 단위(문자, 단어 등)** 로 이루어져 있음
  - 이 둘을 연결하기 위해 **어떤 프레임이 어떤 문자/단어에 대응되는지 정렬하는 과정(Alignment)** 이 필요


### 정렬(Alignment)의 발전
- 전통적인 ASR에서는 프레임 단위 정렬을 **HMM(Hidden Markov Model)** 로 모델링하고, **GMM(Gaussian Mixture Model)** 또는 **신경망**을 통해 음향 프레임의 출력 분포를 학습
  - 프레임 단위 정렬 : 음성 시퀀스의 각 시간 단위(프레임)가 어떤 음소 또는 문자에 대응되는지를 명확히 지정하는 것
    - 예를 들어 “hello”라는 단어를 사람이 말하면, 약 1초 정도의 음성이 나온다고 가정
    - 이 1초를 10ms 단위로 쪼개면 총 100개의 프레임 생성
    - → **이 100개의 프레임 중 1~10은 h, 11~20은 e, …** 처럼 **각 프레임이 어떤 라벨에 해당되는지를 정렬**
  - 음향 프레임의 출력 분포 학습 : 이 프레임이 어떤 소리를 의미할 확률이 높은지 학습
    - 프레임 xₜ가 있을 때 → 이게 음소 /h/ 일 확률, /l/일 확률 등
    - 각 프레임을 입력받아서, **그 프레임이 어떤 소리에 해당될 확률 분포를 출력하는 모델** 
- 신경망 기반 음향 모델 학습을 위해, GMM-HMM을 기반으로 한 **강제 정렬(Forced Alignment) 기법** 이 널리 사용됨
- 최근에는 초기 정렬 없이 **전체 시퀀스를 직접 학습하는 방법(Sequence-level training)** 도 사용 가능


### E2E의 정렬
- 일반적으로 **문자 또는 (sub-)word 단위**로 라벨 정의
  - 전통적인 ASR 시스템에서의 **음소 단위 정렬**은 자연스러운데, **문자 단위 정렬은 직관적이지 않음**
  - **음소**는 발음과 직접 연결되기 때문에 **프레임과의 정렬이 비교적 자연스러움**
  - 하지만 E2E는 문자나 서브워드 단위로 정렬하기에 어떤 프레임이 ‘a’, ‘b’에 대응되는지 모호
- **명시적(Explicit) 정렬**
  - 정렬을 **잠재 변수(latent variable)** 로 명시하고, 학습 시 이를 주변화(marginalization) 처리
    - 잠재 변수 : **직접 관측할 수는 없지만, 모델 내부에서 반드시 추정해야 하는 변수**
    - 주변화 : **가능한 모든 정렬 경우의 수에 대해 확률을 더해서 전체 확률을 계산**
  - **CTC (Connectionist Temporal Classification)**
  - **RNN-T (Recurrent Neural Network Transducer)**
  - **RNA (Recurrent Neural Aligner)**
  - **HAT (Hybrid Auto-Regressive Transducer)**
  - 후속 모델일수록 **독립 가정(independence assumption)** 을 줄이고, **모델 성능이나 표현력이 향상**
    - 독립 가정 : “각 시간 프레임의 예측은 서로 독립적이다”라고 가정
      - “프레임 1에서 h 예측”이 “프레임 2에서 e 예측”에 아무 영향도 안 준다고 가정
    - 독립 가정이 줄어들면 좋은점
      - 자연어는 연속성(context) 이 중요한데, 독립 가정이 있으면 문맥을 반영하지 못함
      - **프레임 간 의존성(문맥)을 모델이 더 잘 학습할 수 있어 → 성능 향상**
- **암시적(Implicit) 정렬**
  - **정렬을 명시하지 않고, attention 메커니즘**을 통해 동적으로 정렬 관계를 학습
  - 전체 음향 시퀀스와 개별 라벨 간의 매핑 관계 학습
  - **Attention-based Encoder-Decoder (LAS 등)**
- **중간적인 접근 방식**
  - 정렬을 명시적으로 구성하면서도, attention 매커니즘을 활용해 유연성 확보
    - **attention 메커니즘을 활용해 주변 음향 정보를 참고하여 예측을 정제(refine)**
  - **Neural Transducer**
  - 모노토닉 정렬(monotonic alignment) 기반
    - **monotonic = 일방향으로만 진행하는** -> 시간 순서를 앞으로만 진행하는 정렬 방식
      - 자연스러운 말하기는 **시간적으로 순차적**이기 때문에, 모델도 정렬을 **시간 순서대로만 하도록 제한하면 더 현실적인 학습**
    - MoChA (Monotonic Chunkwise Attention)
    - MILK (Monotonic Infinite Lookback) 


### A. 명시적 정렬 기반 E2E 접근법 (Explicit Alignment E2E Approaches)
- 입력 음성의 인코더 출력 시퀀스와 정답 텍스트 사이의 정렬 관계를 **잠재 변수(latent variable)** 로 정의
  - **T 길이의 인코더 출력 H(X)** 와 **L 길이의 출력 시퀀스 C** 사이의 정렬 정보를 **명시적인 정렬 변수 A**로 표현
  - CTC, RNN-T, RNA 각각의 모델별 정렬 A를 정의하는 방식이 다름
  - 해당 차이는 모델의 내부 **조건부 독립 가정(conditional independence assumption)**, 그리고 학습 및 디코딩 방식에도 영향
- 공통 구조 : ⟨b⟩ - blank 기호 도입
  - 출력 기호 집합 확장 : $C_b = C \cup \{⟨b⟩\}$
  - 문자와 문자 사이의 공백 혹은 타이밍 조절 역할 수행
  - 각 모델마다 그 의미나 사용 방식은 조금씩 차이 존재
- 주변화(Marginalization)
  - 모델은 하나의 정렬이 아닌, **여러 가능한 정렬 시퀀스(A)** 를 고려
  - 특정 훈련 예제 (X, C)에 대해, 유효한 정렬 집합 ${A}_{(T,C)}$ 를 정의
  - 이를 기반으로 전체 조건부 확률 P(C|X) 주변화
  - $`P(C|X) = P(C|H(X)) = \sum_{A} P(C | A, H(X)) \cdot P(A | H(X)) <br>
= \sum_{A \in \mathcal{A}_{(T = |H(X)|, C)}} P(A | H(X)) \tag{1}`$
    - $`\sum_{A} P(C | A, H(X)) \cdot P(A | H(X))`$ -> 가능한 모든 정렬 시퀀스 A를 다 고려해서 평균을 냄(sum A)
      - P(A | H(X)) -> 인코더 출력 H(X)가 주어졌을 때 **정렬 A가 일어날 확률**
      - P(C | A, H(X)) -> 정렬 A가 주어졌을 때, 실제로 그 정렬이 **정확히 C를 만들었는지 여부**
  - 여기서 조건 : $`P(C | A, H(X)) = 1 \quad \text{if} \quad A \in \mathcal{A}_{(T, C)}`$
    - 즉, **특정 정렬 A가 라벨 시퀀스 C를 정확히 생성할 수 있을 경우에만 1의 확률을 부여**
    - 하나의 정렬 A에서 라벨 시퀀스 C로의 매핑은 **유일하게 정의**


#### 1) CTC (Connectionist Temporal Classification)
- 입력된 **음성 시퀀스 X를 출력 텍스트 시퀀스 C로 직접 매핑**하는 모델
- **Blank 라벨 ⟨b⟩**
  - $`C_b = C \cup \{\langle b \rangle\}`$
  - 인코더 출력 H(X)와 라벨 시퀀스 C 사이의 정렬을 명시적으로 모델링하기 위해
  - 반복되는 라벨을 구분하기 위해 <b> 추가
    - [e, ⟨b⟩, e]처럼 넣어줘야 “두 개의 e”로 인식 가능 
- **정렬 A의 정의**
  - $`A \in C_b^*`$ - 전체 길이는 T (인코더 출력 길이)와 동일
    - 인코더 출력 H(X)는 길이 T의 시퀀스 == 정렬 A도 길이 T인 라벨 시퀀스
    - 인코더 출력이 T 프레임이니까, 각 프레임마다 어떤 라벨이 대응될지 정렬 시퀀스를 만들 수 있음
  - 가능한 정렬 집합 : $`\mathcal{A}_{\text{CTC}}(X, C) = \{ A = (a_1, a_2, …, a_T) \mid a_t \in C_b \}`$
- **유효한 정렬**
  - **연속된 동일 라벨을 먼저 병합(collapse)한 후, 모든 ⟨b⟩ 기호 제거했을때, A가 정확히 C가 되는 경우만 유효**
  - 예를 들어, T = 10, C = (s, e, e) 라고 할 때, $`A = (s, \langle b \rangle, \langle b \rangle, e, e, \langle b \rangle, e, e, \langle b \rangle, \langle b \rangle)`$
  <img width="603" alt="image" src="https://github.com/user-attachments/assets/440025e6-6530-4535-9c77-3a44baa330e7" />  

- **CTC 확률 계산**
  - 가능한 모든 정렬에 대해 **확률을 합산** - **주변화(marginalization)**
  - $`P_{\text{CTC}}(C | X) = \sum_{A \in \mathcal{A}_{\text{CTC}}(X, C)} P(A | H(X))`$
    - $`= \sum_{A \in \mathcal{A}{\text{CTC}}(X, C)} \prod{t=1}^{T} P(a_t \mid a_{t-1}, …, a_1, H(X))`$
    - $`= \sum_{A \in \mathcal{A}{\text{CTC}}(X, C)} \prod{t=1}^{T} P(a_t \mid h_t) \tag{2}`$
- **강한 독립 가정**
  - 각 시간 t의 출력은 다른 시간 스텝과 독립적이라고 가정
    - 오직 해당 시점의 인코더 출력 $h_t$ 만을 기반으로 결정
  - 이 가정 덕분에 학습과 추론이 단순해지지만, 긴 문맥 정보나 복잡한 의존 관계를 반영하는 데는 한계 존재

- **모델 구조**  
  <img width="586" alt="image" src="https://github.com/user-attachments/assets/0776e2ca-66d2-4474-b64d-a9d55195e3f9" />   

  - 각 시점 t에서 $P(a_t | X)$ 를 모델링하는 신경망 구조로 구성
  - **인코더(Encoder)**
    - 입력 음성 X → 인코더 → 고차원 시퀀스 표현 $`H(X) = (h_1, …, h_T)`$
      - $`h_t \in \mathbb{R}^k`$
    - **CNN, RNN (예: LSTM)**
  - **출력(Softmax layer)**
    - 각 시간 t에서 : $`P(a_t = c \mid X) = P(a_t = c \mid H(X))`$
    - 각 프레임에서 ⟨b⟩ 포함 전체 라벨 집합 $`C_b`$ 에 대한 확률 분포 출력
    - 매 스텝마다 ⟨b⟩ 혹은 실제 라벨 중 하나를 선택하여 출력


#### 2) RNN-T
- 기존 CTC가 갖는 **강한 조건부 독립 가정(각 시간 스텝 출력이 독립)을 완화**하여 개선한 모델
- **Blank 라벨 ⟨b⟩**
  - CTC : 반복 라벨을 구분하기 위한 중간 기호
  - 각 인코더 프레임 $ h_t $에 대해, 모델은 0개 이상의 라벨을 출력하고, 마지막에 blank로 종료
  - 즉, 반복되는 라벨을 위한 별도 조치가 필요 없음
- **유효한 정렬**
  - T + L 길이의 시퀀스 A로 정의되며, blank를 제거했을 때 C
    - 모든 ⟨b⟩ 기호를 제거하면 정확히 C가 되는 시퀀스만 유효한 정렬로 간주 
  - $ 𝒜ᴿᴺᴺᵀ(X, C) $ = $ { A = (a₁, a₂, ..., a_{T+L}) } $
  - 출력 위치 τ에서, $i_τ$ 는 정렬 시퀀스 A의 처음부터 τ - 1까지 등장한 non-blank 라벨의 수를 나타냄
    - 이때, 그 구간에 포함된 blank의 수는 τ - $ i_τ $ - 1
  - 예를 들어 T = 7, C = (s, e, e)일 때 A = (⟨b⟩, s, ⟨b⟩, ⟨b⟩, ⟨b⟩, e, e, ⟨b⟩, ⟨b⟩, ⟨b⟩)
    - 이는 유효한 정렬에 포함  

   
  <img width="586" alt="image" src="https://github.com/user-attachments/assets/ed774ebb-60e9-4164-b86f-1da32f4dcd25" />  

- **RNN-T 확률 계산**  
  - $`P_{\text{RNNT}}(C \mid X)
= \sum_{A \in \mathcal{A}^{\text{RNNT}}(X, C)} P(A \mid H(X)) \\
= \sum_{A \in \mathcal{A}^{\text{RNNT}}(X, C)} \prod_{\tau = 1}^{T+L} P(a_\tau \mid a_{\tau-1}, \dots, a_1, H(X)) \\
= \sum_{A \in \mathcal{A}^{\text{RNNT}}(X, C)} \prod_{\tau = 1}^{T+L} P(a_\tau \mid c_{i_\tau}, \dots, c_0, \mathbf{h}_{\tau - i_\tau}) \\
= \sum_{A \in \mathcal{A}^{\text{RNNT}}(X, C)} \prod_{\tau = 1}^{T+L} P(a_\tau \mid \mathbf{p}_{i_\tau}, \mathbf{h}_{\tau - i_\tau}) \tag{3}`$  
    - 여기서 $`a_\tau`$: 정렬된 시퀀스 A의 τ번째 심볼 (blank 포함 가능)
      - 이전까지 예측된 non-blank 라벨 시퀀스에 조건부 종속
      - 하지만 정렬이 이루어진 프레임의 위치 (즉, 언제 출력되었는지)는 고려하지 않음
  	- $`p_{i_\tau}`$: prediction network의 출력 (non-blank 라벨들에 기반)
    - $`h_{\tau - i_\tau}`$: 인코더에서 나온 acoustic context vector
    - $`P = (p_1, \cdots, p_L)`$ 은 prediction network의 출력 시퀀스
      - 이전까지 예측된 non-blank 라벨들의 시퀀스를 요약한 벡터들
  - RNN-T는 CTC보다 약한 독립 가정 사용

- **모델 구조**  
  <img width="580" alt="image" src="https://github.com/user-attachments/assets/9951c35a-c928-4cde-bccf-b2adc764af85" />  

  - 이 prediction network는 또 하나의 신경망으로 구현
    - 각 출력은 다음과 같이 정의된다: $`p_j = NN(c_0, …, c_{j-1})`$
    - 여기서 $c_0$ 은 문장의 시작을 나타내는 특수 라벨 ⟨sos⟩

- **RNN-T의 일반화**
  - [43]의 연구에서는 RNN-T 모델을 더 일반화된 형태로 확장
  - 정렬 가능한 프레임 단위 시퀀스를 임의의 그래프 구조로 표현 가능
  - 조건부 확률 계산을 위한 forward-backward 알고리즘을 일반화하여 적용
  - 이 구조에서는 CTC처럼 각 프레임마다 하나의 출력만 발생시키는 방식도 포함 가능
  - **Prediction Network를 통한 문맥 반영은 그대로 유지**
