# 논문 정리 : End-to-End Speech Recognition: A Survey (T-ASLP 2024)

## Abstract
- **딥러닝이 도입된 ASR** 모델이 도입되지 않은 모델과 비교해 **단어 오류율(WER)이 50%이상 감소**
  - **WER** : 음성인식 시스템이 인식한 텍스트가 정답과 얼마나 다른지를 측정하는 평가 지표
    - WER = (I + D + S) / N
    - 추가, 삭제, 대체된 단어 수를 전체 정답 단어 수로 나누어 계산
      - **I (Insertions)**: 추가된 단어 수 (인식 결과에만 존재)
      - **D (Deletions)**: 빠뜨린 단어 수 (정답에 있었는데 인식 결과엔 없음)
      - **S (Substitutions)**: 잘못 인식된 단어 수 (ex. "love" → "like")
      - **N**: 정답 문장의 총 단어 수
    - 낮을수록 좋은 값
  - **CER** : 음성 인식의 오류율을 **문자 단위**로 계산하는 지표
    - 한국어, 중국어 등 공백이 없는 언어에서도 유용
    - WER에 비해 좀 더 세밀한 오류 파악 가능
- 현재는 **End-to-End(E2E)모델이 음성인식의 주 방식**




## 1. Introduction
### 전통적인 ASR 시스템 구조
- 전통적인 통계적 ASR 아키텍처는 네 가지 주요 요소로 구성
  - **음향 특징 추출 (Feature Extraction)** : 음성 오디오 신호로부터
  - **음향 모델링 (Acoustic Modeling)** : 주로 HMM, GMM
    - 은닉 마르코프 모델(HMM) : 화자의 발화 속도 변화를 반영
  - **언어 모델링 (Language Modeling)** : **카운트 기반 방식(예: N-gram)**
  - **탐색/디코딩 (Search)** : 베이즈 결정 규칙(bayes decision rule)에 기반한 탐색 과정


### 딥러닝 도입 시작
- 음향 모델에서 **가우시안 혼합 모델(GMM)** 대신 신경망 모델 사용
- 언어 모델도 **딥러닝 기반 언어 모델로 대체**
- 하지만 여전히 파이프라인 구조 자체는 유지


### 여전히 많은 별도의 구성 요소와 전문 지식에 의존
- 음성 신호 전처리
- 녹음 환경 변화에 대한 견고성 확보 기법
- 음소 목록과 발음 사전
- 음소 클러스터링
- OOV(사전에 없는 단어) 처리
- 다양한 적응/정규화 방법
- 복잡한 학습 스케줄 (예: 시퀀스 기반 판별 학습 포함) 등


### 시퀀스-투-시퀀스(sequence-to-sequence)
- 음성 전처리 + 특징 추출 + 음향 모델링 → 통합
- 점점 **sequence-to-sequence** 형태로 발전
- 기존 음성처리 특화 설계 대신 **범용 딥러닝 모델로 처리**


### End-to-End 정의
- **Cambridge Dictionary**에 따르면, **“end-to-end”**라는 형용사는 **“하나의 과정의 모든 단계를 포함하는 것”** 으로 정의
  - 하나의 과정으로 끝까지 처리되는 음성 인식 모델
- 단순히 구조만 하나가 아닌 모델 구성, 학습 방식, 데이터 활용 등 모든 측면에서 통합된 구조 의미
  - **a) Joint Modeling (통합 모델링)** : 기존의 모듈화된 모델을 하나의 신경망으로 통합
  - **b) Single-Pass Search (단일 탐색 과정)** : 전체 정보를 기반으로 한 번에 결과 예측
    - Search는 모델이 **어떤 출력을 내릴지 탐색**하는 과정 == **디코딩(decoding)**
    - 전통모델 처럼 여러 모듈 따로 돌아가게 하지 않고, **하나의 디코더 블록이 전체 역할을 맡음**
  - **c) Joint Training (통합 학습)** : 하나의 목적함수로 전체 모델 학습 (ex. WER 최소화)
  - **d) Training Data (단일 종류의 학습 데이터)** : 전사된 음성 데이터만으로 학습
    - **전사(transcription)** : 음성 데이터를 듣고, 그 내용을 문자로 적은 것
    - **전사된 음성 데이터** : 음성과 해당 텍스트가 짝을 이루는 데이터
  - **e) Training from Scratch (처음부터 학습)** : 사전 정렬이나 초기 모델 없이 처음부터 학습
    - 과거 음향 모델은 단어의 음소가 몇 초 구간에 위치하는지 미리 알고 있어야 했음 -> **Forced Alignment**로 구함
    - 그런 정렬 정보 없이도 **모델이 직접 정렬도 학습**
  - **f) Secondary Knowledge Sources (2차 지식원 회피)** : 발음 사전, 음소 집합 같은 외부 지식 사용 안함
    - 모델이 직접 문자나 음소 단위의 분포를 학습하고 정렬까지 담당 
    - **일관성 문제, 비용, 오류 가능성** 등을 줄여줌
  - **g) Vocabulary Modeling (어휘 모델링 방식)** : 발음 사전을 사용하지 않으며, 문자/서브워드 기반 사용
    - 전체 단어 기반 모델은 **엄청난 양의 전사 데이터를 필요로 하므로 현실적으로 어려움 존재**
  - **h) Generic vs. Informed Modeling (범용 vs. 특화 모델링)** : 도메인 지식 없이도 데이터로부터 학습
    - **예를 들어, ASR의 순차적(monotonic) 정렬 특성을 attention 기반 모델에서는 학습을 통해, HMM에서는 시스템 구조로 직접 구현**
- 특히 전통적인 ASR의 발음사전, 정렬 정보, 모듈별 최적화 과정을 피하고, 오직 데이터를 기반으로 전체 시스템을 하나의 목표 아래 학습하는 것이 핵심


### ASR 시스템 개발의 목표
- 주 목표
  - 단어 오류율(WER: Word Error Rate) 최소화
- 부차적 목표
  - 디코더의 시간/메모리 복잡도 줄이기
  - 모델링을 쉽게 하고, 다양한 도메인/언어에 적용할 수 있는 범용성 확보 


### End-to-End 이점
- **구조적 단순화와 범용성 (Simplicity & Generality)**
  - 하나의 신경망 구조로 전체 시스템 구성
  - **디코딩**이 단일 모델 안에서 처리되어 **복잡한 모델 통합 불필요**
  - **새로운 언어나 도메인에 대해 빠른 개발 주기 달성**
- **효율성과 경량화 (Efficiency & Lightweight Design)**
  - 메모리, 연산 자원이 줄어들어 임베디드 환경에서의 ASR 적용에도 유리
- **학습 최적화와 안정성 (Learning Optimization & Robustness)**
  - 통합 학습(joint training)으로 잘못된 국소 최적화 지점 피할 수 있음
- **외부 리소스 없이 데이터 기반 학습 (Data-Driven & Resource-Free)**
  - 발음사전과 같은 2차 지식원 불필요
  - 2차 지식원이 잘못된 경우 생기는 오류 방지
  - 충분한 작업-특화 데이터(task-specific data)만 있으면 학습 가능


### 논문의 구성
- 제2장: E2E 음성 인식의 역사적 발전 과정과, **입출력 정렬 방식(alignment)** 관점의 분류
- 제3장: 기본 E2E 모델에 대한 개선 (구조, 문맥, loss 등)
- 제4장: E2E 모델 학습 방법
- 제5장: 다양한 E2E 디코딩 전략
- 제6장: E2E ASR에서의 **언어 모델(LM)** 의 역할과 통합
- 제7장: 최신 E2E 모델과 전통적인 HMM 기반 ASR의 비교
- 제8장: E2E 및 전통 ASR 접근법에 대한 성능 비교
- 제9장: E2E ASR의 실제 적용 사례
- 제10장: E2E ASR 향후 연구 방향
- 제11장: 결론



## 2. End-to-End 음성 인식 모델의 분류 체계

### 논문 전반 표기법
- **X : 입력 음성 발화**
  - 길이 **T′** 의 **D** 차원 음향 프레임 시퀀스 (예: log-mel features)로 표현
    - D는 각 프레임에서 추출되는 음향 특성 벡터의 차원 수 - **입력 음성의 “한 프레임당 정보량”을 몇 차원으로 표현할 것인지 결정하는 값**
    - 일반적으로는 40~80 차원이 자주 사용
  - X = $(x₁, …, x_{T′})$,   where   $x_t$ ∈ $ℝᴰ$
- **C : 입력 발화에 대응하는 단어 또는 문자 시퀀스**
  - 길이 $L$ 의 라벨(label) 시퀀스로 분해할 수 있으며, 라벨 단위는 문자, 단어, subword (예: BPE, word-piece 등)로 구성
    - **문자 단위는 너무 길고, 단어 단위는 희귀어 문제(OOV)가 심한데, Subword는 둘의 장점을 잘 절충해서 처리할 수 있기 때문**
    - **BPE (Byte Pair Encoding)**
      - 빈도 높은 문자 쌍부터 반복적으로 병합해서 단위를 만들어냄
      - 병합 규칙이 단순함 (문자 쌍 병합)
      - 예측 단위는 subword → 예: “reading” → “read”, “ing” 
    - **WordPiece**
      - Google에서 만든 방식 (BERT, T5 등에서 사용)
      - 기본 원리는 BPE와 비슷하지만, 병합 시 **likelihood**를 기반으로 선택함 (확률 모델 기반)
        - **“주어진 관측값이 어떤 확률 모델 하에서 얼마나 일어날 법한가“를 나타내는 수치**
        - **주어진 데이터가 특정 모델 하에서 발생할 가능성의 척도**
      - 더 언어학적 의미 단위에 가까운 subword를 생성 가능
  - C = $(c₁, …, c_L)$,   where   $c_j$ ∈ $𝒞$
  - ex) 문자 단위: C = (h, e, l, l, o), → L = 5
- **H(X) : 인코더 모듈**
  - 모든 **E2E 모델은 공통적으로 인코더 구조**를 포함
  - 인코더는 입력 음향 시퀀스 **$X$를 고차원 표현 $H(X)$**로 변환
  - X의 원시 특징을 요약하고, 모델이 C를 예측하는 데 필요한 정보를 담는것
  - H(X) = $(h_1, ..., h_T)$ (길이 T) 보통 T ≤ T′
    - 인코더는 **다운샘플링(stride, pooling)** 이나 **요약(압축)** 을 통해 입력보다 짧은 길이의 표현 T를 만들어내는 경우가 많음
    - 중요한 정보만 남기고 요약하기 위해
    - 입력이 너무 길면 연산량이 많아져 메모리, 학습시간 증가
  - 인코더는 RNN, CNN, Transformer 등 다양한 신경망 구조로 구현될 수 있으며, 논문에서는 이러한 구조의 세부 구현에는 구애받지 않고 일반적으로 설명
- **𝓣 : 훈련 데이터 집합**
  - 훈련 데이터는 **(음성, 전사)** 쌍으로 구성된 N개의 학습 샘플
  - $`𝒯 = { (Xᵢ, Cᵢ) ∣ i = 1, …, N }`$
 
- 모델의 학습 목표는, **입력 음성 $X$가 주어졌을 때 출력 시퀀스 $C$가 나올 조건부 확률**을 예측하는 것
- $$P(C|X) = P(C|H(X))$$
  - 입력 음성 X가 주어졌을 때, 그 음성을 인코더를 통해 벡터 표현 H(X)로 바꿔서, 최종적으로 그에 해당하는 텍스트 C를 생성할 확률을 구하겠다
  - 직접 X에서 바로 예측하는 게 아니라, 먼저 인코더를 통해 고차원 표현 H(X)를 만들고, 그 표현을 바탕으로 C를 예측하는 구조로 이루어져 있다는 걸 의미



### E2E 모델 정렬 방식(alignment strategy) 개요
- End-to-End ASR 모델의 핵심 목표는 **입력 음성 X로부터 출력 텍스트 C를 직접 예측하는 확률 분포 P(C|X)를 학습하는 것**
  - 음성(X)은 길고 연속적인 시퀀스이고, 텍스트(C)는 상대적으로 짧은 **불연속적인 단위(문자, 단어 등)** 로 이루어져 있음
  - 이 둘을 연결하기 위해 **어떤 프레임이 어떤 문자/단어에 대응되는지 정렬하는 과정(Alignment)** 이 필요

- 일반적으로, E2E 모델로 $P(C|X)$ 를 추정하는 데 있어서의 핵심 도전 과제 중 하나는, **음향 프레임 시퀀스(길이 T′)** 와 이에 대응되는 **라벨 시퀀스(길이 L)** 사이의 **정렬(Alignment)이 알려지지 않았다는 점**을 처리하는 것
- 전통적인 ASR 시스템에서는 이러한 프레임 단위 정렬을 **HMM(Hidden Markov Model)** 로 모델링하고, **GMM(Gaussian Mixture Model)** 또는 **신경망**을 통해 음향 프레임의 출력 분포를 학습
- 신경망 기반의 음향 모델을 훈련할 때 프레임 단위 정렬은 기존 **GMM-HMM 시스템을 이용한 강제 정렬(force-alignment)** 을 통해 얻을 수 있지만, **초기 정렬 없이도 바로 시퀀스를 학습하는 방법** 또한 가능
- 전통적인 ASR 시스템에서의 **음소 단위 정렬**은 자연스럽지만, E2E 시스템에서는 보통 **문자 기반의 (sub-)word 단위 라벨을 사용**
- 하지만, **문자 단위로의 정렬 개념**은 그리 명확하지 않다
- 따라서 본 논문에서는 **정렬을 어떻게 모델링하는지에 따라** E2E 모델들을 **명시적(Explicit) 정렬** 또는 **암시적(Implicit) 정렬** 방식으로 구분

- 초기의 E2E 모델들은 정렬을 명시적으로(latent variable로) 모델링했으며, 학습과 추론 과정에서 이 **잠재 변수(latent variable)** 는 (근사적으로라도) **주변화(marginalization) 처리**
- 이 계열의 대표적인 모델들에는 **CTC (Connectionist Temporal Classification)**, **RNN-T (Recurrent Neural Network Transducer)**, **RNA (Recurrent Neural Aligner)**, **HAT (Hybrid Auto-Regressive Transducer)** 등이 존재
- 이들 중 후속에 등장한 모델일수록 더 정교한 정렬 모델링을 수행하며, **독립 가정(independence assumption)** 을 줄이고, 그만큼 **모델 성능이나 표현력도 향상**되었다는 점은 이후 섹션에서 설명될 것
- 반대편 극단에는, 기계 번역(MT) 분야에서 처음 주목을 받은 attention 기반의 인코더-디코더(encoder-decoder) 모델
- CTC와 같은 명시적 정렬 모델들과 달리, attention 기반 인코더-디코더 모델은 attention mechanism을 사용하여 전체 음향 시퀀스와 개별 라벨 간의 매핑 관계 학습
- 마지막으로, 이 두 극단 사이에는 **중간적인 접근 방식들도 존재**
- 예를 들어: Neural Transducer, 모노토닉 정렬(monotonic alignment) 기반 모델들과 그 변형들(예: MoChA, MILK 등)은, **명시적 정렬 모델 구조**를 갖고 있으면서도, 동시에 **attention 메커니즘을 활용해 주변 음향 정보를 참고하여 예측을 정제(refine)** 할 수 있게 한다
- 이후 섹션에서는 이러한 각각의 모델 유형을 순서대로 설명할 것
