# 논문 정리 : End-to-End Speech Recognition: A Survey (T-ASLP 2024)

## Abstract
- **딥러닝이 도입된 ASR** 모델이 도입되지 않은 모델과 비교해 **단어 오류율(WER)이 50%이상 감소**
  - **WER** : 음성인식 시스템이 인식한 텍스트가 정답과 얼마나 다른지를 측정하는 평가 지표
    - WER = (I + D + S) / N
    - 추가, 삭제, 대체된 단어 수를 전체 정답 단어 수로 나누어 계산
      - **I (Insertions)**: 추가된 단어 수 (인식 결과에만 존재)
      - **D (Deletions)**: 빠뜨린 단어 수 (정답에 있었는데 인식 결과엔 없음)
      - **S (Substitutions)**: 잘못 인식된 단어 수 (ex. "love" → "like")
      - **N**: 정답 문장의 총 단어 수
    - 낮을수록 좋은 값
  - **CER** : 음성 인식의 오류율을 **문자 단위**로 계산하는 지표
    - 한국어, 중국어 등 공백이 없는 언어에서도 유용
    - WER에 비해 좀 더 세밀한 오류 파악 가능
- 현재는 **End-to-End(E2E)모델이 음성인식의 주 방식**

## 1. Introduction
### 전통적인 과정
- 전통적인 통계적 ASR 아키텍처는 네 가지 주요 요소로 구성
  - 음성 오디오 신호로부터의 **음향 특징 추출**
  - **음향 모델링**(acoustic modeling)
  - **언어 모델링**(language modeling)
  - **베이즈 결정 규칙**(bayes decision rule)에 기반한 탐색 과정
- 기존 음향 모델링은 **화자의 발화 속도 변화를 반영**하기 위해 **은닉 마르코프 모델(HMM)** 기반

### 딥러닝 도입 시작
- 음향 모델링과 언어 모델링에 도입 시작
  - 음향 모델링에서는 **가우시안 혼합 모델(GMM)** 을 대체
  - **음향 특징 집합을 확장**(예: 비선형 판별법 또는 tandem 방식)하는 데 사용
  - 언어 모델링에서는 **카운트 기반 방식(예: N-gram)** 이 **딥러닝 기반 언어 모델로 대체**
  - **기존 ASR 아키텍처 자체는 바뀌지 않음**
- **여전히 많은 별도의 구성 요소와 전문 지식에 의존**
  - 음성 신호 전처리
  - 녹음 환경 변화에 대한 견고성 확보 기법
  - 음소 목록과 발음 사전
  - 음소 클러스터링
  - OOV(사전에 없는 단어) 처리
  - 다양한 적응/정규화 방법
  - 복잡한 학습 스케줄 (예: 시퀀스 기반 판별 학습 포함) 등
- **음성 신호 전처리와 특징 추출을 하나의 음향 모델에 통합**하는 접근 제안
- 딥러닝의 도입은 **HMM 기반 전통 ASR 아키텍처를 완전히 대체하려는 연구**들로 이어짐
- **전통적인 음성 처리 모델**을, 더 일반적인 **시퀀스-투-시퀀스 방식의 머신러닝 모델**로 대체

### End-to-End 정의
- **Cambridge Dictionary**에 따르면, **“end-to-end”**라는 형용사는 **“하나의 과정의 모든 단계를 포함하는 것”** 으로 정의
- 해당 정의는 여러 관점에서 해석될 수 있음
- **a) Joint Modeling (통합 모델링)**
  - E2E는 시스템의 **모든 구성 요소를 하나의 연산 그래프 내에서 통합**하는 것으로 이해
  - **음향 모델, 언어 모델 등의 개별 구성요소를 분리하지 않고 하나의 모델로 통합하는 접근**
- **b) Single-Pass Search (단일 탐색 과정)**
  - **결정을 내리기 전에 모든 지식원을 통합해 단일 탐색을 수행**하는 것으로 해석
  - 즉, **단일 패스(single-pass)로 전체 인식이 끝나는** 구조
- **c) Joint Training (통합 학습)**
  - 모델의 모든 구성요소를 **하나의 목적함수로 통합하여 동시에 학습**한다는 개념이 적용
  - ASR의 경우, 이 목적함수는 보통 **기대 단어 오류율(WER)** 을 최소화하는 것
- **d) Training Data (단일 종류의 학습 데이터)**
  - 일반적으로 **하나의 데이터 유형**을 의미하며, ASR에서는 **전사된 음성 데이터**를 사용
  - 하지만 실제로는 **텍스트 전용이나 음성 전용 데이터가 더 많이 존재**
  - 이런 **비전사 데이터들을 어떻게 효과적으로 활용할 수 있을지가 E2E의 과제 중 하나**
- **e) Training from Scratch (처음부터 학습)**
  - **사전 정렬 정보나 외부 초기 모델 없이, 처음부터 끝까지 학습**하는 방식으로도 정의
  - 물론 상황에 따라서는 **사전학습(pretraining)** 이나 **파인튜닝(fine-tuning)** 전략이 여전히 중요한 경우도 존재
- **f) Secondary Knowledge Sources (2차 지식원 회피)**
  - 기존 ASR에서 사용하는 **발음 사전, 음소 집합, CART(분류 및 회귀 트리)** 같은 것들이 전형적인 2차 지식원
  - **E2E에서는 이러한 외부 지식원을 사용하지 않고, 가능한 한 데이터에서 직접 학습하려는 경향**
  - 이러한 회피는 **일관성 문제, 비용, 오류 가능성** 등을 줄여줌
- **g) Vocabulary Modeling (어휘 모델링 방식)**
  - 발음 사전을 사용하지 않으면, E2E 모델은 **전체 단어** 또는 **문자 기반 어휘에 의존**
  - 전체 단어 기반 모델은 **엄청난 양의 전사 데이터를 필요로 하므로 현실적으로 어려움 존재**
  - **Byte-Pair Encoding(BPE)** 같은 **서브워드 기법은** 일부 해석에서는 **완전한 E2E 철학과는 다소 다를 수 있음**
- **h) Generic vs. Informed Modeling (범용 vs. 특화 모델링)**
  - **도메인 특화 지식을 시스템에 사전에 넣는가? 아니면 모두 데이터로부터 학습하는가?** 라는 관점
  - **예를 들어, ASR의 순차적(monotonic) 정렬 특성을 attention 기반 모델에서는 학습을 통해, HMM에서는 시스템 구조로 직접 구현**
- **정리하자면, E2E ASR은 분리된 구성 요소나 외부 지식 없이, 단일 모델 내에서 학습과 인식을 동시에 수행하며 기대 단어 오류율을 일관되게 최소화하는 통합형 ASR 모델**


### End-to-End 이점
- **ASR 시스템을 개발할 때의 주요 목표는 기대 단어 오류율(WER)을 최소화**하는 것
- 하지만 부차적인 목표들도 존재
  - **결과 디코더의 시간 및 메모리 복잡도를 줄이는 것**
  - **범용성(genericity)** 과 **모델링의 용이성** 역시 중요한 요소
- 우선, ASR 시스템을 하나의 신경망 구조로 End-to-End로 정의하게 되면 범용 모델링이 가능해지며, 새로운 언어나 도메인에 대해 더 빠른 개발 주기 달성
- 이러한 방식의 ASR 모델은 전통적인 방식에 비해 더 경량화된 구조를 가질 수 있으며, 별도의 모델들을 통합할 필요가 없어 디코딩 과정 또한 단순
- 이로 인해 메모리 사용량과 전력 소비가 줄어들게 되어, 임베디드(embedded) 환경에서의 ASR 적용에도 유리
- 또한, E2E에서의 **통합 학습(joint training)** 은 **중간 학습 단계에서 발생할 수 있는 잘못된 최적화 지점(spurious optima)** 을 피하는 데 도움
- **발음 사전(pronunciation lexica)** 과 같은 **2차 지식원(secondary knowledge source)** 을 사용하지 않는 것은,
그러한 리소스를 구하기 어려운 언어나 도메인에서 특히 유용
- 게다가, 2차 지식원 자체가 오류를 포함하고 있을 가능성도 존재
- 따라서, 이러한 리소스를 피하고 충분한 작업(task)-특화 데이터만 있다면, 직접 데이터를 기반으로 학습한 더 나은 모델을 만들 수 있음
- 최근 E2E ASR에 대한 관심이 급증하고, 관련된 연구가 다양해지고 있는 현 시점에서, 본 논문의 저자들은 이제 이 빠르게 진화 중인 연구 분야에 대한 정리와 통찰을 제공할 시점이라 판단
- 이 논문의 목적은 현재 E2E ASR 시스템에 대한 연구 현황을 깊이 있게 다루고, E2E ASR의 모든 관련 측면을 포괄하며, E2E와 전통적인 ASR 구조의 비교 논의까지 포함하는 것

### 논문의 구성
- 제2장: E2E 음성 인식의 역사적 발전 과정과, **입출력 정렬 방식(alignment)** 에 대한 초점을 포함한 기본 모델들의 개요
- 제3장: 기본 E2E 모델에 대한 개선 사항—예: 모델 조합, 학습 손실 함수, 문맥 정보 활용, 인코더/디코더 구조, 종료 지점 처리 등
- 제4장: E2E 모델의 학습 방식에 대해 설명
- 제5장: 다양한 E2E 접근법에 대한 디코딩 알고리즘
- 제6장: E2E ASR에서의 **언어 모델(LM)** 의 역할과 통합 방식 설명
- 제7장: 최신 E2E 모델과 전통적인 HMM 기반 ASR의 관계 논의
- 제8장: E2E 및 전통 ASR 접근법에 대한 실험적 비교
- 제9장: E2E ASR의 실제 응용 사례
- 제10장: E2E ASR 연구의 향후 방향성 탐색
- 제11장에서 결론 제시




## 2. End-to-End 음성 인식 모델의 분류 체계

### 논문 전반 표기법
- **X : 입력 음성 발화**
  - 이는 길이 T′의 D차원 음향 프레임 시퀀스(예: log-mel 특성)로 표현된 것으로 가정
  - X = $(x₁, …, x_{T′})$,   where   $x_t$ ∈ $ℝᴰ$
- **C : 입력 발화에 대응하는 단어 시퀀스**
  - 이는 길이 L의 적절한 라벨(label) 시퀀스로 분해가능
  - 이때, 라벨 시퀀스가 어떤 단위(문자, 단어, subword 등)로 구성되는지에 대해서는 구체적인 표현 방식에 구애받지 않음
  - 실제로는 문자(character), 단어(word), BPE, 워드피스(word-piece) 등의 단위가 자주 사용
  - C = $(c₁, …, c_L)$,   where   $c_j$ ∈ $𝒞$
- **H(X) : 인코더 모듈**
  - 모든 E2E 모델은 인코더 모듈을 포함함
  - 이 인코더는 입력 음향 프레임 시퀀스 X (길이 T′)를 고차원 표현(high-level representation)으로 변환
  - H(X) = $(h_1, ..., h_T)$ (길이 T)
  - 보통 T ≤ T′
  - H(X)를 구현하는 신경망의 구체적인 구조(RNN, CNN, Transformer 등)에 대해서도 논문에서는 구체적인 선택에 구애받지 않고 일반적으로 설명
- **𝓣 : 훈련 데이터 집합**
  - N개의 (음성, 전사) 쌍으로 구성
  - 𝒯 = ${ (X_i, C_i) for i = 1 to N }$
 
- 모델의 학습 목표는, **입력 음성 X에 대한 조건부 확률 분포**인 $P(C|X)$ = $P(C|H(X))$ 를 정확히 추정하는 것
