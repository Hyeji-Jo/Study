# 논문 정리 : End-to-End Speech Recognition: A Survey (T-ASLP 2024)

## Abstract
- **딥러닝이 도입된 ASR** 모델이 도입되지 않은 모델과 비교해 **단어 오류율(WER)이 50%이상 감소**
  - **WER** : 음성인식 시스템이 인식한 텍스트가 정답과 얼마나 다른지를 측정하는 평가 지표
    - WER = (I + D + S) / N
    - 추가, 삭제, 대체된 단어 수를 전체 정답 단어 수로 나누어 계산
      - **I (Insertions)**: 추가된 단어 수 (인식 결과에만 존재)
      - **D (Deletions)**: 빠뜨린 단어 수 (정답에 있었는데 인식 결과엔 없음)
      - **S (Substitutions)**: 잘못 인식된 단어 수 (ex. "love" → "like")
      - **N**: 정답 문장의 총 단어 수
    - 낮을수록 좋은 값
  - **CER** : 음성 인식의 오류율을 **문자 단위**로 계산하는 지표
    - 한국어, 중국어 등 공백이 없는 언어에서도 유용
    - WER에 비해 좀 더 세밀한 오류 파악 가능
- 현재는 **End-to-End(E2E)모델이 음성인식의 주 방식**

## 1. Introduction
### 전통적인 과정
- 전통적인 통계적 ASR 아키텍처는 네 가지 주요 요소로 구성
  - 음성 오디오 신호로부터의 **음향 특징 추출**
  - **음향 모델링**(acoustic modeling)
  - **언어 모델링**(language modeling)
  - **베이즈 결정 규칙**(bayes decision rule)에 기반한 탐색 과정
- 기존 음향 모델링은 **화자의 발화 속도 변화를 반영**하기 위해 **은닉 마르코프 모델(HMM)** 기반

### 딥러닝 도입 시작
- 음향 모델링과 언어 모델링에 도입 시작
  - 음향 모델링에서는 **가우시안 혼합 모델(GMM)** 을 대체
  - **음향 특징 집합을 확장**(예: 비선형 판별법 또는 tandem 방식)하는 데 사용
  - 언어 모델링에서는 **카운트 기반 방식(예: N-gram)** 이 **딥러닝 기반 언어 모델로 대체**
  - **기존 ASR 아키텍처 자체는 바뀌지 않음**
- **여전히 많은 별도의 구성 요소와 전문 지식에 의존**
  - 음성 신호 전처리
  - 녹음 환경 변화에 대한 견고성 확보 기법
  - 음소 목록과 발음 사전
  - 음소 클러스터링
  - OOV(사전에 없는 단어) 처리
  - 다양한 적응/정규화 방법
  - 복잡한 학습 스케줄 (예: 시퀀스 기반 판별 학습 포함) 등
- **음성 신호 전처리와 특징 추출을 하나의 음향 모델에 통합**하는 접근 제안
- 딥러닝의 도입은 **HMM 기반 전통 ASR 아키텍처를 완전히 대체하려는 연구**들로 이어짐
- **전통적인 음성 처리 모델**을, 더 일반적인 **시퀀스-투-시퀀스 방식의 머신러닝 모델**로 대체

### End-to-End 정의
- **Cambridge Dictionary**에 따르면, **“end-to-end”**라는 형용사는 **“하나의 과정의 모든 단계를 포함하는 것”** 으로 정의
- 해당 정의는 여러 관점에서 해석될 수 있음
- **a) Joint Modeling (통합 모델링)**
  - E2E는 시스템의 **모든 구성 요소를 하나의 연산 그래프 내에서 통합**하는 것으로 이해
  - **음향 모델, 언어 모델 등의 개별 구성요소를 분리하지 않고 하나의 모델로 통합하는 접근**
- **b) Single-Pass Search (단일 탐색 과정)**
  - **결정을 내리기 전에 모든 지식원을 통합해 단일 탐색을 수행**하는 것으로 해석
  - 즉, **단일 패스(single-pass)로 전체 인식이 끝나는** 구조
- **c) Joint Training (통합 학습)**
  - 모델의 모든 구성요소를 **하나의 목적함수로 통합하여 동시에 학습**한다는 개념이 적용
  - ASR의 경우, 이 목적함수는 보통 **기대 단어 오류율(WER)** 을 최소화하는 것
- **d) Training Data (단일 종류의 학습 데이터)**
  - 일반적으로 **하나의 데이터 유형**을 의미하며, ASR에서는 **전사된 음성 데이터**를 사용
  - 하지만 실제로는 **텍스트 전용이나 음성 전용 데이터가 더 많이 존재**
  - 이런 **비전사 데이터들을 어떻게 효과적으로 활용할 수 있을지가 E2E의 과제 중 하나**
- **e) Training from Scratch (처음부터 학습)**
  - **사전 정렬 정보나 외부 초기 모델 없이, 처음부터 끝까지 학습**하는 방식으로도 정의
  - 물론 상황에 따라서는 **사전학습(pretraining)** 이나 **파인튜닝(fine-tuning)** 전략이 여전히 중요한 경우도 존재
- **f) Secondary Knowledge Sources (2차 지식원 회피)**
  - 기존 ASR에서 사용하는 **발음 사전, 음소 집합, CART(분류 및 회귀 트리)** 같은 것들이 전형적인 2차 지식원
  - **E2E에서는 이러한 외부 지식원을 사용하지 않고, 가능한 한 데이터에서 직접 학습하려는 경향**
  - 이러한 회피는 **일관성 문제, 비용, 오류 가능성** 등을 줄여줌
- **g) Vocabulary Modeling (어휘 모델링 방식)**
  - 발음 사전을 사용하지 않으면, E2E 모델은 **전체 단어** 또는 **문자 기반 어휘에 의존**
  - 전체 단어 기반 모델은 **엄청난 양의 전사 데이터를 필요로 하므로 현실적으로 어려움 존재**
  - **Byte-Pair Encoding(BPE)** 같은 **서브워드 기법은** 일부 해석에서는 **완전한 E2E 철학과는 다소 다를 수 있음**
- **h) Generic vs. Informed Modeling (범용 vs. 특화 모델링)**
  - **도메인 특화 지식을 시스템에 사전에 넣는가? 아니면 모두 데이터로부터 학습하는가?** 라는 관점
  - **예를 들어, ASR의 순차적(monotonic) 정렬 특성을 attention 기반 모델에서는 학습을 통해, HMM에서는 시스템 구조로 직접 구현**
- **정리하자면, E2E ASR은 분리된 구성 요소나 외부 지식 없이, 단일 모델 내에서 학습과 인식을 동시에 수행하며 기대 단어 오류율을 일관되게 최소화하는 통합형 ASR 모델**


### End-to-End 이점
- **ASR 시스템을 개발할 때의 주요 목표는 기대 단어 오류율(WER)을 최소화**하는 것
- 하지만 부차적인 목표들도 존재
  - **결과 디코더의 시간 및 메모리 복잡도를 줄이는 것**
  - **범용성(genericity)** 과 **모델링의 용이성** 역시 중요한 요소
- 우선, ASR 시스템을 하나의 신경망 구조로 End-to-End로 정의하게 되면 범용 모델링이 가능해지며, 새로운 언어나 도메인에 대해 더 빠른 개발 주기 달성
- 이러한 방식의 ASR 모델은 전통적인 방식에 비해 더 경량화된 구조를 가질 수 있으며, 별도의 모델들을 통합할 필요가 없어 디코딩 과정 또한 단순
- 이로 인해 메모리 사용량과 전력 소비가 줄어들게 되어, 임베디드(embedded) 환경에서의 ASR 적용에도 유리
- 또한, E2E에서의 **통합 학습(joint training)** 은 **중간 학습 단계에서 발생할 수 있는 잘못된 최적화 지점(spurious optima)** 을 피하는 데 도움
- **발음 사전(pronunciation lexica)** 과 같은 **2차 지식원(secondary knowledge source)** 을 사용하지 않는 것은,
그러한 리소스를 구하기 어려운 언어나 도메인에서 특히 유용
- 게다가, 2차 지식원 자체가 오류를 포함하고 있을 가능성도 존재
- 따라서, 이러한 리소스를 피하고 충분한 작업(task)-특화 데이터만 있다면, 직접 데이터를 기반으로 학습한 더 나은 모델을 만들 수 있음
- 최근 E2E ASR에 대한 관심이 급증하고, 관련된 연구가 다양해지고 있는 현 시점에서, 본 논문의 저자들은 이제 이 빠르게 진화 중인 연구 분야에 대한 정리와 통찰을 제공할 시점이라 판단
- 이 논문의 목적은 현재 E2E ASR 시스템에 대한 연구 현황을 깊이 있게 다루고, E2E ASR의 모든 관련 측면을 포괄하며, E2E와 전통적인 ASR 구조의 비교 논의까지 포함하는 것

### 논문의 구성
- 제2장: E2E 음성 인식의 역사적 발전 과정과, **입출력 정렬 방식(alignment)** 에 대한 초점을 포함한 기본 모델들의 개요
- 제3장: 기본 E2E 모델에 대한 개선 사항—예: 모델 조합, 학습 손실 함수, 문맥 정보 활용, 인코더/디코더 구조, 종료 지점 처리 등
- 제4장: E2E 모델의 학습 방식에 대해 설명
- 제5장: 다양한 E2E 접근법에 대한 디코딩 알고리즘
- 제6장: E2E ASR에서의 **언어 모델(LM)** 의 역할과 통합 방식 설명
- 제7장: 최신 E2E 모델과 전통적인 HMM 기반 ASR의 관계 논의
- 제8장: E2E 및 전통 ASR 접근법에 대한 실험적 비교
- 제9장: E2E ASR의 실제 응용 사례
- 제10장: E2E ASR 연구의 향후 방향성 탐색
- 제11장에서 결론 제시




## 2. End-to-End 음성 인식 모델의 분류 체계

### 논문 전반 표기법
- **X : 입력 음성 발화**
  - 이는 길이 T′의 D차원 음향 프레임 시퀀스(예: log-mel 특성)로 표현된 것으로 가정
  - X = $(x₁, …, x_{T′})$,   where   $x_t$ ∈ $ℝᴰ$
- **C : 입력 발화에 대응하는 단어 시퀀스**
  - 이는 길이 L의 적절한 라벨(label) 시퀀스로 분해가능
  - 이때, 라벨 시퀀스가 어떤 단위(문자, 단어, subword 등)로 구성되는지에 대해서는 구체적인 표현 방식에 구애받지 않음
  - 실제로는 문자(character), 단어(word), BPE, 워드피스(word-piece) 등의 단위가 자주 사용
  - C = $(c₁, …, c_L)$,   where   $c_j$ ∈ $𝒞$
- **H(X) : 인코더 모듈**
  - 모든 E2E 모델은 인코더 모듈을 포함함
  - 이 인코더는 입력 음향 프레임 시퀀스 X (길이 T′)를 고차원 표현(high-level representation)으로 변환
  - H(X) = $(h_1, ..., h_T)$ (길이 T)
  - 보통 T ≤ T′
  - H(X)를 구현하는 신경망의 구체적인 구조(RNN, CNN, Transformer 등)에 대해서도 논문에서는 구체적인 선택에 구애받지 않고 일반적으로 설명
- **𝓣 : 훈련 데이터 집합**
  - N개의 (음성, 전사) 쌍으로 구성
  - $`𝒯 = { (Xᵢ, Cᵢ) ∣ i = 1, …, N }`$
 
- 모델의 학습 목표는, **입력 음성 X에 대한 조건부 확률 분포**인 $P(C|X)$ = $P(C|H(X))$ 를 정확히 추정하는 것

### E2E 모델 정렬 방식(alignment strategy)
- 일반적으로, E2E 모델로 $P(C|X)$ 를 추정하는 데 있어서의 핵심 도전 과제 중 하나는, **음향 프레임 시퀀스(길이 T′)** 와 이에 대응되는 **라벨 시퀀스(길이 L)** 사이의 **정렬(Alignment)이 알려지지 않았다는 점**을 처리하는 것
- 전통적인 ASR 시스템에서는 이러한 프레임 단위 정렬을 **HMM(Hidden Markov Model)** 로 모델링하고, **GMM(Gaussian Mixture Model)** 또는 **신경망**을 통해 음향 프레임의 출력 분포를 학습
- 신경망 기반의 음향 모델을 훈련할 때 프레임 단위 정렬은 기존 **GMM-HMM 시스템을 이용한 강제 정렬(force-alignment)** 을 통해 얻을 수 있지만, **초기 정렬 없이도 바로 시퀀스를 학습하는 방법** 또한 가능
- 전통적인 ASR 시스템에서의 **음소 단위 정렬**은 자연스럽지만, E2E 시스템에서는 보통 **문자 기반의 (sub-)word 단위 라벨을 사용**
- 하지만, **문자 단위로의 정렬 개념**은 그리 명확하지 않다
- 따라서 본 논문에서는 **정렬을 어떻게 모델링하는지에 따라** E2E 모델들을 **명시적(Explicit) 정렬** 또는 **암시적(Implicit) 정렬** 방식으로 구분

- 초기의 E2E 모델들은 정렬을 명시적으로(latent variable로) 모델링했으며, 학습과 추론 과정에서 이 **잠재 변수(latent variable)** 는 (근사적으로라도) **주변화(marginalization) 처리**
- 이 계열의 대표적인 모델들에는 **CTC (Connectionist Temporal Classification)**, **RNN-T (Recurrent Neural Network Transducer)**, **RNA (Recurrent Neural Aligner)**, **HAT (Hybrid Auto-Regressive Transducer)** 등이 존재
- 이들 중 후속에 등장한 모델일수록 더 정교한 정렬 모델링을 수행하며, **독립 가정(independence assumption)** 을 줄이고, 그만큼 **모델 성능이나 표현력도 향상**되었다는 점은 이후 섹션에서 설명될 것
- 반대편 극단에는, 기계 번역(MT) 분야에서 처음 주목을 받은 attention 기반의 인코더-디코더(encoder-decoder) 모델
- CTC와 같은 명시적 정렬 모델들과 달리, attention 기반 인코더-디코더 모델은 attention mechanism을 사용하여 전체 음향 시퀀스와 개별 라벨 간의 매핑 관계 학습
- 마지막으로, 이 두 극단 사이에는 **중간적인 접근 방식들도 존재**
- 예를 들어: Neural Transducer, 모노토닉 정렬(monotonic alignment) 기반 모델들과 그 변형들(예: MoChA, MILK 등)은, **명시적 정렬 모델 구조**를 갖고 있으면서도, 동시에 **attention 메커니즘을 활용해 주변 음향 정보를 참고하여 예측을 정제(refine)** 할 수 있게 한다
- 이후 섹션에서는 이러한 각각의 모델 유형을 순서대로 설명할 것


### A. 명시적 정렬 기반 E2E 접근법 (Explicit Alignment E2E Approaches)
- CTC, RNN-T, RNA와 같은 명시적 정렬(explicit alignment) 기반 접근 방식들은, T 길이의 인코더 출력 H(X) 와 L 길이의 출력 시퀀스 C 간의 정렬 관계에 대응되는 잠재 변수(latent variable) 를 명시적으로 정의합니다.
- 이러한 각 접근 방식은 정렬을 정의하는 방식이 서로 다르며, 그에 따라 모델 내부의 조건부 확률 독립 가정(conditional independence assumptions),그리고 훈련 및 디코딩 방식 또한 달라집니다.
- 모든 명시적 정렬 모델들의 공통점은, ⟨b⟩ 기호로 표기되는 추가적인 blank 기호를 도입하여, 출력 기호 집합을 다음과 같이 확장된 집합으로 정의한다는 점입니다: $`C_b = C \cup \{⟨b⟩\}`$
- 이때 ⟨b⟩ 기호의 해석은 모델마다 약간씩 차이가 있으며, 이후에 더 자세히 설명될 예정입니다.
- 우선 지금 시점에서는, 특정 훈련 예제 (X, C) 가 주어졌을 때, 각 모델은 유효한 정렬(valid alignments) 의 집합 $`{A}_{(T,C)}`$ 를 정의하며, 다음과 같이 모든 가능한 정렬 시퀀스에 대해 주변화(marginalize) 함으로써 정의한다는 점만 이해하면 충분합니다: $`\[
P(C|X) = P(C|H(X)) = \sum_{A} P(C | A, H(X)) \cdot P(A | H(X))
= \sum_{A \in \mathcal{A}_{(T = |H(X)|, C)}} P(A | H(X)) \tag{1}
\]`$
- 여기서, 정의에 따라 $`P(C | A, H(X)) = 1 \quad \text{iff} \quad A \in \mathcal{A}_{(T, C)}`$
- 즉, 정렬 A가 유효한 정렬 집합에 속할 경우에만 1이고, 그렇지 않으면 0이 됩니다.
- 이는 곧, 하나의 정렬 A로부터 라벨 시퀀스 C로의 매핑이 유일하게 정의된다는 것을 의미
- 각 모델에서 이 정렬을 어떻게 구체적으로 정의하고 사용하는지는 다음 절들에서 자세히 설명됩니다
