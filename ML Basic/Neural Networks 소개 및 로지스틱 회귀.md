# Neural Networks ì†Œê°œ ë° ë¡œì§€ìŠ¤í‹± íšŒê·€

ì‘ì„±ì: ì¡°í˜œì§€
ì‚¬ìš©ì–¸ì–´: Python
ê°•ì˜ëª…: ML Bootcamp Korea 2023, ë”¥ëŸ¬ë‹ 1ë‹¨ê³„ : ì‹ ê²½ë§ê³¼ ë”¥ëŸ¬ë‹
íˆ¬ì…ê¸°ê°„: 2023/08/30 â†’ 2023/09/05

/ëª©ì°¨

---

# 1. Deep Learning ì†Œê°œ

## 1. Neural Network (ì‹ ê²½ë§ì´ë€?)

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled.png)

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%201.png)

- ì‹ ê²½ë§ì˜ ë‹¨ì¼ ë‰´ëŸ°ì¸ ì‘ì€ ì›ì´ ì˜†ì˜ íŒŒë€ í•¨ìˆ˜ êµ¬ì„±

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%202.png)

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%203.png)

- ì‹ ê²½ë§ ì •ì˜
    - ìœ„ì™€ ê°™ì€ ê³¼ì •ì„ ë”¥ëŸ¬ë‹(Deep learning , ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµ)ìœ¼ë¡œ ì§„í–‰í•˜ê²Œ ë˜ë©´ ê°€ì¡±êµ¬ì„±ì›ì˜ ìˆ˜, ë³´í–‰ì„±, í•™êµ ìˆ˜ì¤€ ë“±ì˜ ë³€ìˆ˜ë¥¼ ìŠ¤ìŠ¤ë¡œ ë§Œë“¤ì–´ í•™ìŠµ
    - ì¦‰, ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ë³€ìˆ˜ x(ì£¼íƒì˜ ì •ë³´)ë§Œ ì…ë ¥í•˜ë©´ y(ì£¼íƒì˜ ê°€ê²©)ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ

<aside>
ğŸ’¡ Every input layer feature is interconnected with every hidden layer feature
â†’ **ì€ë‹‰ì¸µì˜ ë…¸ë“œë“¤ì€ ëª¨ë“  ì…ë ¥ì¸µì˜ ë…¸ë“œë“¤ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤**

</aside>

### â€» ReLU(Rectified Linear Unit) í•¨ìˆ˜

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%204.png)

<aside>
ğŸ’¡ Rectifiedë€ 'ì •ë¥˜ëœ' ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ íë¦„ì„ ì°¨ë‹¨í•œë‹¤ê³  ë³´ë©´ ë¨

**ReLU** í•¨ìˆ˜ë„ **x ê°€ 0 ì´í•˜ì¼ ë•Œ**ë¥¼ ì°¨ë‹¨í•˜ì—¬ ì•„ë¬´ ê°’ë„ ì¶œë ¥í•˜ì§€ ì•Šê³  **0 ì„ ì¶œë ¥**
â†’ ì´ëŸ¬í•œ ì ì—ì„œ ì „ë¥˜ê°€ ì°¨ë‹¨ëœë‹¤ê³  ë³´ì„
**xê°€ ì–‘ìˆ˜ ê°’**ì˜ ê²½ìš° **ê°’ì„ ê·¸ëŒ€ë¡œ** ë‚´ë³´ë‚´ëŠ” í•¨ìˆ˜

</aside>

## 2. Supervised Learning with Neural Network (ì‹ ê²½ë§ ì§€ë„í•™ìŠµ)

### 1) ì‹ ê²½ë§ ì˜ˆì‹œ ë° í™œìš©ì‚¬ë¡€

![ì‹ ê²½ë§ ì§€ë„í•™ìŠµì˜ í™œìš©ì‚¬ë¡€](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%205.png)

ì‹ ê²½ë§ ì§€ë„í•™ìŠµì˜ í™œìš©ì‚¬ë¡€

![ì‹ ê²½ë§ ì˜ˆì‹œ](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%206.png)

ì‹ ê²½ë§ ì˜ˆì‹œ

- ì‹ ê²½ë§ ì¢…ë¥˜
    1. ì¼ë°˜ ì‹ ê²½ë§ - Standard NN
    2. í•©ì„±ê³± ì‹ ê²½ë§ - Convolutional NN
    3. ìˆœí™˜ ì‹ ì…©ë§ - Recurrent NN

### 2) í™œìš© ë°ì´í„° ì¢…ë¥˜

- êµ¬ì¡°í™”ëœ ë°ì´í„°/**ì •í˜• ë°ì´í„°** - Structured Data
    1. ë°ì´í„° ë² ì´ìŠ¤
- ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„°/**ë¹„ì •í˜• ë°ì´í„°** - Unstructured Data
    1. ì˜¤ë””ì˜¤, ìŒì„± íŒŒì¼
    2. ì´ë¯¸ì§€
    3. í…ìŠ¤íŠ¸

## 3. Deep Learningì˜ ê¸‰ë¶€ìƒ ì´ìœ 

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%207.png)

- **ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì§ˆìˆ˜ë¡, ì‹ ê²½ë§ì´ ë³µì¡í• ìˆ˜ë¡ ë”¥ëŸ¬ë‹ í•™ìŠµì˜ ì„±ëŠ¥ì€ ì˜¬ë¼ê°**
    - ë°ì´í„°ì˜ ì–‘ì´ ì ì„ë•ŒëŠ” ì–´ëŠ ì‹ ê²½ë§ì´ í™•ì‹¤íˆ ìš°ì›”í•œì§€ ë°°ì—´ì´ ì˜ ë˜ì–´ ìˆì§€ ì•Šê¸°ì— 
    ë³¸ì¸ì´ ì‚¬ìš©í•˜ê³  ì‹¶ì€ê±° ì‚¬ìš©í•˜ë©´ ë¨
    - ë°ì´í„°ì˜ ì–‘ â†’ **ë ˆì´ë¸” ë˜ì–´ìˆëŠ” ë°ì´í„°ì˜ ì–‘**

1. ë°ì´í„° - Data
    1. ì´ì „ë³´ë‹¤ ë§ì€ ì–‘ì˜ ë°ì´í„°
2. ì»´í“¨í„° - Computation
    1. CPU
    2. GPU
3. ì•Œê³ ë¦¬ì¦˜ - Algorithms
    1. í™œì„±í•¨ìˆ˜ì˜ ë³€í™”(Sigmoid â†’ ReLU)

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%208.png)

![Sigmoid](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%209.png)

Sigmoid

![ReLU](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%204.png)

ReLU

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” **í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œìš´ ê°’ì´ ë¨**
    - Gradient Descent(ê²½ì‚¬ í•˜ê°•ë²•) ë„ì…ì˜ ê²½ìš° ê¸°ìš¸ê¸°ê°€ 0ì´ë©´ ê°œì²´ê°€ ëŠë¦° ì†ë„ë¡œ ë³€í™”
    - ì¦‰, **í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì§**
- ReLU
    - **ê¸°ìš¸ê¸°ê°€ ëª¨ë“  ì–‘ìˆ˜ì— ëŒ€í•´ 1**ì´ ë¨
    - ê¸°ìš¸ê¸°ê°€ 0ìœ¼ë¡œ ì¤„ì–´ë“œëŠ” í™•ë¥ ì€ ê¸‰ê²©íˆ ê°ì†Œ

---

# 2. Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

## 1. Binary Classification (ì´ì§„ ë¶„ë¥˜)

![1 (ê³ ì–‘ì´) VS 0 (ê³ ì–‘ì´X)](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2010.png)

1 (ê³ ì–‘ì´) VS 0 (ê³ ì–‘ì´X)

### â€» ë¡œì§€ìŠ¤í‹± íšŒê·€ í•¨ìˆ˜ í‘œê¸°ë²•

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2011.png)

## 2. Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2012.png)

- Sigmoid í•¨ìˆ˜
    - zì˜ ê°’ì´ ì»¤ì§€ë©´ â†’ 1ì— ê°€ê¹Œì›€
    - zì˜ ê°’ì´ ì‘ì•„ì§€ë©´ â†’ 0ì— ê°€ê¹Œì›€
    

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2013.png)

- ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì˜ ëª©í‘œ
    - **$w^T$ì™€ $b$ë¥¼ ìˆ˜ì •í•¨ìœ¼ë¡œì¨ ë¶„ë¥˜ ê¸°ì¤€ì´ ë˜ëŠ” ì ì ˆí•œ ì„ ì„ ì°¾ëŠ”ê²ƒ**
- ë§¤ê°œ ë³€ìˆ˜(Parameters)
    - $w$ : ê°€ì¤‘ì¹˜ (weight)
    - $b$ : í¸í–¥ (bias)
- Sigmoid í•¨ìˆ˜ í™œìš© ì´ìœ 
    - $0\le \hat{y} \le1$ë¡œ ê°’ì„ ë§ì¶°ì£¼ê¸° ìœ„í•´ 
    ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì‚¬ìš©
    - yì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ 0.5ë³´ë‹¤ í¬ë©´ ê³ ì–‘ì´ ì‚¬ì§„, ì‘ìœ¼ë©´ ê³ ì–‘ì´ ì‚¬ì§„ì´ ì•„ë‹Œê²ƒìœ¼ë¡œ íŒë³„

## 3. Logistic Regression Cost Function 
(ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¹„ìš©í•¨ìˆ˜)

### 1) ì†ì‹¤ í•¨ìˆ˜ (Loss function)

- ì…ë ¥ íŠ¹ì„±(x)ì— ëŒ€í•œ ì˜ˆì¸¡ê°’($\hat{y}$)ê³¼ ì •ë‹µê°’(y) ì°¨ì´ì˜ ì œê³±
    - $L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2$
    - ë³´í†µì˜ ì†ì‹¤ í•¨ìˆ˜ ì‹

![ì§€ì—­ ìµœì†Œê°’](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2014.png)

ì§€ì—­ ìµœì†Œê°’

- ê·¸ëŸ¬ë‚˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì´ëŸ¬í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ 
ì‚¬ìš©í•˜ë©´ **ì§€ì—­ ìµœì†Œê°’**ì— ë¹ ì§ˆ ìˆ˜ ìˆìŒ
    - **$L(\hat{y},y)=-(ylog\hat{y} +  (1-y)log(1-\hat{y}))$**
    - **yê°€ 1ì¼ ê²½ìš°** â†’ $L(\hat{y},1)=-log\hat{y}$
        - $log\hat{y}$ê³¼  $\hat{y}$ê°€ ì»¤ì•¼í•¨ â†’ sigmoid í•¨ìˆ˜ëŠ” 
        $0\le \hat{y} \le1$ â†’ **$\hat{y}$ì€ 1**ì— ìˆ˜ë ´
    - **yê°€ 0ì¼ ê²½ìš°** â†’ $L(\hat{y},0)=-log(1-\hat{y})$
        - $\hat{y}$ì´ ì‘ì•„ì•¼í•¨ â†’ sigmoid í•¨ìˆ˜ëŠ” $0\le \hat{y} \le1$ â†’ **$\hat{y}$ì€ 0**ì— ìˆ˜ë ´

### 2) ë¹„ìš© í•¨ìˆ˜ (Cost function)

- ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ê³„ì‚°í•œ **ì†ì‹¤ í•¨ìˆ˜ì˜ í‰ê·  ê°’**ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥
- **$J(w,b) =\frac{1}{m}\sum^{i=m}_{i=1}L(\hat{y}^{(i)},y^{(i)})= -\frac{1}{m}\sum^{i=m}_{i=1}(y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)}))$**

## 4. Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•)

![ê²½ì‚¬ í•˜ê°•ë²•](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2015.png)

ê²½ì‚¬ í•˜ê°•ë²•

- wì™€ bë¥¼ ì°¾ê³  ì‹¶ë‹¤ë©´ $J(w,b)$ë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒ
- í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ëª¨ë¥´ê¸°ì— ì„ì˜ì˜ ì ì„ ê³¨ë¼ ì‹œì‘
- ì˜¤ì°¨(í•¨ìˆ«ê°’)ê°€ ê°€ì¥ ì ì€ **ê°€ì¥ ì•„ë˜ì˜ ë¹¨ê°„ ì (Global Optimum)**

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2016.png)

- $w : w-\alpha\frac{dJ(w,b)}{dw}$
- $b : b-\alpha\frac{dJ(w,b)}{db}$
- $\frac{dJ(w)}{dw}$ : ë„í•¨ìˆ˜ë¼ê³  í•˜ë©°, ë¯¸ë¶„ì„ í†µí•´ êµ¬í•œ ê°’, dwë¼ê³  í‘œê¸°í•˜ê¸°ë„ í•¨
- $\alpha$ : í•™ìŠµë¥ 
- ë§Œì•½ dw >0ì´ë©´ ê¸°ì¡´ wê°’ ë³´ë‹¤ ì‘ì€ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸, dw<0ì´ë©´ ê¸°ì¡´ wê°’ ë³´ë‹¤ í° ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸
- $dw =\frac{\partial J(w,b)}{\partial w}$ : í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ w ë°©í–¥ìœ¼ë¡œ ì–¼ë§Œí¼ ë³€í–ˆëŠ”ì§€
- $db =\frac{\partial J(w,b)}{\partial b}$ : í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ b ë°©í–¥ìœ¼ë¡œ ì–¼ë§Œí¼ ë³€í–ˆëŠ”ì§€

## 5. Derivatives (ë„í•¨ìˆ˜)

- ì–´ë–¤ í•¨ìˆ˜ a ë„í•¨ìˆ˜ = ì§ì„ ì˜ ê¸°ìš¸ê¸°
- $\frac{d}{da}f(a)$
- ê¸°ìš¸ê¸° í˜¹ì€ ë„í•¨ìˆ˜ëŠ” í•¨ìˆ˜ì˜ ìœ„ì¹˜ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2017.png)

## 6. Computation Graph (ê³„ì‚° ê·¸ë˜í”„)

### 1) $J(a,b,c) =3(a+bc)$ì˜ ê³„ì‚° ê·¸ë˜í”„ ë§Œë“œëŠ” ê³¼ì •

- u = bc
- v = a+u
- J = av

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2018.png)

- ìœ„ì™€ ê°™ì´ **ìˆœë°©í–¥** ë°©ì‹ì€ **í•¨ìˆ«ê°’**ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©í•˜ë©°, **ì—­ë°©í–¥**ì˜ ë°©ì‹ì€ **ë¯¸ë¶„ê°’**ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©

### 2) ê³„ì‚° ê·¸ë˜í”„ì˜ ë¯¸ë¶„

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2019.png)

- ë„í•¨ìˆ˜ì˜ ê³„ì‚°ì€ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì§„í–‰
- $\frac{dJ}{dv} =?=3$
- $\frac{dJ}{da} =3=\frac{dJ}{dv}\frac{dv}{da}$=3*1
- $\frac{dv}{da} =1$

## 7. ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²½ì‚¬ í•˜ê°•ë²•

### 1) ë¡œì§€ìŠ¤í‹± íšŒê·€ ì •ë¦¬

- $z = w^Tx+b$
- $\hat{y}=a=\sigma(z)$
- **$L(a,y)=-(yloga +  (1-y)log(1-a))$**

### 2) ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¯¸ë¶„

- $z = w_1x_1+w_2x_2+b$ **â†’** $a=\sigma(z)$ **â†’** $L(a,y)$
1. $L(a,y)$ â†’  $a=\sigma(z)$ 
    - $da=\frac{dL(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$
2. $a=\sigma(z)$ â†’ $z = w_1x_1+w_2x_2+b$ 
    - $dz=\frac{dL(a,y)}{dz}=a-y=\frac{dL}{da}\frac{da}{dz}$

### 3) mê°œ ìƒ˜í”Œì˜ ê²½ì‚¬ í•˜ê°•ë²•

- ë¹„ìš© í•¨ìˆ˜ **$J(w,b) =\frac{1}{m}\sum^{i=m}_{i=1}L(a^{(i)},y^{(i)})$**
    
    $a^{(i)}=\hat{y}^{(i)}=\sigma(z^{(i)})=\sigma(w^Tx^{(i)}+b)$
    

![ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œì˜ ë¹„ìš©í•¨ìˆ˜ ì½”ë“œí™”](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2020.png)

ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œì˜ ë¹„ìš©í•¨ìˆ˜ ì½”ë“œí™”

- í˜„ì¬ ì½”ë“œì—ì„œëŠ” n=2ì§€ë§Œ, íŠ¹ì„±ì˜ ê°œìˆ˜ê°€ ë§ì•„ì§„ë‹¤ë©´ forë¬¸ ì²˜ë¦¬ë¥¼ í•´ì•¼í•¨
- ê·¸ë ‡ê²Œ ëœë‹¤ë©´ ì´ì¤‘ forë¬¸ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ ë˜ê³  ì´ë¡œì¸í•´ ê³„ì‚° ì†ë„ê°€ ëŠë ¤ì§

---

# 3. Python and Vectorization(íŒŒì´ì¬ ë²¡í„°í™”)

## 1. Vectorization(ë²¡í„°í™”)

### 1) ê°œë… ë° ì˜ˆì œ

- ì½”ë”©ì—ì„œ ëª…ì‹œì  forë¬¸ì„ ì œê±°í•˜ëŠ” ê¸°ìˆ 

![ë²¡í„°í™” VS ë¹„ë²¡í„°í™”](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2021.png)

ë²¡í„°í™” VS ë¹„ë²¡í„°í™”

- ë²¡í„°í™”ì¸ ê²½ìš°ê°€ ë¹„ë²¡í„°í™”ì¸ ê²½ìš°ë³´ë‹¤ ê³„ì‚° ì†ë„ê°€ ë¹ ë¥´ë‹¤

```python
import numpy as np

a = np.array([1,2,3,4])
print(a) # [1 2 3 4]
```

```python
import time

a = np.random.rand(1000000)
b = np.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c) # 250170.62904592504
print('Vectorized version : ' + str(1000*(toc-tic)) +'ms') 
# Vectorized version : 1.4641284942626953ms

c= 0
tic = time.time()
for i in range(1000000):
  c += a[i]*b[i]
toc = time.time()

print(c) # 250170.62904593535
print('For loop : '+str(1000*(toc-tic))+'ms')
# For loop : 530.8642387390137ms
```

- np.dot(a,b)ëŠ” a,bë¥¼ ë²¡í„°í™” í•´ì£¼ëŠ” ì—­í• 
- code ì‹¤ìŠµì„ í†µí•´ ë¹„êµí•´ë´ë„ ë²¡í„°í™” ë²„ì „ì´ í›¨ì”¬ ë¹ ë¥´ë‹¤
- ì´ë ‡ê²Œ **ë²¡í„°ì˜ í˜•íƒœë¡œ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ë³‘ë ¬ì  ì—°ì‚°**ì„ í•˜ê²Œ ë˜ëŠ”ë° ì´ëŸ¬í•œ ê²½ìš° **CPUë³´ë‹¤ GPUê°€ ë” ìœ ë¦¬**
- ì»´í“¨í„°ì˜ ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ ê°€ëŠ¥í•˜ë©´ â€˜for loopâ€™ë¥¼ í”¼í•˜ëŠ”ê²ƒì´ ì¢‹ìŒ

![ë²¡í„°í™” VS ë¹„ë²¡í„°í™” ì˜ˆì œ2 ](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2022.png)

ë²¡í„°í™” VS ë¹„ë²¡í„°í™” ì˜ˆì œ2 

![ë²¡í„°í™” VS ë¹„ë²¡í„°í™” ì˜ˆì œ2](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2023.png)

ë²¡í„°í™” VS ë¹„ë²¡í„°í™” ì˜ˆì œ2

```python
# non-vectorization
u = np.zeros((n,1))
for i in range(n):
  u[i] = math.exp(v[i])

# vectorization
import numpy as np
u = np.exp(v)
```

- np.zeros((n,1)) â†’ 0ìœ¼ë¡œ ì´ë£¨ì–´ì§„ n*1 í–‰ë ¬ ìƒì„±
- np.exp(v) â†’ ìš”ì†Œë³„ ì§€ìˆ˜ ê³„ì‚°
- np.log(v) â†’ ìš”ì†Œë³„ ë¡œê·¸ê°’ ê³„ì‚°
- np.abs(v) â†’ ì ˆëŒ€ê°’ ê³„ì‚°
- np.maximum(v,0) â†’ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ ìµœëŒ€ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
- v**2 â†’ ì œê³± ê³„ì‚°
- 1/v â†’ ì—­ìˆ˜

### 2) ë¡œì§€ìŠ¤í‹± íšŒê·€ ë²¡í„°í™”

![ë¡œì§€ìŠ¤í‹± íšŒê·€ ë²¡í„°í™”](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2024.png)

ë¡œì§€ìŠ¤í‹± íšŒê·€ ë²¡í„°í™”

- ë²¡í„°í™” ì ìš©ì„ í†µí•´ forë¬¸ í•˜ë‚˜ë§Œ ë‚¨ì€ ìƒíƒœ

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2025.png)

### 3) ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²½ì‚¬í•˜ê°•ë²•

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2026.png)

![ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²½ì‚¬ í•˜ê°• ë²¡í„°í™” ì™„ì„±](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2027.png)

ë¡œì§€ìŠ¤í‹± íšŒê·€ ê²½ì‚¬ í•˜ê°• ë²¡í„°í™” ì™„ì„±

## 2. Broadcasting in Python (íŒŒì´ì¬ ë¸Œë¡œë“œ ìºìŠ¤íŒ…)

- Numpy ë°°ì—´ì—ì„œ **ì°¨ì›ì˜ í¬ê¸°ê°€ ì„œë¡œ ë‹¤ë¥¸** ë°°ì—´ì—ì„œë„ **ì‚°ìˆ  ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì›ë¦¬**
    - ë‘ ë°°ì—´ì˜ ì°¨ì›(ndim)ì´ ê°™ì§€ ì•Šë‹¤ë©´ ì°¨ì›ì´ ë” **ë†’ì€ ë°°ì—´ê³¼ ê°™ì€ ì°¨ì›ì˜ ë°°ì—´ë¡œ ì¸ì‹**
    - ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì¼ë•Œ ê°€ëŠ¥

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2028.png)

```python
import numpy as np

A = np.array([[56.0, 0.0, 4.4, 68.0],
             [1.2, 104.0, 52.0, 8.0],
             [1.8, 135.0, 99.0, 0.9]])

print(A)
'''
[[ 56.    0.    4.4  68. ]
 [  1.2 104.   52.    8. ]
 [  1.8 135.   99.    0.9]]
'''

cal = A.sum(axis=0)
print(cal)
# [ 59.  239.  155.4  76.9]

cal.reshape(1,4) # reshapeë¥¼ í™œìš©í•œ í–‰ë ¬í™”
# array([[ 59. , 239. , 155.4,  76.9]])

percentage = 100*A/cal.reshape(1,4)
print(percentage)
'''
[[94.91525424  0.          2.83140283 88.42652796]
 [ 2.03389831 43.51464435 33.46203346 10.40312094]
 [ 3.05084746 56.48535565 63.70656371  1.17035111]]
'''
```

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2029.png)

```python
A = np.array([1,2,3,4])
A+100
# array([101, 102, 103, 104])
```

![Untitled](Neural%20Networks%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%2088ffdb194cb74d8794a8e7f0944748b8/Untitled%2030.png)

```python
A = np.array([[1,2,3],[4,5,6]])
B = np.array([100,200,300])
A+B
'''
array([[101, 202, 303],
       [104, 205, 306]])
'''
```

```python
# rank 1 array (Don't use)
import numpy as np
a = np.random.randn(5)
print(a)
# [-0.93997085  1.37828891  0.23178588 -0.97728334 -1.68825958]
print(a.shape)
# (5,)
print(a.T)
# [-0.93997085  1.37828891  0.23178588 -0.97728334 -1.68825958]
print(np.dot(a,a.T))
# 6.642253350337608
```

```python
# array
import numpy as np
a = np.random.randn(5,1) # ì—´ ë²¡í„°
# a = np.random.randn(1,5) # í–‰ ë²¡í„°
print(a)
'''
[[-1.21270946]
 [ 1.24917393]
 [-1.55620123]
 [-0.85252435]
 [ 0.31968055]]
'''
print(a.T)
# [[-1.21270946  1.24917393 -1.55620123 -0.85252435  0.31968055]]
print(np.dot(a,a.T))
'''
[[ 1.47066423 -1.51488504  1.88721995  1.03386434 -0.38767962]
 [-1.51488504  1.5604355  -1.943966   -1.06495119  0.3993366 ]
 [ 1.88721995 -1.943966    2.42176227  1.32669944 -0.49748726]
 [ 1.03386434 -1.06495119  1.32669944  0.72679776 -0.27253545]
 [-0.38767962  0.3993366  -0.49748726 -0.27253545  0.10219565]]
'''
```

## 3. Quiz

- ë¬¸ì œ í’€ì´ ë° ì‹¤ìŠµ
    
    ### 1) Consider the following code snippet:
    
    a.shape = (3,4)
    
    b.shape = (4,1)
    
    for i in range(3):
    for j in range(4):
    c[i][j] = a[i][j] + b[j]
    
    How do you vectorize this? â†’ c = a + b.T
    
    ### 2) sigmoid í•¨ìˆ˜ ì½”ë“œ â†’ $sigmoid(x) = \frac{1}{1+e^{-x}}$
    
    ```python
    import math
    from public_tests import *
    
    # GRADED FUNCTION: basic_sigmoid
    
    def basic_sigmoid(x):
        """
        Compute sigmoid of x.
    
        Arguments:
        x -- A scalar
    
        Return:
        s -- sigmoid(x)
        """
        # (â‰ˆ 1 line of code)
        # s = 
        # YOUR CODE STARTS HERE
        s = 1/(1+math.exp(-x))
    		# s = 1/(1+np.exp(-x)) 
        
        # YOUR CODE ENDS HERE
        
        return s
    ```
    
    ```python
    # GRADED FUNCTION: sigmoid_derivative
    
    def sigmoid_derivative(x):
        """
        Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.
        You can store the output of the sigmoid function into variables and then use it to calculate the gradient.
        
        Arguments:
        x -- A scalar or numpy array
    
        Return:
        ds -- Your computed gradient.
        """
        
        #(â‰ˆ 2 lines of code)
        # s = 
        # ds = 
        # YOUR CODE STARTS HERE
        s = 1/(1+np.exp(-x))
        ds = s*(1-s)
        
        # YOUR CODE ENDS HERE
        
        return ds
    ```